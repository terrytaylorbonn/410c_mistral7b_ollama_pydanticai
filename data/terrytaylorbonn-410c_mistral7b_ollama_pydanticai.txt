Directory structure:
‚îî‚îÄ‚îÄ terrytaylorbonn-410c_mistral7b_ollama_pydanticai/
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ agent1_research.py
    ‚îú‚îÄ‚îÄ agent2_writer.py
    ‚îú‚îÄ‚îÄ agent_local.py
    ‚îú‚îÄ‚îÄ agent_print_files copy.py
    ‚îú‚îÄ‚îÄ agent_print_files.py
    ‚îú‚îÄ‚îÄ agent_print_files_langgraph.py
    ‚îú‚îÄ‚îÄ agent_print_files_langgraph_new.py
    ‚îú‚îÄ‚îÄ agent_with_file_search.py.py
    ‚îú‚îÄ‚îÄ audio_transcribe_summarize.py
    ‚îú‚îÄ‚îÄ CHANGELOG.md
    ‚îú‚îÄ‚îÄ clean_requirements.py
    ‚îú‚îÄ‚îÄ crew_basic_agent.py
    ‚îú‚îÄ‚îÄ demo_client.py
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ Dockerfile.llamacpp
    ‚îú‚îÄ‚îÄ EXAMPLES.md
    ‚îú‚îÄ‚îÄ find_files_with_phrase.py
    ‚îú‚îÄ‚îÄ find_networkx_versions.py
    ‚îú‚îÄ‚îÄ gitingest_agent_integration.py
    ‚îú‚îÄ‚îÄ gitingest_demo.py
    ‚îú‚îÄ‚îÄ GITINGEST_GUIDE.md
    ‚îú‚îÄ‚îÄ langchain_ollama_agent.py
    ‚îú‚îÄ‚îÄ langchain_ollama_agent222.py
    ‚îú‚îÄ‚îÄ langchain_ollama_agent333.py
    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ mistral_ollama_fastapi.py
    ‚îú‚îÄ‚îÄ multi_agent_requirements.txt
    ‚îú‚îÄ‚îÄ ollama_pydantic_agent.py
    ‚îú‚îÄ‚îÄ openai_rag_demo.py
    ‚îú‚îÄ‚îÄ openai_rag_requirements.txt
    ‚îú‚îÄ‚îÄ pydantic_agent_basic.py
    ‚îú‚îÄ‚îÄ pydantic_lmstudio_agent.py
    ‚îú‚îÄ‚îÄ rag_file_loader.py
    ‚îú‚îÄ‚îÄ rag_mistral_example.py
    ‚îú‚îÄ‚îÄ rag_requirements.txt
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îú‚îÄ‚îÄ requirements_clean.txt
    ‚îú‚îÄ‚îÄ requirements_wsl.txt
    ‚îú‚îÄ‚îÄ schemas.py
    ‚îú‚îÄ‚îÄ selenium1.py
    ‚îú‚îÄ‚îÄ selenium2.py
    ‚îú‚îÄ‚îÄ SETUP.md
    ‚îú‚îÄ‚îÄ simple_gitingest_examples.py
    ‚îú‚îÄ‚îÄ simple_rag.py
    ‚îú‚îÄ‚îÄ start_agents.sh
    ‚îú‚îÄ‚îÄ test.py
    ‚îú‚îÄ‚îÄ test_ollama.py
    ‚îú‚îÄ‚îÄ data/
    ‚îÇ   ‚îú‚îÄ‚îÄ cyclotruc-gitingest.txt
    ‚îÇ   ‚îú‚îÄ‚îÄ example.md
    ‚îÇ   ‚îú‚îÄ‚îÄ octocat-hello-world.txt
    ‚îÇ   ‚îú‚îÄ‚îÄ quantum_computing.txt
    ‚îÇ   ‚îú‚îÄ‚îÄ quantum_mechanics.txt
    ‚îÇ   ‚îî‚îÄ‚îÄ sample.txt
    ‚îî‚îÄ‚îÄ docs/
        ‚îú‚îÄ‚îÄ API.md
        ‚îî‚îÄ‚îÄ PROJECT_STRUCTURE.md

================================================
FILE: README.md
================================================
# 410c Mistral 7B Ollama PydanticAI

## Overview
Local AI development environment using Mistral 7B, Ollama, and PydanticAI for RAG (Retrieval-Augmented Generation) and multi-agent systems. This project demonstrates how to build a complete AI pipeline running entirely offline with local models, plus cloud model integration for comparison.

## What This Repository Contains
- **Local RAG System** with ChromaDB vector storage for document querying
- **Multi-Agent Architecture** with specialized Research and Report Writer agents
- **OpenAI Integration** for model comparison and performance benchmarking
- **GitIngest Integration** for automated GitHub repository analysis
- **Audio Processing** with transcription and summarization capabilities
- **Streaming Responses** optimized for slower hardware (USB SSD setups)

## Key Features
- ü§ñ **Multiple AI Models**: Mistral 7B, Phi3:mini, OpenAI GPT-3.5-turbo
- üîç **RAG Implementation**: Query your own documents with semantic search
- üë• **Multi-Agent System**: Collaborative AI agents with RESTful APIs
- üéØ **Performance Optimized**: Works on USB SSDs with appropriate timeouts
- üîÑ **Streaming Support**: Real-time response generation
- üìä **Model Comparison**: Local vs cloud model performance analysis

## Core Components

### RAG System
- **`rag_file_loader.py`** - Interactive RAG system with document indexing
- **`simple_rag.py`** - Lightweight streaming RAG implementation
- **`openai_rag_demo.py`** - OpenAI-powered RAG for comparison

### Multi-Agent System  
- **`agent1_research.py`** - Research agent using RAG for factual answers
- **`agent2_writer.py`** - Report writer that calls Agent1 for information
- **`demo_client.py`** - Client demonstrating agent interaction
- **`start_agents.sh`** - Startup script for both agents

### Repository Analysis
- **`gitingest_demo.py`** - Full-featured GitHub repository analyzer
- **`gitingest_agent_integration.py`** - Integration with multi-agent system
- **`simple_gitingest_examples.py`** - Basic GitIngest usage patterns

### Utilities
- **`audio_transcribe_summarize.py`** - Audio transcription with Whisper
- **`test_ollama.py`** - Ollama performance testing
- **`clean_requirements.py`** - Dependency management

## Quick Start

### Prerequisites
- WSL2 or Linux environment
- Python 3.11+
- Ollama installed
- 6GB+ free space for models

### Installation
1. **Clone and setup:**
   ```bash
   git clone https://github.com/terrytaylorbonn/410c_mistral7b_ollama_pydanticai
   cd 410c_mistral7b_ollama_pydanticai
   python -m venv .venvUV
   source .venvUV/bin/activate
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Install Ollama models:**
   ```bash
   ollama pull mistral
   ollama pull phi3:mini  
   ollama pull nomic-embed-text
   ```

### Usage Examples

#### RAG System
```bash
# Interactive document querying
python3 rag_file_loader.py

# Ask questions like:
# "What is quantum computing?"
# "Tell me about entanglement"
```

#### Multi-Agent System
```bash
# Terminal 1: Start agents
./start_agents.sh

# Terminal 2: Test interaction
python3 demo_client.py
```

#### OpenAI Comparison
```bash
# Set API key
export OPENAI_API_KEY="your-key-here"

# Compare with cloud models
python3 openai_rag_demo.py
```

#### Repository Analysis
```bash
# Analyze any GitHub repo
python3 gitingest_demo.py
```

## API Endpoints

### Agent1 (Research Assistant) - Port 8001
- **GET** `/health` - Health check
- **POST** `/research` - Answer questions using RAG
  ```json
  {"question": "What is quantum computing?", "max_sources": 2}
  ```

### Agent2 (Report Writer) - Port 8002  
- **GET** `/health` - Health check
- **POST** `/create_report` - Generate comprehensive reports
  ```json
  {
    "topic": "Quantum Computing Overview",
    "questions": ["What is quantum computing?", "What are qubits?"],
    "report_style": "executive_summary"
  }
  ```

- **Interactive Documentation:**
  - Agent1: http://localhost:8001/docs
  - Agent2: http://localhost:8002/docs

## Performance Notes

### USB SSD Optimization
This project is optimized for USB SSD setups (like WSL2 on external drives):
- Extended timeouts (120s for generation)
- Reduced retry attempts
- Shorter response limits
- Streaming support for better UX

### Model Performance
- **Mistral 7B**: ~1.7 tokens/second on USB SSD
- **Phi3:mini**: Faster alternative for testing
- **OpenAI**: Cloud comparison baseline

## Data Sources
Add documents to the `data/` folder for RAG queries:
- `quantum_computing.txt` - Quantum computing concepts
- `quantum_mechanics.txt` - Physics fundamentals  
- Your own documents (.txt files)

## Development Workflow
1. **Add Knowledge**: Place documents in `data/` folder
2. **Query Locally**: Use `rag_file_loader.py` for testing
3. **Generate Reports**: Use multi-agent system for comprehensive analysis
4. **Compare Models**: Test with OpenAI for quality benchmarking
5. **Analyze Repos**: Use GitIngest for codebase understanding

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Documents     ‚îÇ    ‚îÇ   ChromaDB      ‚îÇ    ‚îÇ   Local LLM     ‚îÇ
‚îÇ   (data/*.txt)  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   (Vectors)     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   (Ollama)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ   Agent2        ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Agent1        ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ   (Writer)      ‚îÇ    ‚îÇ   (Research)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                       ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Reports & Answers
```

## Contributing
This is a learning and experimentation repository. Feel free to:
- Add new document sources
- Experiment with different models
- Optimize for your hardware setup
- Create new agent types

## License
MIT License - See LICENSE file for details

## Acknowledgments
- **Ollama** for local LLM serving
- **ChromaDB** for vector storage
- **FastAPI** for agent APIs
- **OpenAI** for model comparison
- **GitIngest** for repository analysis



================================================
FILE: agent1_research.py
================================================
# agent1_research.py
"""
Agent1: Research Assistant
- Uses your existing RAG system
- Provides factual information from documents
- Runs as FastAPI service on port 8001
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import chromadb
import requests
from pathlib import Path
from typing import List, Dict

app = FastAPI(title="Agent1 - Research Assistant", version="1.0.0")

# Request/Response models
class ResearchRequest(BaseModel):
    question: str
    max_sources: int = 2

class ResearchResponse(BaseModel):
    question: str
    answer: str
    sources: List[str]
    agent: str = "Agent1-Research"

# RAG System (simplified from your rag_file_loader.py)
class SimpleRAG:
    def __init__(self, data_folder="./data", ollama_url="http://localhost:11434"):
        self.data_folder = Path(data_folder)
        self.ollama_url = ollama_url
        self.client = chromadb.Client()
        self.collection = None
        self.setup_knowledge_base()
    
    def setup_knowledge_base(self):
        """Load documents and create vector database"""
        print("Setting up Agent1 knowledge base...")
        
        self.collection = self.client.get_or_create_collection(
            name="agent1_docs",
            metadata={"description": "Agent1 Research Assistant"}
        )
        
        # Load documents
        documents = []
        for file_path in self.data_folder.glob("*.txt"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # Simple chunking
                    words = content.split()
                    chunk_size = 300
                    for i in range(0, len(words), chunk_size):
                        chunk = " ".join(words[i:i + chunk_size])
                        documents.append({
                            "id": f"{file_path.stem}_chunk_{i//chunk_size}",
                            "text": chunk,
                            "source": str(file_path)
                        })
                print(f"Loaded {file_path.name}")
            except Exception as e:
                print(f"Error loading {file_path}: {e}")
        
        if not documents:
            print("No documents found!")
            return
        
        # Get embeddings and add to ChromaDB
        embeddings = []
        texts = []
        ids = []
        metadatas = []
        
        for doc in documents:
            try:
                response = requests.post(
                    f"{self.ollama_url}/api/embeddings",
                    json={"model": "nomic-embed-text", "prompt": doc["text"]},
                    timeout=60
                )
                if response.status_code == 200:
                    embedding = response.json()["embedding"]
                    embeddings.append(embedding)
                    texts.append(doc["text"])
                    ids.append(doc["id"])
                    metadatas.append({"source": doc["source"]})
            except Exception as e:
                print(f"Error processing document: {e}")
                continue
        
        if embeddings:
            self.collection.add(
                embeddings=embeddings,
                documents=texts,
                ids=ids,
                metadatas=metadatas
            )
            print(f"Agent1 knowledge base ready with {len(embeddings)} documents")
    
    def research(self, question: str, max_sources: int = 2) -> Dict:
        """Research a question using RAG"""
        try:
            # Get query embedding
            response = requests.post(
                f"{self.ollama_url}/api/embeddings",
                json={"model": "nomic-embed-text", "prompt": question},
                timeout=60
            )
            
            if response.status_code != 200:
                raise Exception("Failed to get query embedding")
                
            query_embedding = response.json()["embedding"]
            
            # Search documents
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=max_sources,
                include=["documents", "metadatas"]
            )
            
            # Prepare context
            context = "\n\n".join([
                f"{doc[:400]}..." if len(doc) > 400 else doc
                for doc in results["documents"][0]
            ])
            
            # Generate answer
            prompt = f"""Answer this research question based on the provided context. Be factual and concise.

Context: {context}

Question: {question}

Answer:"""

            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": "phi3:mini",
                    "prompt": prompt,
                    "stream": False,
                    "options": {"num_predict": 80}
                },
                timeout=120
            )
            
            if response.status_code == 200:
                answer = response.json()["response"]
            else:
                answer = "Error generating answer"
            
            return {
                "question": question,
                "answer": answer,
                "sources": [meta["source"] for meta in results["metadatas"][0]]
            }
            
        except Exception as e:
            return {
                "question": question,
                "answer": f"Research error: {str(e)}",
                "sources": []
            }

# Initialize RAG system
rag = SimpleRAG()

@app.get("/")
async def root():
    return {"message": "Agent1 - Research Assistant", "status": "ready"}

@app.get("/health")
async def health():
    return {"status": "healthy", "agent": "Agent1-Research"}

@app.post("/research", response_model=ResearchResponse)
async def research(request: ResearchRequest):
    """Research a question using RAG system"""
    print(f"Agent1 received research request: {request.question}")
    
    result = rag.research(request.question, request.max_sources)
    
    return ResearchResponse(
        question=result["question"],
        answer=result["answer"],
        sources=result["sources"]
    )

if __name__ == "__main__":
    print("Starting Agent1 - Research Assistant on port 8001...")
    uvicorn.run(app, host="0.0.0.0", port=8001)



================================================
FILE: agent2_writer.py
================================================
# agent2_writer.py
"""
Agent2: Report Writer
- Calls Agent1 for research
- Formats information into reports
- Runs as FastAPI service on port 8002
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import httpx
import requests
from typing import List, Dict

app = FastAPI(title="Agent2 - Report Writer", version="1.0.0")

# Configuration
AGENT1_URL = "http://localhost:8001"
OLLAMA_URL = "http://localhost:11434"

# Request/Response models
class ReportRequest(BaseModel):
    topic: str
    questions: List[str]
    report_style: str = "executive_summary"  # or "detailed", "bullet_points"

class ReportResponse(BaseModel):
    topic: str
    report: str
    research_used: List[Dict]
    agent: str = "Agent2-Writer"

class Agent2Writer:
    def __init__(self):
        self.agent1_url = AGENT1_URL
        self.ollama_url = OLLAMA_URL
    
    async def call_agent1_research(self, question: str) -> Dict:
        """Call Agent1 for research"""
        try:
            async with httpx.AsyncClient(timeout=180.0) as client:  # 3 minute timeout
                response = await client.post(
                    f"{self.agent1_url}/research",
                    json={"question": question, "max_sources": 2}
                )
                if response.status_code == 200:
                    return response.json()
                else:
                    return {
                        "question": question,
                        "answer": f"Agent1 error: {response.status_code}",
                        "sources": []
                    }
        except Exception as e:
            return {
                "question": question,
                "answer": f"Failed to contact Agent1: {str(e)}",
                "sources": []
            }
    
    async def gather_research(self, questions: List[str]) -> List[Dict]:
        """Gather research from Agent1 for multiple questions"""
        research_results = []
        
        for question in questions:
            print(f"Agent2 asking Agent1: {question}")
            result = await self.call_agent1_research(question)
            research_results.append(result)
        
        return research_results
    
    def format_report(self, topic: str, research_data: List[Dict], style: str = "executive_summary") -> str:
        """Format research into a report using LLM"""
        
        # Prepare research context
        research_context = ""
        for i, research in enumerate(research_data, 1):
            research_context += f"\nQuestion {i}: {research['question']}\n"
            research_context += f"Answer: {research['answer']}\n"
            if research['sources']:
                research_context += f"Sources: {', '.join(research['sources'])}\n"
        
        # Create prompt based on style
        style_instructions = {
            "executive_summary": "Write a concise executive summary (2-3 paragraphs)",
            "detailed": "Write a detailed report with clear sections and explanations",
            "bullet_points": "Organize the information into clear bullet points"
        }
        
        instruction = style_instructions.get(style, style_instructions["executive_summary"])
        
        prompt = f"""Create a {style} report about "{topic}" based on the research below.

Research Data:
{research_context}

{instruction}. Be professional and well-structured.

Report:"""

        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": "phi3:mini",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_predict": 200,
                        "temperature": 0.3
                    }
                },
                timeout=120
            )
            
            if response.status_code == 200:
                return response.json()["response"]
            else:
                return f"Error formatting report: {response.status_code}"
                
        except Exception as e:
            return f"Report formatting error: {str(e)}"

# Initialize Agent2
writer = Agent2Writer()

@app.get("/")
async def root():
    return {"message": "Agent2 - Report Writer", "status": "ready"}

@app.get("/health")
async def health():
    return {"status": "healthy", "agent": "Agent2-Writer"}

@app.get("/check_agent1")
async def check_agent1():
    """Check if Agent1 is available"""
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{AGENT1_URL}/health")
            if response.status_code == 200:
                return {"agent1_status": "available", "response": response.json()}
            else:
                return {"agent1_status": "error", "code": response.status_code}
    except Exception as e:
        return {"agent1_status": "unavailable", "error": str(e)}

@app.post("/create_report", response_model=ReportResponse)
async def create_report(request: ReportRequest):
    """Create a report by gathering research from Agent1"""
    print(f"Agent2 creating report on: {request.topic}")
    
    # Gather research from Agent1
    research_results = await writer.gather_research(request.questions)
    
    # Format into report
    report = writer.format_report(
        request.topic, 
        research_results, 
        request.report_style
    )
    
    return ReportResponse(
        topic=request.topic,
        report=report,
        research_used=research_results
    )

if __name__ == "__main__":
    print("Starting Agent2 - Report Writer on port 8002...")
    uvicorn.run(app, host="0.0.0.0", port=8002)



================================================
FILE: agent_local.py
================================================
# agent_local.py

from langchain_community.chat_models import ChatOllama
from langchain.schema import SystemMessage, HumanMessage

# Use local Ollama model
llm = ChatOllama(base_url="http://localhost:11434", model="mistral")

messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="What is the capital of France?")
]

response = llm(messages)
print(response.content)



================================================
FILE: agent_print_files copy.py
================================================
# agent_print_files.py

import os
from langchain.agents import initialize_agent, Tool
from langchain.agents.agent_types import AgentType
from langchain_community.chat_models import ChatOllama
from langchain.tools import tool

# Set up the LLM
llm = ChatOllama(base_url="http://localhost:11434", model="mistral")

# Define a tool with a clear name for the agent
@tool(name="print_all_files", description="Returns the full contents of all .txt, .md, .py, or .json files in ./data directory.")
def print_all_files(dummy: str = "") -> str:
    """
    Returns the full contents of all .txt, .md, .py, or .json files in ./data directory.
    """
    directory = "./data"
    if not os.path.exists(directory):
        return f"Directory '{directory}' not found."

    results = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith((".txt", ".md", ".py", ".json")):
                path = os.path.join(root, file)
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        content = f.read()
                        results.append(f"\n--- {path} ---\n{content.strip()}\n")
                except Exception as e:
                    results.append(f"Error reading {path}: {e}")
    return "\n".join(results) if results else "No files found."

# Register tool
# Use the new Tool class for better compatibility
from langchain.tools import Tool as ToolClass

print_all_files_tool = ToolClass(
    name="print_all_files",
    func=print_all_files,
    description="Returns the full contents of all .txt, .md, .py, or .json files in ./data directory."
)

tools = [print_all_files_tool]

# Initialize agent
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    handle_parsing_errors=True,
)

# Use invoke instead of run (avoids deprecation warning)
response = agent.invoke({"input": "Print out the contents of all local files."})
print("\nüß† Agent Response:\n", response)



================================================
FILE: agent_print_files.py
================================================
# agent_print_files.py

import os
from langchain.agents import initialize_agent, Tool
from langchain.agents.agent_types import AgentType
from langchain_community.chat_models import ChatOllama

# Set up the LLM
llm = ChatOllama(base_url="http://localhost:11434", model="mistral")

# Define the tool function (no decorator)
def print_all_files(dummy: str = "") -> str:
    """
    Returns the full contents of all .txt, .md, .py, or .json files in ./data directory.
    """
    directory = "./data"
    if not os.path.exists(directory):
        return f"Directory '{directory}' not found."

    results = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith((".txt", ".md", ".py", ".json")):
                path = os.path.join(root, file)
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        content = f.read()
                        results.append(f"\n--- {path} ---\n{content.strip()}\n")
                except Exception as e:
                    results.append(f"Error reading {path}: {e}")
    return "\n".join(results) if results else "No files found."

# Register tool using the Tool class
from langchain.tools import Tool as ToolClass

print_all_files_tool = ToolClass(
    name="print_all_files",
    func=print_all_files,
    description="Returns the full contents of all .txt, .md, .py, or .json files in ./data directory."
)

tools = [print_all_files_tool]

# Initialize agent with a custom prompt prefix to enforce correct tool call format
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    handle_parsing_errors=True,
    agent_kwargs={
        "prefix": (
            "You are an agent that can use tools. "
            "When you want to use a tool, use the following format:\n"
            "Action: <tool_name>\n"
            "Action Input: <input>\n"
            "For example:\n"
            "Action: print_all_files\n"
            "Action Input: \"\"\n"
        )
    }
)

# Use invoke instead of run (avoids deprecation warning)
response = agent.invoke({"input": "Print out the contents of all local files."})

# Post-process the response to print only the final answer or file contents
if isinstance(response, dict):
    # If the agent returns a dict, try to extract the most relevant field
    final_output = response.get("output") or response.get("result") or str(response)
else:
    final_output = str(response)

print("\nüß† Agent Response:\n", final_output)



================================================
FILE: agent_print_files_langgraph.py
================================================



================================================
FILE: agent_print_files_langgraph_new.py
================================================
# agent_print_files_langgraph_new.py
"""
LangGraph version of the agent that prints all local file contents.
"""
import os
from langchain_ollama import ChatOllama
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import create_react_agent
from langchain.tools import Tool
from langchain.prompts import ChatPromptTemplate

# Set up the LLM
llm = ChatOllama(base_url="http://localhost:11434", model="mistral")

# Define the tool function
def print_all_files(dummy: str = "") -> str:
    print("[DEBUG] print_all_files tool called")
    directory = "./data"
    if not os.path.exists(directory):
        return f"Directory '{directory}' not found."
    results = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith((".txt", ".md", ".py", ".json")):
                path = os.path.join(root, file)
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        content = f.read()
                        results.append(f"\n--- {path} ---\n{content.strip()}\n")
                except Exception as e:
                    results.append(f"Error reading {path}: {e}")
    return "\n".join(results) if results else "No files found."

# Register the tool
print_all_files_tool = Tool(
    name="print_all_files",
    func=print_all_files,
    description="Returns the full contents of all .txt, .md, .py, or .json files in ./data directory."
)

tools = [print_all_files_tool]

# Remove custom prompt and use default for create_react_agent
react_agent = create_react_agent(llm, tools)

# Build the graph
workflow = StateGraph(input=dict, output=dict)
workflow.add_node("agent", react_agent)
workflow.set_entry_point("agent")
workflow.add_edge("agent", END)
app = workflow.compile()

# Run the graph
if __name__ == "__main__":
    # Directly call the tool and print its output
    print("\n[DIRECT TOOL CALL] Contents of ./data:")
    print(print_all_files())
    # Optionally, still run the agent for future debugging
    # result = app.invoke({"input": "Print out the contents of all local files."})
    # print("\n[DEBUG] Full agent result:", result)
    # if isinstance(result, dict):
    #     final_output = result.get("output") or result.get("result") or str(result)
    # else:
    #     final_output = str(result)
    # print("\nüß† Agent Response (LangGraph):\n", final_output)



================================================
FILE: agent_with_file_search.py.py
================================================
# agent_with_file_search.py

#GPT21


#GPT19

import os
from langchain.agents import initialize_agent, Tool
from langchain.agents.agent_types import AgentType
from langchain_community.chat_models import ChatOllama
from langchain.tools import tool

# Set up the LLM
llm = ChatOllama(base_url="http://localhost:11434", model="mistral")

# Define the tool

#GPT22

from langchain.tools import tool
import os
import json

@tool
def search_files(input_str: str) -> str:
    """
    Search for a keyword in files within a directory. 
    `input_str` should be a JSON string like:
    {"query": "quantum", "directory": "./data"}
    Returns full contents of matching files.
    """
    try:
        params = json.loads(input_str)
        query = params.get("query")
        directory = params.get("directory", "./data")
    except Exception as e:
        return f"Invalid input format. Expected JSON string. Error: {e}"

    results = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith((".txt", ".md", ".py", ".json")):
                path = os.path.join(root, file)
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        content = f.read()
                        if query.lower() in content.lower():
                            results.append(f"\n--- {path} ---\n{content.strip()}\n")
                except Exception as e:
                    results.append(f"Error reading {path}: {e}")
    return "\n".join(results) if results else "No matches found."


#GPT21
# from langchain.tools import tool
# import os

# @tool
# def search_files(query: str, directory: str = "./data") -> str:
#     """
#     Search for a keyword in text, markdown, python, or json files in a directory.
#     If found, print the full contents of those files.
#     """
#     results = []
#     for root, _, files in os.walk(directory):
#         for file in files:
#             if file.endswith((".txt", ".md", ".py", ".json")):
#                 path = os.path.join(root, file)
#                 try:
#                     with open(path, "r", encoding="utf-8") as f:
#                         content = f.read()
#                         if query.lower() in content.lower():
#                             results.append(f"\n--- {path} ---\n{content.strip()}\n")
#                 except Exception as e:
#                     results.append(f"Error reading {path}: {e}")
#     return "\n".join(results) if results else "No matches found."


# @tool
# def search_files(query: str) -> str:
#     """Searches for a keyword in all text, markdown, python, or json files in the ./data directory."""
#     target_dir = "./data"
#     if not os.path.exists(target_dir):
#         return f"Directory '{target_dir}' not found."

#     matches = []
#     for root, _, files in os.walk(target_dir):
#         for file in files:
#             if file.endswith((".txt", ".md", ".py", ".json")):
#                 try:
#                     path = os.path.join(root, file)
#                     with open(path, "r", encoding="utf-8") as f:
#                         for i, line in enumerate(f, start=1):
#                             if query.lower() in line.lower():
#                                 matches.append(f"{file}:{i}: {line.strip()}")
#                 except Exception as e:
#                     matches.append(f"Error reading {file}: {e}")
#     return "\n".join(matches) if matches else "No matches found."

# Define tools
tools = [search_files]

# Initialize agent with parsing error handling
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    handle_parsing_errors=True  # This line is essential!
)

# Run the agent
# response = agent.run("Search for the word 'quantum' in local files.")
# print("\nüß† Agent Response:\n", response)
response = agent.run('{"query": "quantum", "directory": "./data"}')
print(response)

# #GPT18
# from langchain.agents import initialize_agent, Tool
# from langchain.agents.agent_types import AgentType
# from langchain_community.chat_models import ChatOllama
# from langchain.tools import tool
# from langchain.schema import SystemMessage, HumanMessage

# import os

# # === STEP 1: LLM Setup ===
# llm = ChatOllama(base_url="http://localhost:11434", model="mistral")

# # === STEP 2: Tool Definition ===
# @tool
# def search_files(query: str) -> str:
#     """Search local .txt, .md, .py, or .json files in ./data/ for a keyword."""
#     matches = []
#     for root, _, files in os.walk("./data"):
#         for file in files:
#             if not file.lower().endswith((".txt", ".md", ".py", ".json")):
#                 continue
#             file_path = os.path.join(root, file)
#             try:
#                 with open(file_path, "r", encoding="utf-8") as f:
#                     for i, line in enumerate(f):
#                         if query.lower() in line.lower():
#                             matches.append(f"{file} (line {i+1}): {line.strip()}")
#             except Exception as e:
#                 matches.append(f"{file}: ERROR - {e}")
#     return "\n".join(matches) if matches else "No matches found."

# tools = [search_files]

# # === STEP 3: Agent Setup ===
# agent = initialize_agent(
#     tools,
#     llm,
#     agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
#     verbose=True
# )

# # === STEP 4: Optional Sample Files ===
# os.makedirs("data", exist_ok=True)
# with open("data/sample.txt", "w", encoding="utf-8") as f:
#     f.write("This is a test file.\nIt talks about quantum physics.\nEnd of file.")
# with open("data/example.md", "w", encoding="utf-8") as f:
#     f.write("# Example Markdown\nSearching for quantum entanglement.\n")

# # === STEP 5: Ask the Agent ===
# response = agent.run("Search for the word 'quantum' in local files.")
# print("\nüß† Agent Response:\n", response)



================================================
FILE: audio_transcribe_summarize.py
================================================
#audio_transcribe_summarize.py

#CPLT71

"""
Basic offline audio transcription and LLM summarization demo for WSL2.
- Records audio from your PC mic (via WSL2)
- Transcribes audio to text using Whisper (offline)
- Summarizes transcript with local LLM (Ollama/Mistral)
"""

# 1. Install dependencies (run these in WSL2 terminal):
# pip install sounddevice scipy openai-whisper requests
# sudo apt-get install -y portaudio19-dev ffmpeg

import sounddevice as sd
from scipy.io.wavfile import write
import whisper
import requests
import os

# --- Check available audio devices ---
def list_audio_devices():
    print("Available audio devices:")
    print(sd.query_devices())
    return sd.query_devices()

# --- Step 1: Record audio from mic ---
def record_audio(filename="output.wav", duration=10, fs=16000):
    print(f"Recording {duration} seconds of audio...")
    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)
    sd.wait()
    write(filename, fs, recording)
    print(f"Recording saved as {filename}")
    return filename

# --- Step 2: Transcribe with Whisper (offline) ---
def transcribe_audio(filename, model_size="base"):
    print(f"Transcribing {filename} with Whisper ({model_size})...")
    model = whisper.load_model(model_size)
    result = model.transcribe(filename)
    print("Transcript:", result["text"])
    return result["text"]

# --- Step 3: Summarize with local LLM (Ollama/Mistral) ---
def summarize_with_ollama(text, word_count=50):
    prompt = f"Summarize the following in about {word_count} words:\n\n{text}"
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": "mistral", "prompt": prompt},
        stream=True
    )
    summary = ""
    for line in response.iter_lines():
        if line:
            try:
                import json
                data = json.loads(line.decode("utf-8"))
                summary += data.get("response", "")
            except Exception:
                continue
    print("Summary:", summary)
    return summary
#audio_transcribe_summarize.py
#CPLT73
if __name__ == "__main__":
    # Check available audio devices
    devices = list_audio_devices()
    
    # Check if we have any audio devices
    if len(devices) == 0:
        print("No audio devices found. This is common in WSL2.")
        print("Options:")
        print("1. Use an existing audio file")
        print("2. Record audio on Windows and copy to WSL")
        print("3. Use WSL2 with audio support (requires additional setup)")
        
        # For demonstration, let's check if there's an existing audio file
        test_files = ["test.wav", "sample.wav", "audio.wav", "recording.wav"]
        found_file = None
        for file in test_files:
            if os.path.exists(file):
                found_file = file
                break
        
        if found_file:
            print(f"Found existing audio file: {found_file}")
            transcript = transcribe_audio(found_file)
            summarize_with_ollama(transcript, word_count=50)
        else:
            print("No existing audio files found. Please:")
            print("1. Record audio on Windows and save as 'test.wav' in this directory, or")
            print("2. Set up WSL2 with audio support")
            print("\nFor WSL2 audio setup, you need:")
            print("- WSL2 with GUI support")
            print("- PulseAudio or ALSA configuration")
            print("- Windows audio forwarding to WSL2")
    else:
        # Record audio if devices are available
        wav_file = record_audio(duration=10)
        # Transcribe
        transcript = transcribe_audio(wav_file)
        # Summarize
        summarize_with_ollama(transcript, word_count=50)



================================================
FILE: CHANGELOG.md
================================================
# Changelog

All notable changes to the Local RAG + Multi-Agent System project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Planned
- WebSocket support for real-time agent communication
- Advanced vector search with metadata filtering
- Docker containerization for easy deployment
- Batch processing capabilities for large document sets
- Integration with additional AI model providers
- Web UI for system management and queries

## [1.0.0] - 2024-01-XX

### Added
- **Core RAG System**
  - Local document indexing and vector search with ChromaDB
  - TF-IDF based embeddings for local operation
  - Support for multiple document formats (.txt, .md, .json)
  - Persistent vector storage with automatic reindexing

- **Multi-Agent Architecture**
  - Research Agent (port 8001) for information gathering
  - Writer Agent (port 8002) for content generation
  - RESTful API communication between agents
  - Health monitoring and status endpoints
  - Configurable model backends (Ollama integration)

- **Ollama Integration**
  - Native support for Mistral 7B and Phi-3 Mini models
  - Automatic model availability detection
  - Error handling and fallback mechanisms
  - Streaming response support for large outputs

- **OpenAI Integration**
  - Side-by-side comparison with local models
  - Cost tracking and token usage monitoring
  - Support for GPT-3.5-turbo and GPT-4 models
  - Quality assessment and evaluation metrics

- **GitIngest Integration**
  - Automated repository summarization via GitIngest.com
  - Manual and programmatic workflow support
  - Batch processing for multiple repositories
  - Integration with RAG system for codebase queries

- **Documentation and Examples**
  - Comprehensive README with setup instructions
  - Detailed API documentation
  - Step-by-step setup guide
  - Extensive usage examples and best practices
  - Project structure documentation

- **Scripts and Utilities**
  - `simple_rag.py` - Interactive RAG query interface
  - `start_agents.sh` - Multi-agent system launcher
  - `demo_client.py` - Agent system testing client
  - `test_ollama.py` - Ollama connectivity verification
  - Various helper scripts for data processing

### Technical Features
- **Error Handling**: Robust error handling with retry logic
- **Performance**: Optimized for slow hardware (USB SSD support)
- **Streaming**: Support for streaming responses from AI models
- **Timeouts**: Configurable timeouts for slow operations
- **Logging**: Comprehensive logging for debugging and monitoring
- **Modularity**: Clean separation of concerns and modular design

### Dependencies
- FastAPI 0.104.1+ for REST API framework
- Pydantic 2.5.0+ for data validation
- ChromaDB 0.4.18+ for vector storage
- OpenAI 1.3.7+ for OpenAI integration
- scikit-learn 1.3.2+ for ML utilities
- Requests 2.31.0+ for HTTP client functionality

### Supported Models
- **Local Models** (via Ollama):
  - Mistral 7B (primary)
  - Phi-3 Mini (lightweight alternative)
  - Support for additional Ollama models
- **Cloud Models** (via OpenAI API):
  - GPT-3.5-turbo
  - GPT-4 (when available)

### Supported Platforms
- Linux (Ubuntu 20.04+)
- macOS (Intel and Apple Silicon)
- Windows (via WSL2)
- Docker (planned)

## [0.9.0] - Development Phase

### Development History
- Initial project conception and architecture design
- Core RAG system prototype development
- Ollama integration and testing
- Multi-agent system design and implementation
- OpenAI integration for quality comparison
- GitIngest integration and workflow development
- Comprehensive testing on WSL2/Ubuntu environment
- Documentation creation and refinement

### Key Milestones
- ‚úÖ Local RAG system working with ChromaDB
- ‚úÖ Mistral 7B integration via Ollama
- ‚úÖ Multi-agent communication established
- ‚úÖ OpenAI comparison functionality
- ‚úÖ GitIngest workflow integration
- ‚úÖ Complete documentation suite
- ‚úÖ Error handling and robustness testing

## Known Issues

### Current Limitations
- GitIngest API occasionally returns 422 errors (web interface recommended)
- Large models require significant RAM (8GB+ recommended)
- First-time model downloads can be slow
- Vector database rebuilding required when adding many documents

### Workarounds
- Use GitIngest web interface for reliable repository processing
- Use phi3:mini model for lower resource environments
- Pre-download models during setup phase
- Implement incremental document indexing

## Migration Guide

### From Development to v1.0.0
No migration needed - this is the initial release.

### Future Migrations
Migration guides will be provided for breaking changes in future versions.

## Security Considerations

### Current Security Measures
- Environment variable protection for API keys
- Local-only operation by default
- No data transmission to external services (except OpenAI/GitIngest when explicitly used)
- .gitignore protection for sensitive files

### Security Best Practices
- Keep API keys secure and never commit to version control
- Regularly update dependencies for security patches
- Use environment variables for configuration
- Consider network security when deploying agents

## Performance Notes

### Optimization History
- Initial implementation focused on functionality
- Added streaming support for better user experience
- Implemented retry logic for reliability
- Optimized for USB SSD and slower hardware
- Added configurable timeouts and error handling

### Benchmark Results
- Local RAG queries: 1-5 seconds (depending on document size)
- Agent communication: 0.5-2 seconds
- Model inference: 2-10 seconds (varies by model and hardware)
- Document indexing: Proportional to document size and count

## Contributing

This changelog will be updated with each release. Contributors should:
1. Add entries to the [Unreleased] section
2. Follow the established format
3. Include breaking changes prominently
4. Reference issue numbers when applicable

## Release Process

1. Update version numbers in relevant files
2. Move [Unreleased] items to new version section
3. Update installation and setup documentation
4. Create release tag and publish
5. Update project README with new version info



================================================
FILE: clean_requirements.py
================================================
# GP38a

# clean_requirements.py

WINDOWS_ONLY_PACKAGES = {
    'pywin32', 'pypiwin32', 'winshell', 'colorama', 'win32-setctime',
    'wmi', 'comtypes', 'pyreadline', 'pywinauto'
}

output_file = "requirements_clean.txt"

with open("requirements.txt", "r") as infile, open(output_file, "w") as outfile:
    for line in infile:
        pkg = line.strip().split("==")[0].lower()
        if pkg in WINDOWS_ONLY_PACKAGES:
            print(f"‚ö†Ô∏è  Removing Windows-only package: {pkg}")
        else:
            outfile.write(line)

print(f"\n‚úÖ Cleaned requirements saved to: {output_file}")





================================================
FILE: crew_basic_agent.py
================================================
# crew_basic_agent.py

#GPT14

from langchain_community.chat_models import ChatOllama
from crewai import Agent, Task, Crew

# Define LLM
llm = ChatOllama(base_url="http://localhost:11434", model="mistral")

# Patch: Monkey-patch `.model_name` so CrewAI won't crash
llm.model_name = "ollama/mistral"  # CrewAI expects provider/model format

# Define Agent
agent = Agent(
    role="Research Assistant",
    goal="Answer simple factual questions",
    backstory="You're a helpful and precise assistant.",
    allow_delegation=False,
    llm=llm
)

# Define Task
task = Task(
    description="What is the capital of France?",
    expected_output="A one-sentence answer with the capital.",
    agent=agent
)

# Run Crew
crew = Crew(
    agents=[agent],
    tasks=[task],
    verbose=True
)

crew.kickoff()


# #GPT13
# from crewai import Agent, Task, Crew
# from langchain_community.chat_models import ChatOllama

# # ‚úÖ Use your offline Mistral7B via Ollama
# llm = ChatOllama(base_url="http://localhost:11434", model="mistral")

# # Define a basic agent
# agent = Agent(
#     role="Geography expert",
#     goal="Answer geography-related questions clearly",
#     backstory="You are a world-renowned geography professor helping students.",
#     allow_delegation=False,
#     verbose=True,
#     llm=llm
# )

# # Define a task the agent will perform
# task = Task(
#     description="What is the capital of France?",
#     agent=agent,
# )

# # Assemble the crew and run
# crew = Crew(
#     agents=[agent],
#     tasks=[task],
#     verbose=True
# )

# if __name__ == "__main__":
#     result = crew.run()
#     print("\nüîç Final Result:\n", result)



================================================
FILE: demo_client.py
================================================
# demo_client.py
"""
Demo client to test the multi-agent system
Shows how Agent2 calls Agent1 to create reports
"""

import requests
import json
import time

AGENT1_URL = "http://localhost:8001"
AGENT2_URL = "http://localhost:8002"

def test_agent1_direct():
    """Test Agent1 directly"""
    print("=== Testing Agent1 (Research Assistant) ===")
    
    response = requests.post(
        f"{AGENT1_URL}/research",
        json={"question": "What is quantum computing?"}
    )
    
    if response.status_code == 200:
        result = response.json()
        print(f"Question: {result['question']}")
        print(f"Answer: {result['answer']}")
        print(f"Sources: {result['sources']}")
    else:
        print(f"Error: {response.status_code}")

def test_agent2_report():
    """Test Agent2 creating a report (which calls Agent1)"""
    print("\n=== Testing Agent2 (Report Writer) ===")
    
    # Create a report request
    report_request = {
        "topic": "Quantum Computing Overview",
        "questions": [
            "What is quantum computing?",
            "What are qubits?",
            "How does quantum entanglement work?"
        ],
        "report_style": "executive_summary"
    }
    
    print("Agent2 creating report...")
    response = requests.post(
        f"{AGENT2_URL}/create_report",
        json=report_request
    )
    
    if response.status_code == 200:
        result = response.json()
        print(f"\nTopic: {result['topic']}")
        print(f"\nReport:\n{result['report']}")
        print(f"\nResearch Used:")
        for i, research in enumerate(result['research_used'], 1):
            print(f"  {i}. {research['question']}")
            print(f"     Sources: {research['sources']}")
    else:
        print(f"Error: {response.status_code}")

def check_agents_health():
    """Check if both agents are running"""
    print("=== Checking Agent Health ===")
    
    try:
        response1 = requests.get(f"{AGENT1_URL}/health", timeout=5)
        print(f"Agent1: {response1.json()}")
    except Exception as e:
        print(f"Agent1: Not available ({e})")
    
    try:
        response2 = requests.get(f"{AGENT2_URL}/health", timeout=5)
        print(f"Agent2: {response2.json()}")
    except Exception as e:
        print(f"Agent2: Not available ({e})")

if __name__ == "__main__":
    print("Multi-Agent System Demo")
    print("=" * 40)
    
    # Check if agents are running
    check_agents_health()
    
    # Test Agent1 directly
    test_agent1_direct()
    
    # Wait a moment
    time.sleep(2)
    
    # Test Agent2 (which calls Agent1)
    test_agent2_report()



================================================
FILE: Dockerfile
================================================
FROM ubuntu:24.04

# Install dependencies
RUN apt-get update && \
    apt-get install -y wget ca-certificates file && \
    rm -rf /var/lib/apt/lists/*

# Download llamafile
RUN wget https://github.com/Mozilla-Ocho/llamafile/releases/download/0.9.3/llamafile-0.9.3 && \
    chmod +x llamafile-0.9.3

# Download TinyLlama model (example: Q4_K_M quantization)
RUN wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -O tinyllama.gguf

# Set entrypoint to run llamafile with the model
ENTRYPOINT ["./llamafile-0.9.3", "-m", "tinyllama.gguf"]



================================================
FILE: Dockerfile.llamacpp
================================================
FROM ubuntu:24.04

# Install dependencies
RUN apt-get update && \
    apt-get install -y git build-essential cmake wget ca-certificates libcurl4-openssl-dev && \
    rm -rf /var/lib/apt/lists/*

# Clone llama.cpp
RUN git clone https://github.com/ggerganov/llama.cpp.git /llama.cpp
WORKDIR /llama.cpp

# Build llama.cpp using CMake
RUN cmake -B build . && cmake --build build --config Release -- -j $(nproc)

# Download TinyLlama model (example: Q4_K_M quantization)
WORKDIR /
RUN wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -O tinyllama.gguf

# Set entrypoint to run llama.cpp with the model
WORKDIR /llama.cpp
ENTRYPOINT ["./build/bin/llama-cli", "-m", "/tinyllama.gguf"]



================================================
FILE: EXAMPLES.md
================================================
# Usage Examples

This document provides comprehensive examples of how to use the Local RAG + Multi-Agent System.

## Table of Contents
1. [Basic RAG Queries](#basic-rag-queries)
2. [Multi-Agent Workflows](#multi-agent-workflows)
3. [GitIngest Integration](#gitingest-integration)
4. [OpenAI Comparison](#openai-comparison)
5. [Advanced Use Cases](#advanced-use-cases)

## Basic RAG Queries

### Simple Document Query
```python
# Run the simple RAG interface
python3 simple_rag.py

# Example interaction:
# Enter your query (or 'quit' to exit): What is quantum computing?
# Processing query...
# Answer: Quantum computing is a revolutionary approach to computation...
# Sources: quantum_computing.txt (Score: 0.85)
```

### Programmatic RAG Usage
```python
from rag_file_loader import load_and_query_documents

# Query documents programmatically
result = load_and_query_documents(
    query="Explain machine learning algorithms",
    data_dir="./data",
    top_k=3
)

print(f"Answer: {result['answer']}")
print(f"Sources found: {len(result['sources'])}")
for i, source in enumerate(result['sources']):
    print(f"{i+1}. {source['metadata']['source']} (Score: {source['score']:.2f})")
```

### Adding New Documents
```bash
# Add a new document to the RAG system
echo "Machine learning is a subset of artificial intelligence..." > data/ml_basics.txt

# The system will automatically index it on next query
python3 simple_rag.py
# Query: "What is machine learning?"
```

## Multi-Agent Workflows

### Starting the Agent System
```bash
# Start all agents
chmod +x start_agents.sh
./start_agents.sh

# You should see:
# Starting Research Agent on port 8001...
# Starting Writer Agent on port 8002...
# Both agents are now running!
```

### Research Agent Examples

#### Basic Research Query
```python
import requests

# Query the research agent
response = requests.post("http://localhost:8001/research", json={
    "query": "latest developments in quantum computing",
    "max_sources": 5,
    "include_technical": True
})

result = response.json()
print(f"Research Summary: {result['summary']}")
print(f"Confidence: {result['confidence']}")
```

#### Health Check
```bash
# Check agent status
curl http://localhost:8001/health

# Expected response:
# {"status": "healthy", "model": "mistral:7b", "uptime": "1h 23m"}
```

### Writer Agent Examples

#### Content Generation
```python
import requests

# Generate content based on research
response = requests.post("http://localhost:8002/write", json={
    "topic": "Quantum Computing Applications",
    "research_data": [
        {
            "source": "research_paper.pdf",
            "content": "Quantum computing shows promise in cryptography..."
        }
    ],
    "style": "technical",
    "length": "medium"
})

result = response.json()
print(f"Generated content ({result['word_count']} words):")
print(result['content'])
```

#### Content Editing
```python
# Edit existing content
response = requests.post("http://localhost:8002/edit", json={
    "content": "Quantum computing is good for stuff.",
    "instructions": "Make this more technical and detailed",
    "preserve_style": False
})

result = response.json()
print("Improved content:")
print(result['edited_content'])
print(f"Changes made: {result['changes_made']}")
```

### Complete Multi-Agent Workflow
```python
# demo_client.py example usage
python3 demo_client.py

# Follow the interactive prompts:
# 1. Enter research topic
# 2. System researches the topic
# 3. Writer generates content based on research
# 4. View final result
```

## GitIngest Integration

### Basic Repository Analysis
```python
# Use gitingest_demo.py
python3 gitingest_demo.py

# Example interaction:
# Enter GitHub repository URL: https://github.com/octocat/Hello-World
# Processing repository...
# Summary saved to: data/octocat-hello-world.txt
```

### Manual GitIngest Workflow
```bash
# 1. Visit GitIngest.com web interface
# 2. Enter repository URL: https://github.com/username/repo
# 3. Copy the generated summary
# 4. Save to data directory
echo "Repository summary content..." > data/my-repo-summary.txt

# 5. Query the RAG system about the repository
python3 simple_rag.py
# Query: "What is the main purpose of this repository?"
```

### Advanced GitIngest with Agent Integration
```python
# Use the advanced integration
python3 gitingest_agent_integration.py

# Features:
# - Automatic repository processing
# - Integration with RAG system
# - Agent-based analysis
# - Structured output
```

### Batch Repository Processing
```python
# Process multiple repositories
repositories = [
    "https://github.com/microsoft/vscode",
    "https://github.com/python/cpython",
    "https://github.com/tensorflow/tensorflow"
]

for repo_url in repositories:
    # Process each repository
    # Save to data/ directory
    # Add to RAG system
    pass
```

## OpenAI Comparison

### Basic OpenAI RAG Query
```python
# Ensure OPENAI_API_KEY is set
export OPENAI_API_KEY="your-api-key"

# Run OpenAI comparison
python3 openai_rag_demo.py

# Compare responses:
# Local Model: [Response from Mistral/Ollama]
# OpenAI Model: [Response from GPT-3.5/4]
# Cost: $0.0023
```

### Quality Comparison
```python
# Test the same query on both systems
query = "Explain the differences between classical and quantum algorithms"

# Local RAG
local_result = query_local_rag(query)

# OpenAI RAG  
openai_result = query_openai_rag(query)

# Compare results
print("Local Response:")
print(local_result['answer'])
print(f"Processing time: {local_result.get('processing_time', 'N/A')}")

print("\nOpenAI Response:")
print(openai_result['answer'])
print(f"Tokens used: {openai_result.get('tokens_used', 'N/A')}")
print(f"Estimated cost: ${openai_result.get('cost_estimate', 'N/A')}")
```

## Advanced Use Cases

### 1. Custom Document Processing
```python
# Add custom document processor
def process_code_files(directory):
    """Process programming files for RAG"""
    code_extensions = ['.py', '.js', '.java', '.cpp']
    
    for file_path in Path(directory).rglob('*'):
        if file_path.suffix in code_extensions:
            # Extract functions, classes, comments
            # Add to RAG system with metadata
            pass

# Usage
process_code_files("./my_project")
```

### 2. Domain-Specific RAG
```python
# Create domain-specific knowledge base
domains = {
    "medical": ["medical_papers.txt", "drug_interactions.txt"],
    "legal": ["contracts.txt", "case_law.txt"],
    "technical": ["api_docs.txt", "specifications.txt"]
}

# Query specific domain
result = query_domain_rag("medical", "What are the side effects of aspirin?")
```

### 3. Multi-Modal RAG
```python
# Process different file types
file_processors = {
    '.pdf': extract_pdf_text,
    '.docx': extract_word_text,
    '.md': extract_markdown,
    '.html': extract_html_text,
    '.json': extract_json_content
}

for file_path in data_directory:
    processor = file_processors.get(file_path.suffix)
    if processor:
        content = processor(file_path)
        add_to_rag_system(content, metadata={'source': file_path})
```

### 4. Real-time Data Integration
```python
# Integrate with live data sources
import schedule
import time

def update_rag_with_news():
    """Update RAG system with latest news"""
    # Fetch from news API
    # Process and add to RAG
    pass

def update_rag_with_docs():
    """Update with latest documentation"""
    # Check for new documents
    # Process and index
    pass

# Schedule updates
schedule.every(1).hours.do(update_rag_with_news)
schedule.every(24).hours.do(update_rag_with_docs)

while True:
    schedule.run_pending()
    time.sleep(1)
```

### 5. Agent Chain Workflows
```python
# Create complex multi-agent workflows
workflow = [
    ("research", {"query": "AI trends 2024", "depth": "comprehensive"}),
    ("analyze", {"focus": "business_impact", "metrics": ["adoption", "roi"]}),
    ("write", {"format": "executive_summary", "length": "2_pages"}),
    ("review", {"criteria": ["accuracy", "clarity", "completeness"]})
]

# Execute workflow
results = execute_agent_workflow(workflow)
```

### 6. Custom Evaluation Metrics
```python
def evaluate_rag_performance(test_queries, ground_truth):
    """Evaluate RAG system performance"""
    metrics = {
        'accuracy': [],
        'relevance': [],
        'response_time': [],
        'source_quality': []
    }
    
    for query, expected in zip(test_queries, ground_truth):
        result = query_rag_system(query)
        
        # Calculate metrics
        accuracy = calculate_accuracy(result, expected)
        relevance = calculate_relevance(result['sources'])
        
        metrics['accuracy'].append(accuracy)
        metrics['relevance'].append(relevance)
    
    return metrics

# Usage
test_queries = ["What is quantum computing?", "How does ML work?"]
ground_truth = ["Expected answer 1", "Expected answer 2"]
performance = evaluate_rag_performance(test_queries, ground_truth)
```

## Best Practices

### 1. Document Preparation
```bash
# Clean and organize documents
mkdir -p data/categories/{technical,business,research}

# Use descriptive filenames
mv document1.txt data/technical/api_documentation.txt
mv document2.txt data/business/market_analysis.txt
```

### 2. Query Optimization
```python
# Use specific, well-formed queries
good_query = "What are the key differences between supervised and unsupervised machine learning algorithms?"
poor_query = "ML stuff"

# Include context when needed
contextual_query = "In the context of natural language processing, how do transformer models work?"
```

### 3. Performance Monitoring
```python
# Monitor system performance
import time
import psutil

def monitor_query_performance(query):
    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss
    
    result = query_rag_system(query)
    
    end_time = time.time()
    end_memory = psutil.Process().memory_info().rss
    
    metrics = {
        'query_time': end_time - start_time,
        'memory_used': end_memory - start_memory,
        'result_quality': evaluate_result_quality(result)
    }
    
    return result, metrics
```

### 4. Error Handling
```python
# Robust error handling
def safe_rag_query(query, max_retries=3):
    for attempt in range(max_retries):
        try:
            result = query_rag_system(query)
            return result
        except ModelUnavailableError:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
                continue
            else:
                return {"error": "Model unavailable after retries"}
        except Exception as e:
            return {"error": f"Unexpected error: {str(e)}"}
```

## Common Workflows

### Research Paper Analysis
1. Upload research papers to `data/papers/`
2. Query specific concepts: "What are the main findings about X?"
3. Use multi-agent system to generate summaries
4. Compare with OpenAI for quality assessment

### Codebase Documentation
1. Use GitIngest to process repository
2. Save output to `data/codebases/`
3. Query architecture: "How is the authentication system implemented?"
4. Generate documentation with writer agent

### Competitive Analysis
1. Process competitor websites/docs
2. Research market trends
3. Generate comparative analysis
4. Create strategic recommendations

These examples should help you get started with the various features and capabilities of the system. Remember to check the API documentation in `docs/API.md` for detailed parameter information.



================================================
FILE: find_files_with_phrase.py
================================================
# find_files_with_phrase.py

# CPLT51
import os
import requests
import json

def find_files_with_phrase(directory, phrase):
    matches = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith((".txt", ".md", ".py", ".json")):
                path = os.path.join(root, file)
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        content = f.read()
                        if phrase.lower() in content.lower():
                            matches.append(path)
                except Exception as e:
                    print(f"Error reading {path}: {e}")
    return matches

def summarize_with_ollama(text, phrase, word_count):
    prompt = f"Summarize the following document in about {word_count} words, focusing on the topic: '{phrase}'.\n\n{text}"
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": "mistral", "prompt": prompt},
        stream=True
    )
    summary = ""
    for line in response.iter_lines():
        if line:
            try:
                data = json.loads(line.decode("utf-8"))
                summary += data.get("response", "")
            except Exception:
                continue
    return summary if summary else "[No summary returned]"

if __name__ == "__main__":
    search_phrase = input("Enter the word or phrase to search for: ")
    word_count = input("Enter the desired number of words for the summary (e.g., 50): ")
    try:
        word_count = int(word_count)
    except ValueError:
        word_count = 50
    directory = "./data"
    result = find_files_with_phrase(directory, search_phrase)
    if result:
        print("\nFiles containing the phrase:")
        for path in result:
            print(path)
        print("\nSummaries:")
        for path in result:
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()
            summary = summarize_with_ollama(content[:4000], search_phrase, word_count)  # Truncate if too long
            print(f"\n--- {path} ---\n{summary}\n")
    else:
        print("No files found containing the phrase.")



================================================
FILE: find_networkx_versions.py
================================================
# Temporary script to find the latest compatible networkx version for the current Python
import pkg_resources
import sys

try:
    import pip._internal as pip_internal
except ImportError:
    import pip as pip_internal

from subprocess import check_output

# Get all available versions of networkx
output = check_output([sys.executable, '-m', 'pip', 'index', 'versions', 'networkx'])
print(output.decode())



================================================
FILE: gitingest_agent_integration.py
================================================
# gitingest_agent_integration.py
"""
GitIngest + Multi-Agent Integration
Combines GitIngest repository analysis with your local LLM agents
"""

import requests
import json
from pathlib import Path

class GitIngestAgent:
    def __init__(self, agent1_url="http://localhost:8001", agent2_url="http://localhost:8002"):
        self.agent1_url = agent1_url
        self.agent2_url = agent2_url
        self.gitingest_api = "https://gitingest.com/api/ingest"
    
    def ingest_repository(self, github_url: str, save_to_data=True):
        """
        1. Use GitIngest to analyze a GitHub repository
        2. Save the analysis to your data folder for RAG
        3. Create a report using your multi-agent system
        """
        
        print(f"üîç Ingesting repository: {github_url}")
        
        # Step 1: Get repository analysis from GitIngest
        payload = {
            "url": github_url,
            "include_patterns": ["*.py", "*.md", "*.txt", "*.yml", "*.json"],
            "exclude_patterns": ["*.pyc", "__pycache__/*", "node_modules/*", ".git/*"]
        }
        
        try:
            response = requests.post(
                self.gitingest_api,
                json=payload,
                timeout=120
            )
            
            if response.status_code != 200:
                print(f"‚ùå GitIngest error: {response.status_code}")
                return None
                
            analysis = response.json()
            print("‚úÖ Repository analysis complete")
            
            # Step 2: Save to data folder for your RAG system
            if save_to_data:
                self.save_to_rag_data(analysis, github_url)
            
            # Step 3: Generate report using your agents
            report = self.create_code_analysis_report(analysis, github_url)
            
            return {
                "analysis": analysis,
                "report": report,
                "github_url": github_url
            }
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return None
    
    def save_to_rag_data(self, analysis: dict, github_url: str):
        """Save GitIngest analysis to your data folder for RAG queries"""
        
        # Create filename from repo URL
        repo_name = github_url.split("/")[-1].replace(".git", "")
        filename = f"data/gitingest_{repo_name}.txt"
        
        # Format for RAG consumption
        content = f"GitHub Repository Analysis: {github_url}\n"
        content += "=" * 60 + "\n\n"
        
        if "summary" in analysis:
            summary = analysis["summary"]
            content += f"Repository Summary:\n"
            content += f"- Total files: {summary.get('total_files', 'N/A')}\n"
            content += f"- Languages: {', '.join(summary.get('languages', []))}\n"
            content += f"- Total lines: {summary.get('total_lines', 'N/A')}\n\n"
        
        if "tree" in analysis:
            content += f"Project Structure:\n{analysis['tree']}\n\n"
        
        if "files" in analysis:
            content += "Key Files and Content:\n\n"
            for filepath, file_content in analysis["files"].items():
                content += f"File: {filepath}\n"
                content += "-" * 40 + "\n"
                content += file_content + "\n\n"
        
        # Save to data folder
        try:
            Path("data").mkdir(exist_ok=True)
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"üíæ Saved to RAG data: {filename}")
        except Exception as e:
            print(f"‚ùå Error saving to data folder: {e}")
    
    def create_code_analysis_report(self, analysis: dict, github_url: str):
        """Use Agent2 to create a comprehensive code analysis report"""
        
        print("ü§ñ Creating analysis report with Agent2...")
        
        # Prepare questions for Agent2 to research
        questions = [
            f"What is the purpose and main functionality of the repository {github_url}?",
            "What are the key technologies and frameworks used in this project?",
            "What is the overall architecture and structure of this codebase?",
            "What are the main components and how do they interact?",
            "What are the key files and their purposes?"
        ]
        
        report_request = {
            "topic": f"Code Analysis Report: {github_url}",
            "questions": questions,
            "report_style": "detailed"
        }
        
        try:
            response = requests.post(
                f"{self.agent2_url}/create_report",
                json=report_request,
                timeout=300  # 5 minutes for detailed analysis
            )
            
            if response.status_code == 200:
                result = response.json()
                print("‚úÖ Analysis report generated")
                
                # Save report
                repo_name = github_url.split("/")[-1]
                report_filename = f"analysis_report_{repo_name}.md"
                
                with open(report_filename, 'w', encoding='utf-8') as f:
                    f.write(f"# Code Analysis Report\n\n")
                    f.write(f"**Repository:** {github_url}\n")
                    f.write(f"**Generated:** {result.get('timestamp', 'N/A')}\n\n")
                    f.write(result["report"])
                    
                    f.write(f"\n\n## Research Sources\n")
                    for i, research in enumerate(result["research_used"], 1):
                        f.write(f"{i}. {research['question']}\n")
                
                print(f"üìù Report saved: {report_filename}")
                return result["report"]
                
            else:
                print(f"‚ùå Agent2 error: {response.status_code}")
                return "Error generating report"
                
        except Exception as e:
            print(f"‚ùå Error creating report: {e}")
            return f"Error: {e}"

def demo_workflow():
    """Demonstrate the complete GitIngest + Multi-Agent workflow"""
    
    print("üöÄ GITINGEST + MULTI-AGENT DEMO")
    print("=" * 50)
    print()
    
    # Initialize the integration
    agent = GitIngestAgent()
    
    # Example repositories
    repos = [
        "https://github.com/fastapi/fastapi",
        "https://github.com/streamlit/streamlit", 
        "https://github.com/pydantic/pydantic",
        "https://github.com/pallets/flask"
    ]
    
    print("Available repositories to analyze:")
    for i, repo in enumerate(repos, 1):
        print(f"{i}. {repo}")
    print()
    
    try:
        choice = input("Choose a repository (1-4) or enter a custom GitHub URL: ").strip()
        
        if choice.isdigit() and 1 <= int(choice) <= len(repos):
            repo_url = repos[int(choice) - 1]
        elif choice.startswith("https://github.com"):
            repo_url = choice
        else:
            repo_url = repos[0]  # Default
        
        print(f"\nüîÑ Processing: {repo_url}")
        print("This will:")
        print("1. Analyze the repository with GitIngest")
        print("2. Save analysis to your RAG data folder")
        print("3. Generate a detailed report using your agents")
        print("4. You can then query the repository using your RAG system")
        print()
        
        # Run the complete workflow
        result = agent.ingest_repository(repo_url)
        
        if result:
            print("\n‚úÖ WORKFLOW COMPLETE!")
            print("=" * 30)
            print("What you can do now:")
            print("1. Ask questions about this repository using rag_file_loader.py")
            print("2. Review the generated analysis report")
            print("3. The repository data is now in your local knowledge base")
            
    except KeyboardInterrupt:
        print("\nDemo cancelled.")
    except Exception as e:
        print(f"Error: {e}")

if __name__ == "__main__":
    demo_workflow()



================================================
FILE: gitingest_demo.py
================================================
# gitingest_demo.py
"""
GitIngest Getting Started Demo
https://gitingest.com/

GitIngest creates summaries of GitHub repositories that are perfect for:
- Code analysis with AI/LLMs
- Understanding project structure
- Getting quick repository overviews
- Preparing context for development tasks
"""

import requests
import json
import time
from urllib.parse import urlparse
import os

class GitIngestDemo:
    def __init__(self):
        self.base_url = "https://gitingest.com"
        self.api_url = f"{self.base_url}/api/ingest"
        
    def analyze_repo(self, github_url: str, include_patterns: list = None, exclude_patterns: list = None):
        """
        Analyze a GitHub repository using GitIngest
        
        Args:
            github_url: GitHub repository URL
            include_patterns: List of file patterns to include (e.g., ['*.py', '*.md'])
            exclude_patterns: List of file patterns to exclude (e.g., ['*.pyc', 'node_modules/*'])
        """
        
        print(f"Analyzing repository: {github_url}")
        print("-" * 50)
        
        # Prepare request payload
        payload = {
            "url": github_url,
            "include_patterns": include_patterns or [],
            "exclude_patterns": exclude_patterns or []
        }
        
        try:
            # Make request to GitIngest API
            response = requests.post(
                self.api_url,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                self.display_results(result, github_url)
                return result
            else:
                print(f"Error: {response.status_code}")
                print(f"Response: {response.text}")
                return None
                
        except Exception as e:
            print(f"Error analyzing repository: {e}")
            return None
    
    def display_results(self, result: dict, github_url: str):
        """Display the analysis results in a readable format"""
        
        print("üìä REPOSITORY ANALYSIS COMPLETE")
        print("=" * 50)
        
        # Basic info
        if "summary" in result:
            summary = result["summary"]
            print(f"Repository: {github_url}")
            print(f"Files analyzed: {summary.get('total_files', 'N/A')}")
            print(f"Total lines: {summary.get('total_lines', 'N/A')}")
            print(f"Languages: {', '.join(summary.get('languages', []))}")
            print()
        
        # File structure
        if "tree" in result:
            print("üìÅ PROJECT STRUCTURE:")
            print(result["tree"])
            print()
        
        # Key files
        if "files" in result:
            print("üìÑ KEY FILES CONTENT:")
            files = result["files"]
            for file_path, content in files.items():
                print(f"\n--- {file_path} ---")
                # Show first 500 characters
                preview = content[:500]
                if len(content) > 500:
                    preview += "... (truncated)"
                print(preview)
                print()
        
        # Save full results
        self.save_results(result, github_url)
    
    def save_results(self, result: dict, github_url: str):
        """Save results to a local file"""
        
        # Create filename from repo URL
        parsed = urlparse(github_url)
        repo_name = parsed.path.strip('/').replace('/', '_')
        filename = f"gitingest_{repo_name}_{int(time.time())}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            print(f"üíæ Full results saved to: {filename}")
        except Exception as e:
            print(f"Error saving results: {e}")
    
    def create_ai_prompt(self, result: dict, task: str = "analyze this codebase"):
        """Create a prompt suitable for AI analysis"""
        
        prompt = f"""Please {task} based on the following repository analysis:

REPOSITORY SUMMARY:
"""
        
        if "summary" in result:
            summary = result["summary"]
            prompt += f"- Total files: {summary.get('total_files', 'N/A')}\n"
            prompt += f"- Languages: {', '.join(summary.get('languages', []))}\n"
            prompt += f"- Total lines: {summary.get('total_lines', 'N/A')}\n\n"
        
        if "tree" in result:
            prompt += f"PROJECT STRUCTURE:\n{result['tree']}\n\n"
        
        if "files" in result:
            prompt += "KEY FILES CONTENT:\n"
            for file_path, content in result["files"].items():
                prompt += f"\n--- {file_path} ---\n"
                prompt += content + "\n"
        
        # Save prompt to file
        prompt_filename = f"ai_prompt_{int(time.time())}.txt"
        try:
            with open(prompt_filename, 'w', encoding='utf-8') as f:
                f.write(prompt)
            print(f"ü§ñ AI prompt saved to: {prompt_filename}")
        except Exception as e:
            print(f"Error saving prompt: {e}")
        
        return prompt

def demo_examples():
    """Run some example analyses"""
    
    demo = GitIngestDemo()
    
    print("üöÄ GITINGEST DEMO - GETTING STARTED")
    print("=" * 60)
    print()
    
    # Example repositories to analyze
    examples = [
        {
            "name": "Simple Python Project",
            "url": "https://github.com/psf/requests",
            "include": ["*.py", "*.md", "*.txt"],
            "exclude": ["*.pyc", "__pycache__/*", "tests/*"]
        },
        {
            "name": "JavaScript Library",
            "url": "https://github.com/lodash/lodash",
            "include": ["*.js", "*.json", "*.md"],
            "exclude": ["node_modules/*", "*.min.js", "test/*"]
        },
        {
            "name": "Documentation Project",
            "url": "https://github.com/microsoft/vscode-docs",
            "include": ["*.md", "*.json"],
            "exclude": ["node_modules/*", ".git/*"]
        }
    ]
    
    print("Available examples:")
    for i, example in enumerate(examples, 1):
        print(f"{i}. {example['name']} - {example['url']}")
    print()
    
    # Interactive selection
    try:
        choice = input("Choose an example (1-3) or enter a custom GitHub URL: ").strip()
        
        if choice.isdigit() and 1 <= int(choice) <= len(examples):
            example = examples[int(choice) - 1]
            print(f"\nAnalyzing: {example['name']}")
            result = demo.analyze_repo(
                example["url"],
                include_patterns=example["include"],
                exclude_patterns=example["exclude"]
            )
        elif choice.startswith("https://github.com"):
            print(f"\nAnalyzing custom repository: {choice}")
            result = demo.analyze_repo(choice)
        else:
            print("Invalid choice. Using default example...")
            example = examples[0]
            result = demo.analyze_repo(
                example["url"],
                include_patterns=example["include"],
                exclude_patterns=example["exclude"]
            )
        
        # Create AI-ready prompt
        if result:
            print("\n" + "=" * 50)
            task = input("What would you like to analyze about this codebase? (press Enter for general analysis): ").strip()
            if not task:
                task = "provide a comprehensive analysis of this codebase including its purpose, architecture, and key components"
            
            demo.create_ai_prompt(result, task)
            
    except KeyboardInterrupt:
        print("\nDemo cancelled.")
    except Exception as e:
        print(f"Error during demo: {e}")

def quick_analysis(repo_url: str):
    """Quick analysis function"""
    demo = GitIngestDemo()
    return demo.analyze_repo(repo_url)

if __name__ == "__main__":
    # Run the interactive demo
    demo_examples()



================================================
FILE: GITINGEST_GUIDE.md
================================================
# GitIngest Getting Started Guide

## What is GitIngest?

GitIngest (https://gitingest.com/) analyzes GitHub repositories and creates comprehensive summaries perfect for AI analysis. It extracts:
- Project structure
- Key file contents  
- Repository metadata
- Code snippets

## Quick Start (3 Methods)

### Method 1: Web Interface (Easiest)
1. Go to https://gitingest.com/
2. Paste GitHub URL (e.g., `https://github.com/python/requests`)
3. Click "Ingest Repository"
4. Copy the generated summary
5. Paste into your AI chat

### Method 2: Direct URL
```
https://gitingest.com/api/ingest?url=https://github.com/owner/repo
```

### Method 3: API Call (Python)
```python
import requests

response = requests.post(
    "https://gitingest.com/api/ingest",
    json={"url": "https://github.com/fastapi/fastapi"}
)

analysis = response.json()
```

## Common Filter Patterns

### Python Projects
```python
{
    "url": "https://github.com/owner/repo",
    "include_patterns": ["*.py", "*.md", "*.txt", "requirements.txt"],
    "exclude_patterns": ["*.pyc", "__pycache__/*", ".env", "venv/*"]
}
```

### JavaScript/Node.js
```python
{
    "url": "https://github.com/owner/repo", 
    "include_patterns": ["*.js", "*.ts", "*.json", "*.md"],
    "exclude_patterns": ["node_modules/*", "*.min.js", "dist/*"]
}
```

### Documentation Only
```python
{
    "url": "https://github.com/owner/repo",
    "include_patterns": ["*.md", "*.rst", "*.txt"],
    "exclude_patterns": [".git/*", "node_modules/*"]
}
```

## Integration with Your Multi-Agent System

1. **Run GitIngest** on a repository
2. **Save output** to your `data/` folder  
3. **Query with RAG** using your existing system
4. **Generate reports** with Agent2

## Example Workflow

```bash
# 1. Analyze repository and add to knowledge base
python3 gitingest_agent_integration.py

# 2. Query the repository using your RAG system
python3 rag_file_loader.py

# 3. Ask questions like:
# "What does this repository do?"
# "How is the code structured?" 
# "What are the main components?"
```

## Use Cases

- **Code Review**: Understand unfamiliar codebases
- **Learning**: Study how popular projects are structured
- **Documentation**: Generate project overviews
- **AI Analysis**: Prepare repositories for LLM analysis
- **Research**: Compare different implementations

## Best Practices

1. **Filter appropriately** - Don't include huge files or binaries
2. **Focus on key files** - README, main source files, configs
3. **Exclude noise** - Tests, logs, build artifacts
4. **Save for reuse** - Cache analyses locally
5. **Combine with AI** - Use output as context for deeper analysis

## Example Questions to Ask Your RAG System After GitIngest

- "What is the architecture of this project?"
- "How do I get started with this codebase?"
- "What are the main dependencies?"
- "How is error handling implemented?"
- "What are the key design patterns used?"

## Files Created by This Demo

- `gitingest_demo.py` - Full-featured demo with interactive examples
- `simple_gitingest_examples.py` - Basic usage patterns
- `gitingest_agent_integration.py` - Integration with your multi-agent system

## Try It Now!

```bash
# Simple example
python3 simple_gitingest_examples.py

# Full demo
python3 gitingest_demo.py

# Multi-agent integration
python3 gitingest_agent_integration.py
```



================================================
FILE: langchain_ollama_agent.py
================================================
# langchain_ollama_agent.py

from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

# ‚úÖ Connect to Ollama (make sure it's running)
llm = ChatOpenAI(
    base_url="http://localhost:11434/v1",  # Ollama's OpenAI-compatible endpoint
    api_key="ollama",                      # Can be anything for Ollama
    model_name="mistral"                   # Or "mistral:7b-instruct" if applicable
)

# üß™ Run a simple prompt
response = llm([HumanMessage(content="What is the capital of France?")])
print(response.content)



================================================
FILE: langchain_ollama_agent222.py
================================================
# langchain_ollama_agent222.py (v2, clean)

from langchain_core.messages import HumanMessage
# from langchain_community.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI

# Connect to Ollama (must be running mistral)
llm = ChatOpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama",
    model_name="mistral"
)

# Run a prompt
response = llm.invoke([HumanMessage(content="What is the capital of France?")])
print(response.content)


# from langchain.chat_models import ChatOpenAI
# from langchain.schema import HumanMessage

# # ‚úÖ Connect to Ollama (make sure it's running)
# llm = ChatOpenAI(
#     base_url="http://localhost:11434/v1",  # Ollama's OpenAI-compatible endpoint
#     api_key="ollama",                      # Can be anything for Ollama
#     model_name="mistral"                   # Or "mistral:7b-instruct" if applicable
# )

# # üß™ Run a simple prompt
# response = llm([HumanMessage(content="What is the capital of France?")])
# print(response.content)



================================================
FILE: langchain_ollama_agent333.py
================================================
# langchain_ollama_agent333.py

try:
    import torch
    if torch.cuda.is_available():
        print(f"GPU is available: {torch.cuda.get_device_name(torch.cuda.current_device())}")
    else:
        print("GPU is NOT available. Running on CPU.")
except ImportError:
    print("PyTorch is not installed. Cannot check GPU status.")

from langchain_core.messages import HumanMessage
# from langchain_community.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI

# Connect to Ollama (must be running mistral)
llm = ChatOpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama",
    model_name="mistral"
)

# Run a prompt
response = llm.invoke([HumanMessage(content="What is the capital of France?")])
print(response.content)


# from langchain.chat_models import ChatOpenAI
# from langchain.schema import HumanMessage

# # ‚úÖ Connect to Ollama (make sure it's running)
# llm = ChatOpenAI(
#     base_url="http://localhost:11434/v1",  # Ollama's OpenAI-compatible endpoint
#     api_key="ollama",                      # Can be anything for Ollama
#     model_name="mistral"                   # Or "mistral:7b-instruct" if applicable
# )

# # üß™ Run a simple prompt
# response = llm([HumanMessage(content="What is the capital of France?")])
# print(response.content)



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 Local RAG + Multi-Agent System

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

## Third-Party Licenses

This project uses several third-party libraries and services:

### Ollama
- License: MIT License
- Website: https://ollama.com
- Used for: Local AI model hosting and inference

### ChromaDB
- License: Apache License 2.0
- Website: https://www.trychroma.com
- Used for: Vector database and embeddings storage

### FastAPI
- License: MIT License
- Website: https://fastapi.tiangolo.com
- Used for: REST API framework for multi-agent communication

### Pydantic
- License: MIT License
- Website: https://pydantic-docs.helpmanual.io
- Used for: Data validation and settings management

### OpenAI Python Library
- License: Apache License 2.0
- Website: https://github.com/openai/openai-python
- Used for: OpenAI API integration (optional)

### scikit-learn
- License: BSD-3-Clause License
- Website: https://scikit-learn.org
- Used for: Machine learning utilities and TF-IDF embeddings

### GitIngest.com
- Service: Third-party web service
- Website: https://gitingest.com
- Used for: Repository summarization (external service)

## Model Licenses

### Mistral 7B
- License: Apache License 2.0
- Provider: Mistral AI
- Usage: Local inference via Ollama

### Phi-3 Mini
- License: MIT License
- Provider: Microsoft
- Usage: Local inference via Ollama

## Data and Content

Sample data files in the `data/` directory are provided for testing and demonstration purposes only. Users should replace with their own content for production use.

## Disclaimer

This software is provided for educational and research purposes. Users are responsible for:
- Complying with applicable laws and regulations
- Respecting intellectual property rights
- Following terms of service for external APIs and services
- Ensuring appropriate use of AI models and generated content

The authors make no warranties about the accuracy, completeness, or suitability of the software for any particular purpose.



================================================
FILE: mistral_ollama_fastapi.py
================================================
#mistral_ollama_fastapi.py

#GPT72

from fastapi import FastAPI, Request
from pydantic import BaseModel
import httpx
import traceback

app = FastAPI()

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: list[Message]

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "http://localhost:11434/api/chat",
                json={
                    "model": request.model,
                    "messages": [msg.dict() for msg in request.messages],
                    "stream": False
                }
            )
            response.raise_for_status()
            return {
                "id": "chatcmpl-244",
                "object": "chat.completion",
                "created": 1750005646,
                "model": request.model,
                "system_fingerprint": "fp_ollama",
                "choices": [{
                    "index": 0,
                    "message": response.json()["message"],
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": 12,
                    "completion_tokens": 8,
                    "total_tokens": 20
                }
            }
    except Exception as e:
        print("Exception occurred:")
        traceback.print_exc()
        return {
            "error": str(e),
            "trace": traceback.format_exc()
        }



#GPT 71

# from fastapi import FastAPI
# from pydantic import BaseModel
# import httpx

# app = FastAPI()

# class Message(BaseModel):
#     role: str
#     content: str

# class ChatRequest(BaseModel):
#     model: str
#     messages: list[Message]

# @app.post("/v1/chat/completions")
# async def chat_completions(request: ChatRequest):
#     try:
#         async with httpx.AsyncClient() as client:
#             response = await client.post(
#                 "http://localhost:11434/api/chat",
#                 json={
#                     "model": request.model,
#                     "messages": [msg.dict() for msg in request.messages],
#                     "stream": False   # üî• KEY FIX: disable streaming
#                 }
#             )
#             response.raise_for_status()
#             return response.json()
#     except httpx.HTTPError as e:
#         return {"error": str(e), "details": e.response.text if e.response else None}


# from fastapi import FastAPI, Request
# from fastapi.responses import JSONResponse
# import httpx

# app = FastAPI()

# OLLAMA_API_URL = "http://localhost:11434/v1/chat/completions"

# @app.get("/")
# def root():
#     return {"message": "Ollama proxy running (OpenAI-compatible style)"}

# @app.post("/v1/chat/completions")
# async def proxy_chat(request: Request):
#     try:
#         payload = await request.json()

#         async with httpx.AsyncClient() as client:
#             response = await client.post(OLLAMA_API_URL, json=payload)

#         return JSONResponse(status_code=response.status_code, content=response.json())

#     except Exception as e:
#         return JSONResponse(status_code=500, content={"error": str(e)})



================================================
FILE: multi_agent_requirements.txt
================================================
# Multi-agent requirements
fastapi>=0.104.0
uvicorn>=0.24.0
httpx>=0.25.0
chromadb>=0.4.0
requests>=2.28.0



================================================
FILE: ollama_pydantic_agent.py
================================================
# ollama_pydantic_agent.py

#GPT23

from pydantic_ai import Agent
from pydantic_ai.providers.openai import OpenAIProvider
import asyncio

#GPT24
agent = Agent(
    model="openai:mistral",  # No colons or version tag
    provider=OpenAIProvider(
        api_key="ollama",
        base_url="http://localhost:11434/v1"
    )
)


# agent = Agent(
#     model="openai:mistral:7b-instruct",
#     provider=OpenAIProvider(
#         api_key="ollama",  # dummy key (required but not used by Ollama)
#         base_url="http://localhost:11434/v1"
#     )
# )

async def main():
    response = await agent.run("What is the capital of France?")
    print(response.output)

asyncio.run(main())


# #GPT20

# import asyncio
# from pydantic_ai import Agent
# from pydantic_ai.providers.openai import OpenAIProvider

# agent = Agent(
#     model="openai:mistral",  # 'openai:' prefix is needed even for Ollama
#     provider=OpenAIProvider(
#         api_key="ollama",  # Required, even if not used
#         base_url="http://localhost:11434/v1",  # Ollama's OpenAI-compatible endpoint
#     )
# )

# async def main():
#     response = await agent.run("What is the capital of France?")
#     print(response.output)

# asyncio.run(main())



#GPT 18

# from pydantic_ai import Agent
# from pydantic_ai.providers.openai import OpenAIProvider

# agent = Agent(
#     model="openai:mistral",  # Provider prefix required
#     provider=OpenAIProvider(
#         api_key="ollama",  # dummy key
#         base_url="http://localhost:11434/v1"
#     )
# )

# response = agent.run("What is the capital of France?")
# print(response.output)


# response = agent.chat("What is the capital of France?")
# print(response.content)



# GPT17
# from pydantic_ai import Agent
# from pydantic_ai.providers.openai import OpenAIProvider

# # Create the agent
# agent = Agent(
#     model="mistral",  # Name of your model (used by Ollama)
#     provider=OpenAIProvider(
#         api_key="ollama",  # Required but not validated for local Ollama
#         base_url="http://localhost:11434/v1"
#     )
# )

# # Send a prompt
# response = agent.chat("What is the capital of Canada?")
# print(response.content)




# GPT16

# from pydantic_ai import Agent
# from pydantic_ai.providers.openai import OpenAIProvider

# # Define your Ollama-based agent
# agent = Agent(
#     model="mistral",  # Just the model name as a string now
#     provider=OpenAIProvider(
#         api_key="ollama",  # Required field, but not used by Ollama
#         base_url="http://localhost:11434/v1",  # Ollama OpenAI-compatible endpoint
#         model="mistral"
#     )
# )

# # Make a test call
# response = agent.chat("What is the capital of Canada?")
# print(response.content)



# # GPT15

# from pydantic_ai import Agent
# from pydantic_ai.providers.openai import OpenAIProvider
# from pydantic_ai.models import KnownModelName

# # Use Ollama's OpenAI-compatible API
# agent = Agent(
#     model=KnownModelName.OPENAI,  # Required if you want structured output later
#     provider=OpenAIProvider(
#         api_key="ollama",  # Required by OpenAI spec, ignored by Ollama
#         base_url="http://localhost:11434/v1",  # Ollama's OpenAI endpoint
#         model="mistral"  # Ollama model name
#     )
# )

# # Simple chat test
# response = agent.chat("What is the capital of Canada?")
# print(response.content)




### pre GPT15
# from pydantic import BaseModel
# from pydantic_ai import Agent
# # from pydantic_ai.models import OpenAIAgentSettings
# from pydantic_ai.models.openai import OpenAIAgentSettings #GPT5
# import os


# # Pydantic model for structured output
# class CityInfo(BaseModel):
#     city: str
#     country: str
#     population: str | None = None


# # Settings for local Ollama using OpenAI-compatible API
# settings = OpenAIAgentSettings(
#     provider="openai",
#     model="mistral",
#     base_url="http://localhost:11434/v1",
#     api_key="ollama",  # not validated
# )

# agent = Agent(settings=settings)

# # User query
# query = "What is the population of Tokyo and which country is it in?"

# # Run agent with output model
# response = agent.run(query, output_model=CityInfo)

# print("\nüß† Structured Response:")
# print(response)



================================================
FILE: openai_rag_demo.py
================================================
# openai_rag_demo.py
"""
Simple OpenAI-powered RAG demo
Compare with local model performance
"""

import os
from openai import OpenAI
from pathlib import Path
import json
from typing import List, Dict
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class OpenAIRAG:
    def __init__(self, api_key: str = None):
        """Initialize with OpenAI API key"""
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OpenAI API key required. Set OPENAI_API_KEY environment variable or pass api_key parameter.")
        
        # Set up OpenAI client (new format)
        self.client = OpenAI(api_key=self.api_key)
        
        # Simple document storage
        self.documents = []
        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
        self.doc_vectors = None
        
    def load_documents(self, data_folder: str = "./data"):
        """Load all text files from data folder"""
        data_path = Path(data_folder)
        self.documents = []
        
        print(f"Loading documents from {data_path}...")
        
        for txt_file in data_path.glob("*.txt"):
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # Simple chunking - split by paragraphs
                chunks = content.split('\n\n')
                
                for i, chunk in enumerate(chunks):
                    if len(chunk.strip()) > 50:  # Skip very short chunks
                        self.documents.append({
                            'content': chunk.strip(),
                            'source': txt_file.name,
                            'chunk_id': f"{txt_file.stem}_{i}"
                        })
                
                print(f"  Loaded {txt_file.name}: {len([c for c in chunks if len(c.strip()) > 50])} chunks")
                
            except Exception as e:
                print(f"  Error loading {txt_file}: {e}")
        
        if self.documents:
            # Create TF-IDF vectors for all documents
            doc_texts = [doc['content'] for doc in self.documents]
            self.doc_vectors = self.vectorizer.fit_transform(doc_texts)
            print(f"\nTotal documents loaded: {len(self.documents)}")
        else:
            print("No documents found!")
    
    def search_documents(self, query: str, top_k: int = 3) -> List[Dict]:
        """Find most relevant documents using TF-IDF similarity"""
        if not self.documents or self.doc_vectors is None:
            return []
        
        # Vectorize the query
        query_vector = self.vectorizer.transform([query])
        
        # Calculate cosine similarity
        similarities = cosine_similarity(query_vector, self.doc_vectors).flatten()
        
        # Get top-k most similar documents
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:  # Only include documents with some similarity
                results.append({
                    'content': self.documents[idx]['content'],
                    'source': self.documents[idx]['source'],
                    'chunk_id': self.documents[idx]['chunk_id'],
                    'similarity': float(similarities[idx])
                })
        
        return results
    
    def ask_openai(self, question: str, context_docs: List[Dict]) -> str:
        """Ask OpenAI using retrieved context"""
        
        # Build context from retrieved documents
        context = ""
        for i, doc in enumerate(context_docs, 1):
            context += f"Source {i} ({doc['source']}):\n{doc['content']}\n\n"
        
        # Create prompt
        prompt = f"""Based on the following context documents, answer the question. Be accurate and cite which sources you're using.

Context:
{context}

Question: {question}

Answer:"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",  # or "gpt-4" if you have access
                messages=[
                    {"role": "system", "content": "You are a helpful assistant that answers questions based on provided context. Always cite your sources."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=300,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return f"OpenAI API Error: {e}"
    
    def answer_question(self, question: str) -> Dict:
        """Complete RAG pipeline: search + generate answer"""
        print(f"\nQuestion: {question}")
        print("-" * 50)
        
        # Step 1: Retrieve relevant documents
        print("üîç Searching for relevant documents...")
        relevant_docs = self.search_documents(question, top_k=3)
        
        if not relevant_docs:
            return {
                "question": question,
                "answer": "No relevant documents found.",
                "sources": [],
                "context_used": []
            }
        
        print(f"Found {len(relevant_docs)} relevant documents:")
        for i, doc in enumerate(relevant_docs, 1):
            print(f"  {i}. {doc['source']} (similarity: {doc['similarity']:.3f})")
        
        # Step 2: Generate answer with OpenAI
        print("\nü§ñ Generating answer with OpenAI...")
        answer = self.ask_openai(question, relevant_docs)
        
        return {
            "question": question,
            "answer": answer,
            "sources": [doc['source'] for doc in relevant_docs],
            "context_used": relevant_docs
        }

def main():
    """Interactive demo"""
    print("üöÄ OPENAI RAG DEMO")
    print("=" * 40)
    
    # Get API key
    api_key = input("Enter your OpenAI API key (or press Enter if set in environment): ").strip()
    if not api_key:
        api_key = os.getenv('OPENAI_API_KEY')
    
    if not api_key:
        print("‚ùå OpenAI API key required!")
        print("Set OPENAI_API_KEY environment variable or enter it when prompted.")
        return
    
    try:
        # Initialize RAG system
        rag = OpenAIRAG(api_key=api_key)
        
        # Load documents
        rag.load_documents("./data")
        
        if not rag.documents:
            print("‚ùå No documents found in ./data folder!")
            return
        
        print("\n" + "=" * 50)
        print("OpenAI RAG System Ready!")
        print("Type 'quit' to exit.")
        print("=" * 50)
        
        # Interactive Q&A
        while True:
            question = input("\nYour question: ").strip()
            
            if question.lower() in ['quit', 'exit', 'q']:
                break
                
            if question:
                result = rag.answer_question(question)
                
                print(f"\n‚úÖ Answer:")
                print(result['answer'])
                
                print(f"\nüìö Sources used:")
                for source in set(result['sources']):
                    print(f"  - {source}")
                
                # Show context details
                print(f"\nüîç Context details:")
                for i, doc in enumerate(result['context_used'], 1):
                    print(f"  {i}. {doc['source']} (similarity: {doc['similarity']:.3f})")
                    print(f"     Preview: {doc['content'][:100]}...")
                    print()
        
    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    main()



================================================
FILE: openai_rag_requirements.txt
================================================
# OpenAI RAG Requirements
openai>=0.28.0
scikit-learn>=1.3.0
numpy>=1.24.0



================================================
FILE: pydantic_agent_basic.py
================================================
# pydantic_agent_basic.py

#GPT7

import asyncio
from pydantic import BaseModel
from pydantic_ai import Agent
from pydantic_ai.models import OllamaModel

# Define a structured task schema
class Task(BaseModel):
    title: str
    priority: str
    due: str

# Use the local Ollama server
agent = Agent(
    model=OllamaModel(model_name="mistral", base_url="http://localhost:11434")
)

async def main():
    task = await agent.run(
        schema=Task,
        input="Schedule a task to review documents tomorrow. Make it high priority.",
    )
    print(task)

if __name__ == "__main__":
    asyncio.run(main())

# #GPT5

# import asyncio
# from pydantic import BaseModel
# from pydantic_ai import Agent
# from pydantic_ai.providers.openai import OpenAIProvider

# # Define the structured schema
# class Task(BaseModel):
#     title: str
#     priority: str
#     due: str

# # Create the agent
# agent = Agent(
#     model="openai:mistral",  # Pydantic AI uses this format for Ollama
#     provider=OpenAIProvider(
#         api_key="ollama",
#         base_url="http://localhost:11434/v1"
#     )
# )

# # Async entry point
# async def main():
#     task = await agent.run(
#         schema=Task,
#         input="Create a high-priority task to review documents, due tomorrow."
#     )
#     print(task)

# # Run the agent
# if __name__ == "__main__":
#     asyncio.run(main())


# # GPT3
# import asyncio
# from pydantic_ai import Agent
# from schemas import Task

# agent = Agent(model="openai:mistral")  # Ollama compatible

# async def main():
#     task = await agent.run(
#         Task,
#         "Create a high-priority task to review documents, due tomorrow."
#     )
#     print(task)

# asyncio.run(main())


# from pydantic_ai import Agent
# from pydantic_ai.providers.openai import OpenAIProvider
# from schemas import Task

# agent = Agent(
#     model="openai:mistral",
#     provider=OpenAIProvider(
#         api_key="ollama",
#         base_url="http://localhost:8000/v1"
#     )
# )

# task = agent.complete(
#     Task,
#     "Create a high-priority task to review documents, due tomorrow."
# )

# print(task)



================================================
FILE: pydantic_lmstudio_agent.py
================================================
from pydantic_ai import Agent
import os
import asyncio

# Patch: configure OpenAI client via environment vars
os.environ["OPENAI_API_KEY"] = "not-needed"  # LM Studio ignores this
os.environ["OPENAI_BASE_URL"] = "http://localhost:1234/v1"  # LM Studio base URL

async def main():
    agent = Agent(model="openai:mistral-7b-instruct-v0.1")  # Must have "openai:" prefix
    result = await agent.run("What is the capital of France?")
    print(result.output)

asyncio.run(main())


================================================
FILE: rag_file_loader.py
================================================
# rag_file_loader.py
"""
RAG system that loads documents from your data/ folder
Works with your existing Mistral 7B setup

NOTE: Optimized for USB SSD setup - responses may take 1-2 minutes
For faster responses, consider using simple_rag.py with streaming
"""

import os
import requests
import chromadb
from pathlib import Path
from typing import List, Dict
import re

class DocumentRAG:
    def __init__(self, data_folder="./data", ollama_url="http://localhost:11434"):
        self.data_folder = Path(data_folder)
        self.ollama_url = ollama_url
        self.client = chromadb.Client()
        self.collection = None
        
    def load_documents_from_folder(self) -> List[Dict]:
        """Load all text files from data folder"""
        documents = []
        
        for file_path in self.data_folder.glob("*.txt"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # Split into chunks (simple approach)
                chunks = self.chunk_text(content, chunk_size=500)
                
                for i, chunk in enumerate(chunks):
                    documents.append({
                        "id": f"{file_path.stem}_chunk_{i}",
                        "text": chunk,
                        "source": str(file_path),
                        "chunk_index": i
                    })
                    
                print(f"Loaded {len(chunks)} chunks from {file_path.name}")
                
            except Exception as e:
                print(f"Error loading {file_path}: {e}")
                
        return documents
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """Split text into overlapping chunks"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            chunk = " ".join(chunk_words)
            chunks.append(chunk)
            
        return chunks
    
    def get_embedding(self, text: str, model="nomic-embed-text") -> List[float]:
        """Get embeddings from Ollama"""
        try:
            response = requests.post(
                f"{self.ollama_url}/api/embeddings",
                json={"model": model, "prompt": text},
                timeout=60  # Longer timeout for USB SSD
            )
            if response.status_code == 200:
                return response.json()["embedding"]
            else:
                raise Exception(f"Embedding failed: {response.text}")
        except Exception as e:
            print(f"Embedding error: {e}")
            raise
    
    def setup_knowledge_base(self, collection_name="file_documents"):
        """Load documents and create vector database"""
        print("Setting up knowledge base...")
        
        # Create collection
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"description": "File-based RAG system"}
        )
        
        # Load documents
        documents = self.load_documents_from_folder()
        
        if not documents:
            print("No documents found in data folder!")
            return
            
        print(f"Processing {len(documents)} document chunks...")
        
        # Prepare data for ChromaDB
        embeddings = []
        texts = []
        ids = []
        metadatas = []
        
        for doc in documents:
            try:
                embedding = self.get_embedding(doc["text"])
                embeddings.append(embedding)
                texts.append(doc["text"])
                ids.append(doc["id"])
                metadatas.append({
                    "source": doc["source"],
                    "chunk_index": doc["chunk_index"]
                })
            except Exception as e:
                print(f"Error processing document {doc['id']}: {e}")
                continue
        
        # Add to ChromaDB
        if embeddings:
            self.collection.add(
                embeddings=embeddings,
                documents=texts,
                ids=ids,
                metadatas=metadatas
            )
            print(f"Successfully added {len(embeddings)} documents to knowledge base!")
        
    def search(self, query: str, n_results: int = 3) -> List[Dict]:
        """Search for relevant documents"""
        if not self.collection:
            raise Exception("Knowledge base not set up. Run setup_knowledge_base() first.")
            
        query_embedding = self.get_embedding(query)
        
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results,
            include=["documents", "metadatas", "distances"]
        )
        
        search_results = []
        for i in range(len(results["documents"][0])):
            search_results.append({
                "text": results["documents"][0][i],
                "source": results["metadatas"][0][i]["source"],
                "distance": results["distances"][0][i]
            })
            
        return search_results
    
    def ask_question(self, question: str, model="mistral") -> Dict:
        """Ask a question and get an answer based on your documents"""
        print(f"\nQuestion: {question}")
        
        # Search for relevant documents
        relevant_docs = self.search(question, n_results=2)  # Reduce to 2 for shorter context
        
        # Create shorter context
        context = "\n\n".join([
            f"{doc['text'][:300]}..." if len(doc['text']) > 300 else doc['text']
            for doc in relevant_docs
        ])
        
        # Shorter, more focused prompt
        prompt = f"""Answer this question using the provided context. Be concise.

Context: {context}

Question: {question}

Answer:"""

        # Try with retries and much longer timeout for USB SSD setup
        answer = None
        for attempt in range(2):  # Reduce to 2 attempts since each takes so long
            try:
                print(f"Generating answer (attempt {attempt + 1})... This may take 1-2 minutes on USB SSD...")
                response = requests.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.3,
                            "top_p": 0.9,
                            "num_predict": 100  # Shorter responses for faster completion
                        }
                    },
                    timeout=120  # 2 minutes timeout for USB SSD setup
                )
                
                if response.status_code == 200:
                    answer = response.json()["response"]
                    break
                else:
                    print(f"HTTP Error: {response.status_code}")
                    if attempt == 1:  # Last attempt
                        answer = f"Error generating answer: {response.text}"
                    
            except requests.exceptions.Timeout:
                print(f"Timeout on attempt {attempt + 1} (this is normal on USB SSD)")
                if attempt == 1:  # Last attempt
                    answer = "Response took longer than 2 minutes. Consider using the streaming version (simple_rag.py) for better experience on USB SSD."
                continue
            except Exception as e:
                print(f"Error on attempt {attempt + 1}: {e}")
                if attempt == 1:  # Last attempt
                    answer = f"Error: {e}"
                break
        
        return {
            "question": question,
            "answer": answer,
            "sources": [doc["source"] for doc in relevant_docs],
            "relevant_chunks": len(relevant_docs)
        }

# Example usage
if __name__ == "__main__":
    print("Setting up RAG system with your documents...")
    
    # Initialize RAG with your data folder
    rag = DocumentRAG(data_folder="./data")
    
    # Set up knowledge base (this will process all .txt files in data/)
    rag.setup_knowledge_base()
    
    # Interactive Q&A
    print("\n" + "="*50)
    print("RAG System Ready! Ask questions about your documents.")
    print("Type 'quit' to exit.")
    print("="*50)
    
    while True:
        question = input("\nYour question: ").strip()
        
        if question.lower() in ['quit', 'exit', 'q']:
            break
            
        if question:
            # Try with phi3:mini first (faster), fallback to mistral
            try:
                result = rag.ask_question(question, model="phi3:mini")
            except:
                print("Trying with mistral model...")
                result = rag.ask_question(question, model="mistral")
                
            print(f"\nAnswer: {result['answer']}")
            print(f"\nSources used: {', '.join(set(result['sources']))}")



================================================
FILE: rag_mistral_example.py
================================================



================================================
FILE: rag_requirements.txt
================================================
# RAG Requirements
chromadb>=0.4.0
requests>=2.28.0



================================================
FILE: requirements.txt
================================================
aiohappyeyeballs==2.6.1
aiohttp==3.12.12
aiosignal==1.3.2
annotated-types==0.7.0
anthropic==0.54.0
anyio==4.9.0
argcomplete==3.6.2
attrs==25.3.0
boto3==1.38.36
botocore==1.38.36
cachetools==5.5.2
certifi==2025.4.26
charset-normalizer==3.4.2
click==8.2.1
cohere==5.15.0
colorama==0.4.6
dataclasses-json==0.6.7
distro==1.9.0
eval_type_backport==0.2.2
fasta2a @ git+https://github.com/pydantic/pydantic-ai.git@352acff89074a4d11fdb24b631df1beda0c9fb45#subdirectory=fasta2a
fastavro==1.11.1
filelock==3.18.0
frozenlist==1.7.0
fsspec==2025.5.1
google-auth==2.40.3
google-genai==1.20.0
greenlet==3.2.3
griffe==1.7.3
groq==0.28.0
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
httpx-sse==0.4.0
huggingface-hub==0.33.0
idna==3.10
importlib_metadata==8.7.0
Jinja2==3.1.6
jiter==0.10.0
jmespath==1.0.1
jsonpatch==1.33
jsonpointer==3.0.0
langchain==0.3.25
langchain-community==0.3.25
langchain-core==0.3.65
langchain-openai==0.3.23
langchain-text-splitters==0.3.8
langsmith==0.3.45
logfire-api==3.19.0
markdown-it-py==3.0.0
MarkupSafe==3.0.2
marshmallow==3.26.1
mcp==1.9.4
mdurl==0.1.2
mistralai==1.8.2
mpmath==1.3.0
multidict==6.4.4
mypy_extensions==1.1.0
networkx==3.5
numpy==2.3.0
openai==1.86.0
opentelemetry-api==1.34.1
orjson==3.10.18
packaging==24.2
prompt_toolkit==3.0.51
propcache==0.3.2
pyasn1==0.6.1
pyasn1_modules==0.4.2
pydantic==2.11.6
pydantic-ai @ git+https://github.com/pydantic/pydantic-ai.git@352acff89074a4d11fdb24b631df1beda0c9fb45
pydantic-ai-slim @ git+https://github.com/pydantic/pydantic-ai.git@352acff89074a4d11fdb24b631df1beda0c9fb45#subdirectory=pydantic_ai_slim
pydantic-evals @ git+https://github.com/pydantic/pydantic-ai.git@352acff89074a4d11fdb24b631df1beda0c9fb45#subdirectory=pydantic_evals
pydantic-graph @ git+https://github.com/pydantic/pydantic-ai.git@352acff89074a4d11fdb24b631df1beda0c9fb45#subdirectory=pydantic_graph
pydantic-settings==2.9.1
pydantic_core==2.33.2
Pygments==2.19.1
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-multipart==0.0.20
PyYAML==6.0.2
regex==2024.11.6
requests==2.32.4
requests-toolbelt==1.0.0
rich==14.0.0
rsa==4.9.1
s3transfer==0.13.0
setuptools==80.9.0
six==1.17.0
sniffio==1.3.1
SQLAlchemy==2.0.41
sse-starlette==2.3.6
starlette==0.47.0
sympy==1.14.0
tenacity==9.1.2
tiktoken==0.9.0
tokenizers==0.21.1
torch==2.7.1
tqdm==4.67.1
types-requests==2.32.4.20250611
typing-inspect==0.9.0
typing-inspection==0.4.1
typing_extensions==4.14.0
urllib3==2.4.0
uv==0.7.13
uvicorn==0.34.3
wcwidth==0.2.13
websockets==15.0.1
yarl==1.20.1
zipp==3.23.0
zstandard==0.23.0



================================================
FILE: requirements_clean.txt
================================================
aiohappyeyeballs==2.6.1
aiohttp==3.12.12
aiosignal==1.3.2
annotated-types==0.7.0
anthropic==0.54.0
anyio==4.9.0
argcomplete==3.6.2
attrs==25.3.0
boto3==1.38.36
botocore==1.38.36
cachetools==5.5.2
certifi==2025.4.26
charset-normalizer==3.4.2
click==8.2.1
cohere==5.15.0
dataclasses-json==0.6.7
distro==1.9.0
eval_type_backport==0.2.2
fasta2a @ git+https://github.com/pydantic/pydantic-ai.git@352acff89074a4d11fdb24b631df1beda0c9fb45#subdirectory=fasta2a
fastavro==1.11.1
filelock==3.18.0
frozenlist==1.7.0
fsspec==2025.5.1
google-auth==2.40.3
google-genai==1.20.0
greenlet==3.2.3
griffe==1.7.3
groq==0.28.0
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
httpx-sse==0.4.0
huggingface-hub==0.33.0
idna==3.10
importlib_metadata==8.7.0
Jinja2==3.1.6
jiter==0.10.0
jmespath==1.0.1
jsonpatch==1.33
jsonpointer==3.0.0
langchain==0.3.25
langchain-community==0.3.25
langchain-core==0.3.65
langchain-openai==0.3.23
langchain-text-splitters==0.3.8
langsmith==0.3.45
logfire-api==3.19.0
markdown-it-py==3.0.0
MarkupSafe==3.0.2
marshmallow==3.26.1
mcp==1.9.4
mdurl==0.1.2
mistralai==1.8.2
mpmath==1.3.0
multidict==6.4.4
mypy_extensions==1.1.0
networkx==3.4.1
numpy==2.2.6
openai==1.86.0
opentelemetry-api==1.34.1
orjson==3.10.18
packaging==24.2
prompt_toolkit==3.0.51
propcache==0.3.2
pyasn1==0.6.1
pyasn1_modules==0.4.2
pydantic==2.11.6
pydantic-ai-slim @ git+https://github.com/pydantic/pydantic-ai.git@352acff89074a4d11fdb24b631df1beda0c9fb45#subdirectory=pydantic_ai_slim
pydantic-ai @ git+https://github.com/pydantic/pydantic-ai.git@352acff89074a4d11fdb24b631df1beda0c9fb45
pydantic-evals @ git+https://github.com/pydantic/pydantic-ai.git@352acff89074a4d11fdb24b631df1beda0c9fb45#subdirectory=pydantic_evals
pydantic-graph @ git+https://github.com/pydantic/pydantic-ai.git@352acff89074a4d11fdb24b631df1beda0c9fb45#subdirectory=pydantic_graph
pydantic-settings==2.9.1
pydantic_core==2.33.2
Pygments==2.19.1
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-multipart==0.0.20
PyYAML==6.0.2
regex==2024.11.6
requests==2.32.4
requests-toolbelt==1.0.0
rich==14.0.0
rsa==4.9.1
s3transfer==0.13.0
setuptools==80.9.0
six==1.17.0
sniffio==1.3.1
SQLAlchemy==2.0.41
sse-starlette==2.3.6
starlette==0.47.0
sympy==1.14.0
tenacity==9.1.2
tiktoken==0.9.0
tokenizers==0.21.1
torch==2.7.1
tqdm==4.67.1
types-requests==2.32.4.20250611
typing-inspect==0.9.0
typing-inspection==0.4.1
typing_extensions==4.14.0
urllib3==2.4.0
uv==0.7.13
uvicorn==0.34.3
wcwidth==0.2.13
websockets==15.0.1
yarl==1.20.1
zipp==3.23.0
zstandard==0.23.0



================================================
FILE: requirements_wsl.txt
================================================
attrs==23.2.0
Automat==22.10.0
Babel==2.10.3
bcrypt==3.2.2
blinker==1.7.0
certifi==2023.11.17
chardet==5.2.0
click==8.1.6
cloud-init==24.4
colorama==0.4.6
command-not-found==0.3
configobj==5.0.8
constantly==23.10.4
cryptography==41.0.7
dbus-python==1.3.2
distro==1.9.0
distro-info==1.7+build1
h11==0.14.0
httplib2==0.20.4
hyperlink==21.0.0
idna==3.6
incremental==22.10.0
Jinja2==3.1.2
jsonpatch==1.32
jsonpointer==2.0
jsonschema==4.10.3
launchpadlib==1.11.0
lazr.restfulclient==0.14.6
lazr.uri==1.0.6
markdown-it-py==3.0.0
MarkupSafe==2.1.5
mdurl==0.1.2
netifaces==0.11.0
oauthlib==3.2.2
pyasn1==0.4.8
pyasn1-modules==0.2.8
pycurl==7.45.3
Pygments==2.17.2
PyGObject==3.48.2
PyHamcrest==2.1.0
PyJWT==2.7.0
pyOpenSSL==23.2.0
pyparsing==3.1.1
pyrsistent==0.20.0
pyserial==3.5
python-apt==2.7.7+ubuntu4
pytz==2024.1
PyYAML==6.0.1
requests==2.31.0
rich==13.7.1
service-identity==24.1.0
six==1.16.0
systemd-python==235
Twisted==24.3.0
typing_extensions==4.10.0
ubuntu-pro-client==8001
unattended-upgrades==0.1
urllib3==2.0.7
uvicorn==0.27.1
uvloop==0.19.0
wadllib==1.3.6
wsproto==1.2.0
zope.interface==6.1



================================================
FILE: schemas.py
================================================
# schemas.py
# GPT3

from pydantic import BaseModel

class Task(BaseModel):
    title: str
    priority: str
    due: str



# from pydantic import BaseModel

# class Task(BaseModel):
#     title: str
#     due_date: str
#     priority: int



================================================
FILE: selenium1.py
================================================
# selenium1.py

# CPLT52

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from webdriver_manager.chrome import ChromeDriverManager
import time

service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service)

try:
    driver.get("https://www.google.com")
    search_box = driver.find_element(By.NAME, "q")
    search_box.send_keys("quantum computing")
    search_box.send_keys(Keys.RETURN)
    time.sleep(2)  # Wait for results to load

    results = driver.find_elements(By.CSS_SELECTOR, "h3")
    print("Top Google results:")
    for r in results[:5]:
        print("-", r.text)
finally:
    driver.quit()



================================================
FILE: selenium2.py
================================================
# selenium1.py

# CPLT66

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
import time

chrome_options = Options()
# chrome_options.add_argument("--headless=new")  # Use new headless mode
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument(
    "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
)
service = Service("/usr/bin/chromedriver")  # Path to chromedriver in WSL2
driver = webdriver.Chrome(service=service, options=chrome_options)
try:
    driver.get("https://duckduckgo.com")
    search_box = driver.find_element(By.NAME, "q")
    search_box.send_keys("quantum computing")
    search_box.send_keys(Keys.RETURN)
    time.sleep(3)  # Wait longer for results to load

    results = driver.find_elements(By.CSS_SELECTOR, "h2 a.result__a")
    print("Top DuckDuckGo results:")
    for r in results[:5]:
        print("-", r.text)

    # Debug: print the first 500 characters of the page source
    print("\n[DEBUG] Page source snippet:")
    print(driver.page_source[:500])

    # Keep browser open for 30 seconds for inspection
    print("\n[INFO] Browser will remain open for 30 seconds...")
    time.sleep(30)
finally:
    driver.quit()



================================================
FILE: SETUP.md
================================================
# Setup Guide

This guide provides detailed setup instructions for the Local RAG + Multi-Agent System.

## Prerequisites

### System Requirements
- **OS**: Linux (Ubuntu/WSL2), macOS, or Windows
- **Python**: 3.11 or higher
- **RAM**: 8GB minimum, 16GB+ recommended for Mistral 7B
- **Storage**: 10GB free space for models and data
- **Network**: Internet connection for model downloads

### Required Software

#### 1. Ollama Installation
```bash
# Linux/macOS
curl -fsSL https://ollama.com/install.sh | sh

# Windows (WSL2)
curl -fsSL https://ollama.com/install.sh | sh

# Verify installation
ollama --version
```

#### 2. Python Environment
```bash
# Check Python version
python3 --version

# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate    # Windows

# Upgrade pip
pip install --upgrade pip
```

## Installation Steps

### 1. Clone and Setup Project
```bash
# Navigate to your projects directory
cd /home/your-username/

# If you don't have the project files, create the directory
mkdir 410_mistral7b_ollama_pydanticai
cd 410_mistral7b_ollama_pydanticai

# Install dependencies
pip install -r requirements.txt
```

### 2. Install and Setup Ollama Models
```bash
# Pull required models (this may take 10-30 minutes)
ollama pull mistral:7b
ollama pull phi3:mini

# Verify models are installed
ollama list

# Test model functionality
ollama run mistral:7b "Hello, how are you?"
```

### 3. Environment Configuration

#### Create Environment Variables
```bash
# For OpenAI integration (optional but recommended)
export OPENAI_API_KEY="your-openai-api-key-here"

# Make it permanent (Linux/macOS)
echo 'export OPENAI_API_KEY="your-openai-api-key-here"' >> ~/.bashrc
source ~/.bashrc

# For WSL2/Windows users
echo 'export OPENAI_API_KEY="your-openai-api-key-here"' >> ~/.bashrc
source ~/.bashrc
```

#### Create .env file (Alternative)
```bash
# Create .env file in project root
cat > .env << EOF
OPENAI_API_KEY=your-openai-api-key-here
OLLAMA_HOST=http://localhost:11434
RAG_DATA_PATH=./data
VECTOR_DB_PATH=./chroma_db
EOF
```

### 4. Initialize Data Directory
```bash
# Create data directory if it doesn't exist
mkdir -p data

# Add sample documents (optional)
echo "This is a sample document for testing RAG functionality." > data/sample.txt
echo "# Sample Markdown\nThis is a markdown document." > data/example.md
```

## Verification Tests

### 1. Test Ollama Connection
```bash
python3 test_ollama.py
```

Expected output:
```
‚úÖ Ollama is running
‚úÖ Model mistral:7b is available
‚úÖ Model phi3:mini is available
‚úÖ Test query successful
```

### 2. Test RAG System
```bash
python3 simple_rag.py
```

Follow the interactive prompts to test document querying.

### 3. Test Multi-Agent System
```bash
# Terminal 1: Start agents
chmod +x start_agents.sh
./start_agents.sh

# Terminal 2: Test client
python3 demo_client.py
```

### 4. Test OpenAI Integration (if configured)
```bash
python3 openai_rag_demo.py
```

## Troubleshooting

### Common Issues

#### 1. Ollama Not Found
```bash
# Check if Ollama is running
ps aux | grep ollama

# Start Ollama service
ollama serve &

# Or restart system service
sudo systemctl restart ollama
```

#### 2. Model Loading Errors
```bash
# Check available models
ollama list

# Re-pull models if necessary
ollama pull mistral:7b --force
ollama pull phi3:mini --force
```

#### 3. Memory Issues
```bash
# Check available RAM
free -h

# For low memory systems, use smaller model
ollama pull phi3:mini
# Then modify scripts to use phi3:mini instead of mistral:7b
```

#### 4. Port Conflicts
```bash
# Check if ports are in use
netstat -tlnp | grep :8001
netstat -tlnp | grep :8002

# Kill processes if necessary
pkill -f "agent1_research.py"
pkill -f "agent2_writer.py"
```

#### 5. Python Dependencies
```bash
# If requirements.txt fails, install individually
pip install fastapi uvicorn pydantic requests
pip install scikit-learn numpy chromadb
pip install openai python-dotenv

# For development
pip install --upgrade pip setuptools wheel
```

### Performance Optimization

#### 1. Hardware Recommendations
- **CPU**: Multi-core processor (8+ cores recommended)
- **RAM**: 16GB+ for smooth Mistral 7B operation
- **Storage**: SSD recommended for faster model loading
- **GPU**: Optional, but improves performance significantly

#### 2. Model Selection
```bash
# For faster responses (lower quality)
ollama pull phi3:mini

# For better quality (slower responses)
ollama pull mistral:7b

# For best quality (requires more resources)
ollama pull llama3:8b
```

#### 3. System Tuning
```bash
# Increase file descriptor limits
ulimit -n 4096

# For WSL2 users - allocate more memory
# Edit .wslconfig in Windows user directory:
# [wsl2]
# memory=8GB
# processors=4
```

## Advanced Configuration

### 1. Custom Model Configuration
Edit individual Python files to change models:
```python
# In ollama_pydantic_agent.py, simple_rag.py, etc.
MODEL = "your-preferred-model"  # e.g., "phi3:mini", "llama3:8b"
```

### 2. Vector Database Configuration
```python
# In rag_file_loader.py
CHROMA_DB_PATH = "./custom_chroma_db"
COLLECTION_NAME = "custom_collection"
```

### 3. Agent Port Configuration
```python
# In agent1_research.py
PORT = 8001  # Change if needed

# In agent2_writer.py  
PORT = 8002  # Change if needed
```

## Production Deployment

### 1. Docker Setup (Optional)
```bash
# Build Docker image (if Dockerfile is available)
docker build -t rag-system .

# Run container
docker run -p 8001:8001 -p 8002:8002 rag-system
```

### 2. Systemd Service (Linux)
```bash
# Create service file
sudo tee /etc/systemd/system/rag-agents.service << EOF
[Unit]
Description=RAG Multi-Agent System
After=network.target

[Service]
Type=forking
User=your-username
WorkingDirectory=/path/to/410_mistral7b_ollama_pydanticai
ExecStart=/path/to/410_mistral7b_ollama_pydanticai/start_agents.sh
Restart=always

[Install]
WantedBy=multi-user.target
EOF

# Enable and start service
sudo systemctl enable rag-agents
sudo systemctl start rag-agents
```

## Next Steps

After successful setup:
1. Review [EXAMPLES.md](EXAMPLES.md) for usage examples
2. Check [API.md](docs/API.md) for API documentation
3. Explore different models and configurations
4. Add your own documents to the `data/` directory
5. Experiment with GitIngest integration for codebase analysis

## Getting Help

If you encounter issues:
1. Check the logs in terminal outputs
2. Verify all prerequisites are met
3. Test individual components separately
4. Review the troubleshooting section above
5. Check Ollama documentation: https://ollama.com/docs
6. Refer to project README.md for additional context



================================================
FILE: simple_gitingest_examples.py
================================================
# simple_gitingest_examples.py
"""
Simple GitIngest Examples - Quick Start Guide
"""

import requests
import json

def basic_usage():
    """Most basic GitIngest usage"""
    
    # 1. Simply paste a GitHub URL into GitIngest web interface
    print("METHOD 1: Web Interface (Easiest)")
    print("1. Go to https://gitingest.com/")
    print("2. Paste your GitHub repo URL")
    print("3. Click 'Ingest Repository'")
    print("4. Copy the generated summary")
    print()
    
    # 2. Direct URL method
    print("METHOD 2: Direct URL (Quick)")
    github_url = "https://github.com/python/cpython"
    gitingest_url = f"https://gitingest.com/api/ingest?url={github_url}"
    print(f"Direct link: {gitingest_url}")
    print()
    
    # 3. API method (programmatic)
    print("METHOD 3: API Call (Programmatic)")
    api_example()

def api_example():
    """Example API usage - Fixed for actual GitIngest API"""
    
    # Try a smaller repository first
    repo_url = "https://github.com/octocat/Hello-World"
    
    print(f"Testing with: {repo_url}")
    
    # Correct API format based on error response
    payload = {
        "input_text": repo_url,  # Changed from "url" to "input_text"
        "max_file_size": 32768,  # 32KB max file size
        "pattern_type": "include",  # or "exclude"
        "pattern": "*.py,*.md,*.txt,*.json"  # Comma-separated patterns
    }
    
    try:
        response = requests.post(
            "https://gitingest.com/api/ingest",
            json=payload,
            headers={"Content-Type": "application/json"},
            timeout=60
        )
        
        print(f"Status: {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            print(f"‚úÖ Successfully analyzed: {repo_url}")
            
            # GitIngest returns plain text, not JSON structure
            if isinstance(result, str):
                content = result
            elif isinstance(result, dict) and "content" in result:
                content = result["content"]
            else:
                content = str(result)
            
            print(f"Content length: {len(content)} characters")
            
            # Save to file for AI analysis
            with open("repo_analysis.txt", "w", encoding='utf-8') as f:
                f.write(f"Repository Analysis: {repo_url}\n")
                f.write("=" * 50 + "\n\n")
                f.write(content)
            
            print("üìù Analysis saved to 'repo_analysis.txt'")
            print("ü§ñ Ready to paste into AI chat!")
            
            # Show preview
            print(f"\nPreview (first 300 chars):")
            print("-" * 30)
            print(content[:300] + "..." if len(content) > 300 else content)
            
        elif response.status_code == 422:
            print(f"‚ùå Validation Error (422)")
            try:
                error_details = response.json()
                print(f"Error details: {json.dumps(error_details, indent=2)}")
            except:
                print(f"Error text: {response.text}")
                
        else:
            print(f"‚ùå Error: {response.status_code}")
            print(f"Response: {response.text[:200]}...")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")
    
    # Always show the web alternative
    print(f"\nüí° Web Interface Alternative:")
    print(f"   https://gitingest.com/")
    print(f"   Just paste: {repo_url}")

def use_with_local_llm():
    """Example: Using GitIngest output with your local Ollama setup"""
    
    print("USING GITINGEST WITH YOUR LOCAL LLM:")
    print("1. Run GitIngest on a repository")
    print("2. Save the output to a text file")
    print("3. Use it as context for your local Mistral/Phi3 model")
    print()
    
    # Example prompt template
    prompt_template = """
Analyze this repository and tell me:
1. What does this project do?
2. What are the main components?
3. How is it structured?
4. What technologies does it use?

Repository Analysis:
{gitingest_output}

Please provide a comprehensive analysis:
"""
    
    print("Example prompt template:")
    print(prompt_template)

def common_patterns():
    """Show common include/exclude patterns"""
    
    patterns = {
        "Python Projects": {
            "include": ["*.py", "*.md", "*.txt", "*.yml", "*.yaml", "requirements.txt"],
            "exclude": ["*.pyc", "__pycache__/*", ".git/*", "venv/*", ".env"]
        },
        "JavaScript/Node.js": {
            "include": ["*.js", "*.ts", "*.json", "*.md", "*.jsx", "*.tsx"],
            "exclude": ["node_modules/*", "*.min.js", "dist/*", ".git/*"]
        },
        "Documentation Only": {
            "include": ["*.md", "*.rst", "*.txt"],
            "exclude": [".git/*", "node_modules/*"]
        },
        "Configuration Files": {
            "include": ["*.yml", "*.yaml", "*.json", "*.toml", "*.ini", "*.conf"],
            "exclude": [".git/*", "*.log"]
        }
    }
    
    print("COMMON FILTER PATTERNS:")
    print("=" * 30)
    
    for project_type, filters in patterns.items():
        print(f"\n{project_type}:")
        print(f"  Include: {filters['include']}")
        print(f"  Exclude: {filters['exclude']}")

if __name__ == "__main__":
    print("üîç GITINGEST QUICK START GUIDE")
    print("=" * 40)
    print()
    
    basic_usage()
    print()
    
    print("Running API example...")
    api_example()
    print()
    
    use_with_local_llm()
    print()
    
    common_patterns()



================================================
FILE: simple_rag.py
================================================
# simple_rag.py
"""
Ultra-simple RAG that works around timeout issues
Uses streaming responses and minimal context
"""

import requests
import chromadb
import json
from pathlib import Path

class SimpleRAG:
    def __init__(self):
        self.client = chromadb.Client()
        self.collection = None
        self.ollama_url = "http://localhost:11434"
        
    def setup_documents(self):
        """Load and index documents"""
        self.collection = self.client.get_or_create_collection("simple_docs")
        
        # Load your data files
        data_folder = Path("./data")
        docs = []
        
        for txt_file in data_folder.glob("*.txt"):
            with open(txt_file, 'r') as f:
                content = f.read()
                # Simple chunking - split by sentences
                sentences = content.split('. ')
                for i, sentence in enumerate(sentences):
                    if len(sentence.strip()) > 20:  # Skip very short sentences
                        docs.append({
                            "id": f"{txt_file.stem}_{i}",
                            "text": sentence.strip(),
                            "source": txt_file.name
                        })
        
        print(f"Loaded {len(docs)} document chunks")
        
        # Get embeddings and add to ChromaDB
        embeddings = []
        texts = []
        ids = []
        metadatas = []
        
        for doc in docs:
            try:
                # Get embedding
                response = requests.post(
                    f"{self.ollama_url}/api/embeddings",
                    json={"model": "nomic-embed-text", "prompt": doc["text"]},
                    timeout=10
                )
                if response.status_code == 200:
                    embedding = response.json()["embedding"]
                    embeddings.append(embedding)
                    texts.append(doc["text"])
                    ids.append(doc["id"])
                    metadatas.append({"source": doc["source"]})
            except Exception as e:
                print(f"Error processing: {e}")
                continue
        
        if embeddings:
            self.collection.add(
                embeddings=embeddings,
                documents=texts,
                ids=ids,
                metadatas=metadatas
            )
            print(f"Added {len(embeddings)} documents to knowledge base")
    
    def search(self, query, n_results=2):
        """Search for relevant documents"""
        # Get query embedding
        response = requests.post(
            f"{self.ollama_url}/api/embeddings",
            json={"model": "nomic-embed-text", "prompt": query},
            timeout=10
        )
        
        if response.status_code == 200:
            query_embedding = response.json()["embedding"]
            
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )
            
            return results["documents"][0]
        return []
    
    def ask_streaming(self, question, model="phi3:mini"):
        """Ask question with streaming response"""
        print(f"\nQuestion: {question}")
        
        # Get relevant documents
        relevant_docs = self.search(question)
        
        if not relevant_docs:
            print("No relevant documents found.")
            return
        
        # Create simple context (just the most relevant sentence)
        context = relevant_docs[0][:200]  # Use only first 200 chars
        
        # Simple prompt
        prompt = f"Based on this: '{context}' Answer: {question}"
        
        # Streaming request
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": True,
                    "options": {"num_predict": 50}  # Very short response
                },
                stream=True,
                timeout=5
            )
            
            print("Answer: ", end="", flush=True)
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        if "response" in data:
                            print(data["response"], end="", flush=True)
                        if data.get("done", False):
                            break
                    except:
                        continue
            print()  # New line
            
        except Exception as e:
            print(f"Error: {e}")

if __name__ == "__main__":
    print("Setting up Simple RAG...")
    rag = SimpleRAG()
    rag.setup_documents()
    
    print("\nReady! Ask short questions:")
    while True:
        question = input("\nQ: ").strip()
        if question.lower() in ['quit', 'q', 'exit']:
            break
        if question:
            rag.ask_streaming(question)



================================================
FILE: start_agents.sh
================================================
#!/bin/bash
# start_agents.sh - Start both agents

echo "Starting Multi-Agent System..."

# Start Agent1 in background
echo "Starting Agent1 (Research Assistant) on port 8001..."
python3 agent1_research.py &
AGENT1_PID=$!

# Wait a moment for Agent1 to start
sleep 3

# Start Agent2 in background  
echo "Starting Agent2 (Report Writer) on port 8002..."
python3 agent2_writer.py &
AGENT2_PID=$!

echo "Both agents started!"
echo "Agent1 PID: $AGENT1_PID"
echo "Agent2 PID: $AGENT2_PID"
echo ""
echo "Agent1 API: http://localhost:8001/docs"
echo "Agent2 API: http://localhost:8002/docs"
echo ""
echo "Press Ctrl+C to stop all agents"

# Wait for interrupt
trap 'kill $AGENT1_PID $AGENT2_PID; echo "Stopping agents..."; exit' INT
wait



================================================
FILE: test.py
================================================
import requests

url = "http://localhost:8000/v1/chat/completions"

payload = {
    "model": "mistral",
    "messages": [
        {"role": "user", "content": "What is the capital of France?"}
    ]
}

response = requests.post(url, json=payload)
print(response.json())



================================================
FILE: test_ollama.py
================================================
# test_ollama.py
"""
Simple test to check Ollama response times
"""

import requests
import time

def test_ollama_simple():
    """Test a very simple prompt to Ollama"""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "mistral",
                "prompt": "Say hello in one word.",
                "stream": False,
                "options": {
                    "num_predict": 10
                }
            },
            timeout=15
        )
        
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            print(f"‚úÖ Success! Response time: {end_time - start_time:.2f}s")
            print(f"Answer: {result['response']}")
            return True
        else:
            print(f"‚ùå HTTP Error: {response.status_code}")
            print(response.text)
            return False
            
    except requests.exceptions.Timeout:
        print(f"‚ùå Timeout after {time.time() - start_time:.2f}s")
        return False
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False

def test_embedding():
    """Test embedding generation"""
    start_time = time.time()
    
    try:
        response = requests.post(
            "http://localhost:11434/api/embeddings",
            json={
                "model": "nomic-embed-text",
                "prompt": "test text"
            },
            timeout=10
        )
        
        end_time = time.time()
        
        if response.status_code == 200:
            print(f"‚úÖ Embedding success! Response time: {end_time - start_time:.2f}s")
            return True
        else:
            print(f"‚ùå Embedding error: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå Embedding error: {e}")
        return False

if __name__ == "__main__":
    print("Testing Ollama performance...")
    print("\n1. Testing simple text generation:")
    test_ollama_simple()
    
    print("\n2. Testing embeddings:")
    test_embedding()
    
    print("\n3. Testing with different options:")
    start_time = time.time()
    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "mistral",
                "prompt": "What is quantum computing? Answer in 2 sentences.",
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_predict": 50,
                    "top_p": 0.5
                }
            },
            timeout=20
        )
        
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            print(f"‚úÖ Longer prompt success! Response time: {end_time - start_time:.2f}s")
            print(f"Answer: {result['response']}")
        else:
            print(f"‚ùå Longer prompt failed: {response.status_code}")
            
    except Exception as e:
        print(f"‚ùå Longer prompt error: {e}")



================================================
FILE: data/cyclotruc-gitingest.txt
================================================
Directory structure:
‚îî‚îÄ‚îÄ cyclotruc-gitingest/
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ CODE_OF_CONDUCT.md
    ‚îú‚îÄ‚îÄ CONTRIBUTING.md
    ‚îú‚îÄ‚îÄ Dockerfile
    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ pyproject.toml
    ‚îú‚îÄ‚îÄ requirements-dev.txt
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îú‚îÄ‚îÄ SECURITY.md
    ‚îú‚îÄ‚îÄ .dockerignore
    ‚îú‚îÄ‚îÄ .pre-commit-config.yaml
    ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îú‚îÄ‚îÄ gitingest/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cli.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cloning.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entrypoint.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ output_formatters.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query_parsing.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ filesystem_schema.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ingestion_schema.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ exceptions.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ file_utils.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ git_utils.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ignore_patterns.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ingestion_utils.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ notebook_utils.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ os_utils.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ path_utils.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ query_parser_utils.py
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ timeout_wrapper.py
    ‚îÇ   ‚îú‚îÄ‚îÄ server/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query_processor.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server_config.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ server_utils.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routers/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ download.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dynamic.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ templates/
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ api.jinja
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ base.jinja
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ git.jinja
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ index.jinja
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ components/
    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ footer.jinja
    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ git_form.jinja
    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ navbar.jinja
    ‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ result.jinja
    ‚îÇ   ‚îî‚îÄ‚îÄ static/
    ‚îÇ       ‚îú‚îÄ‚îÄ robots.txt
    ‚îÇ       ‚îî‚îÄ‚îÄ js/
    ‚îÇ           ‚îî‚îÄ‚îÄ utils.js
    ‚îú‚îÄ‚îÄ tests/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ conftest.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_cli.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_flow_integration.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_ingestion.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_notebook_utils.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_repository_clone.py
    ‚îÇ   ‚îú‚îÄ‚îÄ .pylintrc
    ‚îÇ   ‚îî‚îÄ‚îÄ query_parser/
    ‚îÇ       ‚îú‚îÄ‚îÄ test_git_host_agnostic.py
    ‚îÇ       ‚îî‚îÄ‚îÄ test_query_parser.py
    ‚îî‚îÄ‚îÄ .github/
        ‚îú‚îÄ‚îÄ dependabot.yml
        ‚îî‚îÄ‚îÄ workflows/
            ‚îú‚îÄ‚îÄ ci.yml
            ‚îú‚îÄ‚îÄ publish.yml
            ‚îî‚îÄ‚îÄ scorecard.yml

================================================
FILE: README.md
================================================
# Gitingest

[![Image](./docs/frontpage.png "Gitingest main page")](https://gitingest.com)

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/cyclotruc/gitingest/blob/main/LICENSE)
[![PyPI version](https://badge.fury.io/py/gitingest.svg)](https://badge.fury.io/py/gitingest)
[![GitHub stars](https://img.shields.io/github/stars/cyclotruc/gitingest?style=social.svg)](https://github.com/cyclotruc/gitingest)
[![Downloads](https://pepy.tech/badge/gitingest)](https://pepy.tech/project/gitingest)

[![Discord](https://dcbadge.limes.pink/api/server/https://discord.com/invite/zerRaGK9EC)](https://discord.com/invite/zerRaGK9EC)

Turn any Git repository into a prompt-friendly text ingest for LLMs.

You can also replace `hub` with `ingest` in any GitHub URL to access the corresponding digest.

[gitingest.com](https://gitingest.com) ¬∑ [Chrome Extension](https://chromewebstore.google.com/detail/adfjahbijlkjfoicpjkhjicpjpjfaood) ¬∑ [Firefox Add-on](https://addons.mozilla.org/firefox/addon/gitingest)

## üöÄ Features

- **Easy code context**: Get a text digest from a Git repository URL or a directory
- **Smart Formatting**: Optimized output format for LLM prompts
- **Statistics about**:
  - File and directory structure
  - Size of the extract
  - Token count
- **CLI tool**: Run it as a shell command
- **Python package**: Import it in your code

## üìö Requirements

- Python 3.8+
- For private repositories: A GitHub Personal Access Token (PAT). You can generate one at [https://github.com/settings/personal-access-tokens](https://github.com/settings/personal-access-tokens) (Profile ‚Üí Settings ‚Üí Developer Settings ‚Üí Personal Access Tokens ‚Üí Fine-grained Tokens)

### üì¶ Installation

Gitingest is available on [PyPI](https://pypi.org/project/gitingest/).
You can install it using `pip`:

```bash
pip install gitingest
```

However, it might be a good idea to use `pipx` to install it.
You can install `pipx` using your preferred package manager.

```bash
brew install pipx
apt install pipx
scoop install pipx
...
```

If you are using pipx for the first time, run:

```bash
pipx ensurepath
```

```bash
# install gitingest
pipx install gitingest
```

## üß© Browser Extension Usage

<!-- markdownlint-disable MD033 -->
<a href="https://chromewebstore.google.com/detail/adfjahbijlkjfoicpjkhjicpjpjfaood" target="_blank" title="Get Gitingest Extension from Chrome Web Store"><img height="48" src="https://github.com/user-attachments/assets/20a6e44b-fd46-4e6c-8ea6-aad436035753" alt="Available in the Chrome Web Store" /></a>
<a href="https://addons.mozilla.org/firefox/addon/gitingest" target="_blank" title="Get Gitingest Extension from Firefox Add-ons"><img height="48" src="https://github.com/user-attachments/assets/c0e99e6b-97cf-4af2-9737-099db7d3538b" alt="Get The Add-on for Firefox" /></a>
<a href="https://microsoftedge.microsoft.com/addons/detail/nfobhllgcekbmpifkjlopfdfdmljmipf" target="_blank" title="Get Gitingest Extension from Microsoft Edge Add-ons"><img height="48" src="https://github.com/user-attachments/assets/204157eb-4cae-4c0e-b2cb-db514419fd9e" alt="Get from the Edge Add-ons" /></a>
<!-- markdownlint-enable MD033 -->

The extension is open source at [lcandy2/gitingest-extension](https://github.com/lcandy2/gitingest-extension).

Issues and feature requests are welcome to the repo.

## üí° Command line usage

The `gitingest` command line tool allows you to analyze codebases and create a text dump of their contents.

```bash
# Basic usage (writes to digest.txt by default)
gitingest /path/to/directory

# From URL
gitingest https://github.com/cyclotruc/gitingest
```

For private repositories, use the `--token/-t` option.

```bash
# Get your token from https://github.com/settings/personal-access-tokens
gitingest https://github.com/username/private-repo --token github_pat_...

# Or set it as an environment variable
export GITHUB_TOKEN=github_pat_...
gitingest https://github.com/username/private-repo
```

By default, the digest is written to a text file (`digest.txt`) in your current working directory. You can customize the output in two ways:

- Use `--output/-o <filename>` to write to a specific file.
- Use `--output/-o -` to output directly to `STDOUT` (useful for piping to other tools).

See more options and usage details with:

```bash
gitingest --help
```

## üêç Python package usage

```python
# Synchronous usage
from gitingest import ingest

summary, tree, content = ingest("path/to/directory")

# or from URL
summary, tree, content = ingest("https://github.com/cyclotruc/gitingest")
```

For private repositories, you can pass a token:

```python
# Using token parameter
summary, tree, content = ingest("https://github.com/username/private-repo", token="github_pat_...")

# Or set it as an environment variable
import os
os.environ["GITHUB_TOKEN"] = "github_pat_..."
summary, tree, content = ingest("https://github.com/username/private-repo")
```

By default, this won't write a file but can be enabled with the `output` argument.

```python
# Asynchronous usage
from gitingest import ingest_async
import asyncio

result = asyncio.run(ingest_async("path/to/directory"))
```

### Jupyter notebook usage

```python
from gitingest import ingest_async

# Use await directly in Jupyter
summary, tree, content = await ingest_async("path/to/directory")

```

This is because Jupyter notebooks are asynchronous by default.

## üê≥ Self-host

1. Build the image:

   ``` bash
   docker build -t gitingest .
   ```

2. Run the container:

   ``` bash
   docker run -d --name gitingest -p 8000:8000 gitingest
   ```

The application will be available at `http://localhost:8000`.

If you are hosting it on a domain, you can specify the allowed hostnames via env variable `ALLOWED_HOSTS`.

   ```bash
   # Default: "gitingest.com, *.gitingest.com, localhost, 127.0.0.1".
   ALLOWED_HOSTS="example.com, localhost, 127.0.0.1"
   ```

## ü§ù Contributing

### Non-technical ways to contribute

- **Create an Issue**: If you find a bug or have an idea for a new feature, please [create an issue](https://github.com/cyclotruc/gitingest/issues/new) on GitHub. This will help us track and prioritize your request.
- **Spread the Word**: If you like Gitingest, please share it with your friends, colleagues, and on social media. This will help us grow the community and make Gitingest even better.
- **Use Gitingest**: The best feedback comes from real-world usage! If you encounter any issues or have ideas for improvement, please let us know by [creating an issue](https://github.com/cyclotruc/gitingest/issues/new) on GitHub or by reaching out to us on [Discord](https://discord.com/invite/zerRaGK9EC).

### Technical ways to contribute

Gitingest aims to be friendly for first time contributors, with a simple Python and HTML codebase. If you need any help while working with the code, reach out to us on [Discord](https://discord.com/invite/zerRaGK9EC). For detailed instructions on how to make a pull request, see [CONTRIBUTING.md](./CONTRIBUTING.md).

## üõ†Ô∏è Stack

- [Tailwind CSS](https://tailwindcss.com) - Frontend
- [FastAPI](https://github.com/fastapi/fastapi) - Backend framework
- [Jinja2](https://jinja.palletsprojects.com) - HTML templating
- [tiktoken](https://github.com/openai/tiktoken) - Token estimation
- [posthog](https://github.com/PostHog/posthog) - Amazing analytics

### Looking for a JavaScript/FileSystemNode package?

Check out the NPM alternative üì¶ Repomix: <https://github.com/yamadashy/repomix>

## üöÄ Project Growth

[![Star History Chart](https://api.star-history.com/svg?repos=cyclotruc/gitingest&type=Date)](https://star-history.com/#cyclotruc/gitingest&Date)



================================================
FILE: CODE_OF_CONDUCT.md
================================================
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, religion, or sexual identity
and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the
  overall community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or
  advances of any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email
  address, without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
<romain@coderamp.io>.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series
of actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or
permanent ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior,  harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within
the community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org),
version 2.0, available at
<https://www.contributor-covenant.org/version/2/0/code_of_conduct.html>.

Community Impact Guidelines were inspired by [Mozilla's code of conduct
enforcement ladder](https://github.com/mozilla/diversity).

For answers to common questions about this code of conduct, see the FAQ at
<https://www.contributor-covenant.org/faq>. Translations are available at
<https://www.contributor-covenant.org/translations>.



================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to Gitingest

Thanks for your interest in contributing to Gitingest! üöÄ Gitingest aims to be friendly for first time contributors, with a simple Python and HTML codebase. We would love your help to make it even better. If you need any help while working with the code, please reach out to us on [Discord](https://discord.com/invite/zerRaGK9EC).

## How to Contribute (non-technical)

- **Create an Issue**: If you find a bug or have an idea for a new feature, please [create an issue](https://github.com/cyclotruc/gitingest/issues/new) on GitHub. This will help us track and prioritize your request.
- **Spread the Word**: If you like Gitingest, please share it with your friends, colleagues, and on social media. This will help us grow the community and make Gitingest even better.
- **Use Gitingest**: The best feedback comes from real-world usage! If you encounter any issues or have ideas for improvement, please let us know by [creating an issue](https://github.com/cyclotruc/gitingest/issues/new) on GitHub or by reaching out to us on [Discord](https://discord.com/invite/zerRaGK9EC).

## How to submit a Pull Request

1. Fork the repository.

2. Clone the forked repository:

   ```bash
   git clone https://github.com/cyclotruc/gitingest.git
   cd gitingest
   ```

3. Set up the development environment and install dependencies:

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements-dev.txt
   pre-commit install
   ```

4. Create a new branch for your changes:

    ```bash
    git checkout -b your-branch
    ```

5. Make your changes. Make sure to add corresponding tests for your changes.

6. Stage your changes:

    ```bash
    git add .
    ```

7. Run the tests:

   ```bash
   pytest
   ```

8. Run the local web server

   1. Navigate to src folder

        ``` bash
        cd src
        ```

   2. Run the local web server:

      ``` bash
      uvicorn server.main:app
      ```

   3. Open your browser and navigate to `http://localhost:8000` to see the app running.

9. Confirm that everything is working as expected. If you encounter any issues, fix them and repeat steps 6 to 8.

10. Commit your changes:

    ```bash
    git commit -m "Your commit message"
    ```

    If `pre-commit` raises any issues, fix them and repeat steps 6 to 9.

11. Push your changes:

    ```bash
    git push origin your-branch
    ```

12. Open a pull request on GitHub. Make sure to include a detailed description of your changes.

13. Wait for the maintainers to review your pull request. If there are any issues, fix them and repeat steps 6 to 12.

    *(Optional) Invite project maintainer to your branch for easier collaboration.*



================================================
FILE: Dockerfile
================================================
# Build stage
FROM python:3.12-slim AS builder

WORKDIR /build

# Copy requirements first to leverage Docker cache
COPY requirements.txt .

# Install build dependencies and Python packages
RUN apt-get update \
    && apt-get install -y --no-install-recommends gcc python3-dev \
    && pip install --no-cache-dir --upgrade pip \
    && pip install --no-cache-dir --timeout 1000 -r requirements.txt \
    && rm -rf /var/lib/apt/lists/*

# Runtime stage
FROM python:3.12-slim

# Set Python environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Install Git
RUN apt-get update \
    && apt-get install -y --no-install-recommends git curl\
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Create a non-root user
RUN useradd -m -u 1000 appuser

COPY --from=builder /usr/local/lib/python3.12/site-packages/ /usr/local/lib/python3.12/site-packages/
COPY src/ ./

# Change ownership of the application files
RUN chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

EXPOSE 8000

CMD ["python", "-m", "uvicorn", "server.main:app", "--host", "0.0.0.0", "--port", "8000"]



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2024 Romain Courtois

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: pyproject.toml
================================================
[project]
name = "gitingest"
version = "0.1.4"
description="CLI tool to analyze and create text dumps of codebases for LLMs"
readme = {file = "README.md", content-type = "text/markdown" }
requires-python = ">= 3.8"
dependencies = [
    "click>=8.0.0",
    "fastapi[standard]>=0.109.1",  # Vulnerable to https://osv.dev/vulnerability/PYSEC-2024-38
    "pydantic",
    "python-dotenv",
    "slowapi",
    "starlette>=0.40.0",  # Vulnerable to https://osv.dev/vulnerability/GHSA-f96h-pmfr-66vw
    "tiktoken>=0.7.0",  # Support for o200k_base encoding
    "tomli",
    "typing_extensions; python_version < '3.10'",
    "uvicorn>=0.11.7",  # Vulnerable to https://osv.dev/vulnerability/PYSEC-2020-150
]

license = {file = "LICENSE"}
authors = [{name = "Romain Courtois", email = "romain@coderamp.io"}]
classifiers=[
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
]

[project.scripts]
gitingest = "gitingest.cli:main"

[project.urls]
homepage = "https://gitingest.com"
github = "https://github.com/cyclotruc/gitingest"

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
packages = {find = {where = ["src"]}}
include-package-data = true

# Linting configuration
[tool.pylint.format]
max-line-length = 119

[tool.pylint.'MESSAGES CONTROL']
disable = [
    "too-many-arguments",
    "too-many-positional-arguments",
    "too-many-locals",
    "too-few-public-methods",
    "broad-exception-caught",
    "duplicate-code",
    "fixme",
]

[tool.pycln]
all = true

[tool.isort]
profile = "black"
line_length = 119
remove_redundant_aliases = true
float_to_top = true
order_by_type = true
filter_files = true

[tool.black]
line-length = 119

# Test configuration
[tool.pytest.ini_options]
pythonpath = ["src"]
testpaths = ["tests/"]
python_files = "test_*.py"
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
python_classes = "Test*"
python_functions = "test_*"



================================================
FILE: requirements-dev.txt
================================================
-r requirements.txt
black
djlint
pre-commit
pylint
pytest
pytest-asyncio



================================================
FILE: requirements.txt
================================================
click>=8.0.0
fastapi[standard]>=0.109.1  # Vulnerable to https://osv.dev/vulnerability/PYSEC-2024-38
pydantic
python-dotenv
slowapi
starlette>=0.40.0  # Vulnerable to https://osv.dev/vulnerability/GHSA-f96h-pmfr-66vw
tiktoken>=0.7.0  # Support for o200k_base encoding
tomli
uvicorn>=0.11.7  # Vulnerable to https://osv.dev/vulnerability/PYSEC-2020-150



================================================
FILE: SECURITY.md
================================================
# Security Policy

## Reporting a Vulnerability

If you have discovered a vulnerability inside the project, report it privately at <romain@coderamp.io>. This way the maintainer can work on a proper fix without disclosing the problem to the public before it has been solved.



================================================
FILE: .dockerignore
================================================
# Git
.git
.gitignore

# Python
__pycache__
*.pyc
*.pyo
*.pyd
.Python
env
pip-log.txt
pip-delete-this-directory.txt
.tox
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.log

# Virtual environment
venv
.env
.venv
ENV

# IDE
.idea
.vscode
*.swp
*.swo

# Project specific
docs/
tests/
*.md
LICENSE
setup.py



================================================
FILE: .pre-commit-config.yaml
================================================
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0
    hooks:
      # Files
      - id: check-added-large-files
        description: "Prevent large files from being committed."
        args: ["--maxkb=10000"]
      - id: check-case-conflict
        description: "Check for files that would conflict in case-insensitive filesystems."
      - id: fix-byte-order-marker
        description: "Remove utf-8 byte order marker."
      - id: mixed-line-ending
        description: "Replace mixed line ending."

      # Links
      - id: destroyed-symlinks
        description: "Detect symlinks which are changed to regular files with a content of a path which that symlink was pointing to."

      # File files for parseable syntax: python
      - id: check-ast

      # File and line endings
      - id: end-of-file-fixer
        description: "Ensure that a file is either empty, or ends with one newline."
      - id: trailing-whitespace
        description: "Trim trailing whitespace."

      # Python
      - id: check-docstring-first
        description: "Check a common error of defining a docstring after code."
      - id: requirements-txt-fixer
        description: "Sort entries in requirements.txt."

  - repo: https://github.com/MarcoGorelli/absolufy-imports
    rev: v0.3.1
    hooks:
      - id: absolufy-imports
        description: "Automatically convert relative imports to absolute. (Use `args: [--never]` to revert.)"

  - repo: https://github.com/psf/black
    rev: 25.1.0
    hooks:
      - id: black

  - repo: https://github.com/asottile/pyupgrade
    rev: v3.20.0
    hooks:
      - id: pyupgrade
        description: "Automatically upgrade syntax for newer versions."
        args: [--py3-plus, --py36-plus]

  - repo: https://github.com/pre-commit/pygrep-hooks
    rev: v1.10.0
    hooks:
      - id: python-check-blanket-noqa
        description: "Enforce that `noqa` annotations always occur with specific codes. Sample annotations: `# noqa: F401`, `# noqa: F401,W203`."
      - id: python-check-blanket-type-ignore
        description: "Enforce that `# type: ignore` annotations always occur with specific codes. Sample annotations: `# type: ignore[attr-defined]`, `# type: ignore[attr-defined, name-defined]`."
      - id: python-use-type-annotations
        description: "Enforce that python3.6+ type annotations are used instead of type comments."

  - repo: https://github.com/PyCQA/isort
    rev: 6.0.1
    hooks:
      - id: isort
        description: "Sort imports alphabetically, and automatically separated into sections and by type."


  - repo: https://github.com/djlint/djLint
    rev: v1.36.4
    hooks:
      - id: djlint-reformat-jinja

  - repo: https://github.com/igorshubovych/markdownlint-cli
    rev: v0.45.0
    hooks:
      - id: markdownlint
        description: "Lint markdown files."
        args: ["--disable=line-length"]

  - repo: https://github.com/terrencepreilly/darglint
    rev: v1.8.1
    hooks:
      - id: darglint
        name: darglint for source
        args: [--docstring-style=numpy]
        files: ^src/

  - repo: https://github.com/pycqa/pylint
    rev: v3.3.7
    hooks:
      - id: pylint
        name: pylint for source
        files: ^src/
        additional_dependencies:
          [
            click>=8.0.0,
            "fastapi[standard]>=0.109.1",
            pydantic,
            pytest-asyncio,
            python-dotenv,
            slowapi,
            starlette>=0.40.0,
            tiktoken,
            tomli,
            uvicorn>=0.11.7,
          ]
      - id: pylint
        name: pylint for tests
        files: ^tests/
        args:
          - --rcfile=tests/.pylintrc
        additional_dependencies:
          [
            click>=8.0.0,
            "fastapi[standard]>=0.109.1",
            pydantic,
            pytest-asyncio,
            python-dotenv,
            slowapi,
            starlette>=0.40.0,
            tiktoken,
            tomli,
            uvicorn>=0.11.7,
          ]

  - repo: meta
    hooks:
      - id: check-hooks-apply
      - id: check-useless-excludes



================================================
FILE: src/gitingest/__init__.py
================================================
"""Gitingest: A package for ingesting data from Git repositories."""

from gitingest.cloning import clone_repo
from gitingest.entrypoint import ingest, ingest_async
from gitingest.ingestion import ingest_query
from gitingest.query_parsing import parse_query

__all__ = ["ingest_query", "clone_repo", "parse_query", "ingest", "ingest_async"]



================================================
FILE: src/gitingest/cli.py
================================================
"""Command-line interface for the Gitingest package."""

# pylint: disable=no-value-for-parameter

import asyncio
from typing import Optional, Tuple

import click

from gitingest.config import MAX_FILE_SIZE, OUTPUT_FILE_NAME
from gitingest.entrypoint import ingest_async


@click.command()
@click.argument("source", type=str, default=".")
@click.option(
    "--output",
    "-o",
    default=None,
    help="Output file path (default: <repo_name>.txt in current directory)",
)
@click.option(
    "--max-size",
    "-s",
    default=MAX_FILE_SIZE,
    help="Maximum file size to process in bytes",
)
@click.option(
    "--exclude-pattern",
    "-e",
    multiple=True,
    help=(
        "Patterns to exclude. Handles Python's arbitrary subset of Unix shell-style "
        "wildcards. See: https://docs.python.org/3/library/fnmatch.html"
    ),
)
@click.option(
    "--include-pattern",
    "-i",
    multiple=True,
    help=(
        "Patterns to include. Handles Python's arbitrary subset of Unix shell-style "
        "wildcards. See: https://docs.python.org/3/library/fnmatch.html"
    ),
)
@click.option("--branch", "-b", default=None, help="Branch to clone and ingest")
@click.option(
    "--token",
    "-t",
    envvar="GITHUB_TOKEN",
    default=None,
    help=(
        "GitHub personal access token for accessing private repositories. "
        "If omitted, the CLI will look for the GITHUB_TOKEN environment variable."
    ),
)
def main(
    source: str,
    output: Optional[str],
    max_size: int,
    exclude_pattern: Tuple[str, ...],
    include_pattern: Tuple[str, ...],
    branch: Optional[str],
    token: Optional[str],
):
    """
    Main entry point for the CLI. This function is called when the CLI is run as a script.

    It calls the async main function to run the command.

    Parameters
    ----------
    source : str
        A directory path or a Git repository URL.
    output : str, optional
        The path where the output file will be written. If not specified, the output will be written
        to a file named `<repo_name>.txt` in the current directory. Use '-' to output to stdout.
    max_size : int
        Maximum file size (in bytes) to consider.
    exclude_pattern : Tuple[str, ...]
        Glob patterns for pruning the file set.
    include_pattern : Tuple[str, ...]
        Glob patterns for including files in the output.
    branch : str, optional
        Specific branch to ingest (defaults to the repository's default).
    token: str, optional
        GitHub personal-access token (PAT). Needed when *source* refers to a
        **private** repository. Can also be set via the ``GITHUB_TOKEN`` env var.
    """

    asyncio.run(
        _async_main(
            source=source,
            output=output,
            max_size=max_size,
            exclude_pattern=exclude_pattern,
            include_pattern=include_pattern,
            branch=branch,
            token=token,
        )
    )


async def _async_main(
    source: str,
    output: Optional[str],
    max_size: int,
    exclude_pattern: Tuple[str, ...],
    include_pattern: Tuple[str, ...],
    branch: Optional[str],
    token: Optional[str],
) -> None:
    """
    Analyze a directory or repository and create a text dump of its contents.

    This command analyzes the contents of a specified source directory or repository, applies custom include and
    exclude patterns, and generates a text summary of the analysis which is then written to an output file
    or printed to stdout.

    Parameters
    ----------
    source : str
        A directory path or a Git repository URL.
    output : str, optional
        The path where the output file will be written. If not specified, the output will be written
        to a file named `<repo_name>.txt` in the current directory. Use '-' to output to stdout.
    max_size : int
        Maximum file size (in bytes) to consider.
    exclude_pattern : Tuple[str, ...]
        Glob patterns for pruning the file set.
    include_pattern : Tuple[str, ...]
        Glob patterns for including files in the output.
    branch : str, optional
        Specific branch to ingest (defaults to the repository's default).
    token: str, optional
        GitHub personal-access token (PAT). Needed when *source* refers to a
        **private** repository. Can also be set via the ``GITHUB_TOKEN`` env var.

    Raises
    ------
    Abort
        If there is an error during the execution of the command, this exception is raised to abort the process.
    """
    try:
        # Normalise pattern containers (the ingest layer expects sets)
        exclude_patterns = set(exclude_pattern)
        include_patterns = set(include_pattern)

        output_target = output if output is not None else OUTPUT_FILE_NAME

        if output_target == "-":
            click.echo("Analyzing source, preparing output for stdout...", err=True)
        else:
            click.echo(f"Analyzing source, output will be written to '{output_target}'...", err=True)

        summary, _, _ = await ingest_async(
            source=source,
            max_file_size=max_size,
            include_patterns=include_patterns,
            exclude_patterns=exclude_patterns,
            branch=branch,
            output=output_target,
            token=token,
        )

        if output_target == "-":  # stdout
            click.echo("\n--- Summary ---", err=True)
            click.echo(summary, err=True)
            click.echo("--- End Summary ---", err=True)
            click.echo("Analysis complete! Output sent to stdout.", err=True)
        else:  # file
            click.echo(f"Analysis complete! Output written to: {output_target}")
            click.echo("\nSummary:")
            click.echo(summary)

    except Exception as exc:
        # Convert any exception into Click.Abort so that exit status is non-zero
        click.echo(f"Error: {exc}", err=True)
        raise click.Abort() from exc


if __name__ == "__main__":
    main()



================================================
FILE: src/gitingest/cloning.py
================================================
"""This module contains functions for cloning a Git repository to a local path."""

from pathlib import Path
from typing import Optional

from gitingest.config import DEFAULT_TIMEOUT
from gitingest.schemas import CloneConfig
from gitingest.utils.git_utils import (
    check_repo_exists,
    create_git_auth_header,
    create_git_command,
    ensure_git_installed,
    run_command,
    validate_github_token,
)
from gitingest.utils.os_utils import ensure_directory
from gitingest.utils.timeout_wrapper import async_timeout


@async_timeout(DEFAULT_TIMEOUT)
async def clone_repo(config: CloneConfig, token: Optional[str] = None) -> None:
    """
    Clone a repository to a local path based on the provided configuration.

    This function handles the process of cloning a Git repository to the local file system.
    It can clone a specific branch or commit if provided, and it raises exceptions if
    any errors occur during the cloning process.

    Parameters
    ----------
    config : CloneConfig
        The configuration for cloning the repository.
    token : str, optional
        GitHub personal-access token (PAT). Needed when *source* refers to a
        **private** repository. Can also be set via the ``GITHUB_TOKEN`` env var.
        Must start with 'github_pat_' or 'gph_' for GitHub repositories.

    Raises
    ------
    ValueError
        If the repository is not found, if the provided URL is invalid, or if the token format is invalid.
    """
    # Extract and validate query parameters
    url: str = config.url
    local_path: str = config.local_path
    commit: Optional[str] = config.commit
    branch: Optional[str] = config.branch
    partial_clone: bool = config.subpath != "/"

    # Validate token if provided
    if token and url.startswith("https://github.com"):
        validate_github_token(token)

    # Create parent directory if it doesn't exist
    await ensure_directory(Path(local_path).parent)

    # Check if the repository exists
    if not await check_repo_exists(url, token=token):
        raise ValueError("Repository not found. Make sure it is public or that you have provided a valid token.")

    clone_cmd = ["git"]
    if token and url.startswith("https://github.com"):
        clone_cmd += ["-c", create_git_auth_header(token)]

    clone_cmd += ["clone", "--single-branch"]
    # TODO: Re-enable --recurse-submodules when submodule support is needed

    if partial_clone:
        clone_cmd += ["--filter=blob:none", "--sparse"]

    if not commit:
        clone_cmd += ["--depth=1"]
        if branch and branch.lower() not in ("main", "master"):
            clone_cmd += ["--branch", branch]

    clone_cmd += [url, local_path]

    # Clone the repository
    await ensure_git_installed()
    await run_command(*clone_cmd)

    # Checkout the subpath if it is a partial clone
    if partial_clone:
        subpath = config.subpath.lstrip("/")
        if config.blob:
            # When ingesting from a file url (blob/branch/path/file.txt), we need to remove the file name.
            subpath = str(Path(subpath).parent.as_posix())

        checkout_cmd = create_git_command(["git"], local_path, url, token)
        await run_command(*checkout_cmd, "sparse-checkout", "set", subpath)

    # Checkout the commit if it is provided
    if commit:
        checkout_cmd = create_git_command(["git"], local_path, url, token)
        await run_command(*checkout_cmd, "checkout", commit)



================================================
FILE: src/gitingest/config.py
================================================
"""Configuration file for the project."""

import tempfile
from pathlib import Path

MAX_FILE_SIZE = 10 * 1024 * 1024  # 10 MB
MAX_DIRECTORY_DEPTH = 20  # Maximum depth of directory traversal
MAX_FILES = 10_000  # Maximum number of files to process
MAX_TOTAL_SIZE_BYTES = 500 * 1024 * 1024  # 500 MB
DEFAULT_TIMEOUT = 60  # seconds

OUTPUT_FILE_NAME = "digest.txt"

TMP_BASE_PATH = Path(tempfile.gettempdir()) / "gitingest"



================================================
FILE: src/gitingest/entrypoint.py
================================================
"""Main entry point for ingesting a source and processing its contents."""

import asyncio
import inspect
import os
import shutil
import sys
from typing import Optional, Set, Tuple, Union

from gitingest.cloning import clone_repo
from gitingest.config import TMP_BASE_PATH
from gitingest.ingestion import ingest_query
from gitingest.query_parsing import IngestionQuery, parse_query


async def ingest_async(
    source: str,
    max_file_size: int = 10 * 1024 * 1024,  # 10 MB
    include_patterns: Optional[Union[str, Set[str]]] = None,
    exclude_patterns: Optional[Union[str, Set[str]]] = None,
    branch: Optional[str] = None,
    token: Optional[str] = None,
    output: Optional[str] = None,
) -> Tuple[str, str, str]:
    """
    Main entry point for ingesting a source and processing its contents.

    This function analyzes a source (URL or local path), clones the corresponding repository (if applicable),
    and processes its files according to the specified query parameters. It returns a summary, a tree-like
    structure of the files, and the content of the files. The results can optionally be written to an output file.

    Parameters
    ----------
    source : str
        The source to analyze, which can be a URL (for a Git repository) or a local directory path.
    max_file_size : int
        Maximum allowed file size for file ingestion. Files larger than this size are ignored, by default
        10*1024*1024 (10 MB).
    include_patterns : Union[str, Set[str]], optional
        Pattern or set of patterns specifying which files to include. If `None`, all files are included.
    exclude_patterns : Union[str, Set[str]], optional
        Pattern or set of patterns specifying which files to exclude. If `None`, no files are excluded.
    branch : str, optional
        The branch to clone and ingest. If `None`, the default branch is used.
    token : str, optional
        GitHub personal-access token (PAT). Needed when *source* refers to a
        **private** repository. Can also be set via the ``GITHUB_TOKEN`` env var.
    output : str, optional
        File path where the summary and content should be written. If `None`, the results are not written to a file.

    Returns
    -------
    Tuple[str, str, str]
        A tuple containing:
        - A summary string of the analyzed repository or directory.
        - A tree-like string representation of the file structure.
        - The content of the files in the repository or directory.

    Raises
    ------
    TypeError
        If `clone_repo` does not return a coroutine, or if the `source` is of an unsupported type.
    """
    repo_cloned = False

    if not token:
        token = os.getenv("GITHUB_TOKEN")

    try:
        query: IngestionQuery = await parse_query(
            source=source,
            max_file_size=max_file_size,
            from_web=False,
            include_patterns=include_patterns,
            ignore_patterns=exclude_patterns,
        )

        if query.url:
            selected_branch = branch if branch else query.branch  # prioritize branch argument
            query.branch = selected_branch

            clone_config = query.extract_clone_config()
            clone_coroutine = clone_repo(clone_config, token=token)

            if inspect.iscoroutine(clone_coroutine):
                if asyncio.get_event_loop().is_running():
                    await clone_coroutine
                else:
                    asyncio.run(clone_coroutine)
            else:
                raise TypeError("clone_repo did not return a coroutine as expected.")

            repo_cloned = True

        summary, tree, content = ingest_query(query)

        if output == "-":
            loop = asyncio.get_running_loop()
            output_data = tree + "\n" + content
            await loop.run_in_executor(None, sys.stdout.write, output_data)
            await loop.run_in_executor(None, sys.stdout.flush)
        elif output is not None:
            with open(output, "w", encoding="utf-8") as f:
                f.write(tree + "\n" + content)

        return summary, tree, content
    finally:
        # Clean up the temporary directory if it was created
        if repo_cloned:
            shutil.rmtree(TMP_BASE_PATH, ignore_errors=True)


def ingest(
    source: str,
    max_file_size: int = 10 * 1024 * 1024,  # 10 MB
    include_patterns: Optional[Union[str, Set[str]]] = None,
    exclude_patterns: Optional[Union[str, Set[str]]] = None,
    branch: Optional[str] = None,
    token: Optional[str] = None,
    output: Optional[str] = None,
) -> Tuple[str, str, str]:
    """
    Synchronous version of ingest_async.

    This function analyzes a source (URL or local path), clones the corresponding repository (if applicable),
    and processes its files according to the specified query parameters. It returns a summary, a tree-like
    structure of the files, and the content of the files. The results can optionally be written to an output file.

    Parameters
    ----------
    source : str
        The source to analyze, which can be a URL (for a Git repository) or a local directory path.
    max_file_size : int
        Maximum allowed file size for file ingestion. Files larger than this size are ignored, by default
        10*1024*1024 (10 MB).
    include_patterns : Union[str, Set[str]], optional
        Pattern or set of patterns specifying which files to include. If `None`, all files are included.
    exclude_patterns : Union[str, Set[str]], optional
        Pattern or set of patterns specifying which files to exclude. If `None`, no files are excluded.
    branch : str, optional
        The branch to clone and ingest. If `None`, the default branch is used.
    token : str, optional
        GitHub personal-access token (PAT). Needed when *source* refers to a
        **private** repository. Can also be set via the ``GITHUB_TOKEN`` env var.
    output : str, optional
        File path where the summary and content should be written. If `None`, the results are not written to a file.

    Returns
    -------
    Tuple[str, str, str]
        A tuple containing:
        - A summary string of the analyzed repository or directory.
        - A tree-like string representation of the file structure.
        - The content of the files in the repository or directory.

    See Also
    --------
    ingest_async : The asynchronous version of this function.
    """
    return asyncio.run(
        ingest_async(
            source=source,
            max_file_size=max_file_size,
            include_patterns=include_patterns,
            exclude_patterns=exclude_patterns,
            branch=branch,
            token=token,
            output=output,
        )
    )



================================================
FILE: src/gitingest/ingestion.py
================================================
"""Functions to ingest and analyze a codebase directory or single file."""

import warnings
from pathlib import Path
from typing import Tuple

from gitingest.config import MAX_DIRECTORY_DEPTH, MAX_FILES, MAX_TOTAL_SIZE_BYTES
from gitingest.output_formatters import format_node
from gitingest.query_parsing import IngestionQuery
from gitingest.schemas import FileSystemNode, FileSystemNodeType, FileSystemStats
from gitingest.utils.ingestion_utils import _should_exclude, _should_include

try:
    import tomllib  # type: ignore[import]
except ImportError:
    import tomli as tomllib


def ingest_query(query: IngestionQuery) -> Tuple[str, str, str]:
    """
    Run the ingestion process for a parsed query.

    This is the main entry point for analyzing a codebase directory or single file. It processes the query
    parameters, reads the file or directory content, and generates a summary, directory structure, and file content,
    along with token estimations.

    Parameters
    ----------
    query : IngestionQuery
        The parsed query object containing information about the repository and query parameters.

    Returns
    -------
    Tuple[str, str, str]
        A tuple containing the summary, directory structure, and file contents.

    Raises
    ------
    ValueError
        If the path cannot be found, is not a file, or the file has no content.
    """
    subpath = Path(query.subpath.strip("/")).as_posix()
    path = query.local_path / subpath

    apply_gitingest_file(path, query)

    if not path.exists():
        raise ValueError(f"{query.slug} cannot be found")

    if (query.type and query.type == "blob") or query.local_path.is_file():
        # TODO: We do this wrong! We should still check the branch and commit!
        if not path.is_file():
            raise ValueError(f"Path {path} is not a file")

        relative_path = path.relative_to(query.local_path)

        file_node = FileSystemNode(
            name=path.name,
            type=FileSystemNodeType.FILE,
            size=path.stat().st_size,
            file_count=1,
            path_str=str(relative_path),
            path=path,
        )

        if not file_node.content:
            raise ValueError(f"File {file_node.name} has no content")

        return format_node(file_node, query)

    root_node = FileSystemNode(
        name=path.name,
        type=FileSystemNodeType.DIRECTORY,
        path_str=str(path.relative_to(query.local_path)),
        path=path,
    )

    stats = FileSystemStats()

    _process_node(
        node=root_node,
        query=query,
        stats=stats,
    )

    return format_node(root_node, query)


def apply_gitingest_file(path: Path, query: IngestionQuery) -> None:
    """
    Apply the .gitingest file to the query object.

    This function reads the .gitingest file in the specified path and updates the query object with the ignore
    patterns found in the file.

    Parameters
    ----------
    path : Path
        The path of the directory to ingest.
    query : IngestionQuery
        The parsed query object containing information about the repository and query parameters.
        It should have an attribute `ignore_patterns` which is either None or a set of strings.
    """
    path_gitingest = path / ".gitingest"

    if not path_gitingest.is_file():
        return

    try:
        with path_gitingest.open("rb") as f:
            data = tomllib.load(f)
    except tomllib.TOMLDecodeError as exc:
        warnings.warn(f"Invalid TOML in {path_gitingest}: {exc}", UserWarning)
        return

    config_section = data.get("config", {})
    ignore_patterns = config_section.get("ignore_patterns")

    if not ignore_patterns:
        return

    # If a single string is provided, make it a list of one element
    if isinstance(ignore_patterns, str):
        ignore_patterns = [ignore_patterns]

    if not isinstance(ignore_patterns, (list, set)):
        warnings.warn(
            f"Expected a list/set for 'ignore_patterns', got {type(ignore_patterns)} in {path_gitingest}. Skipping.",
            UserWarning,
        )
        return

    # Filter out duplicated patterns
    ignore_patterns = set(ignore_patterns)

    # Filter out any non-string entries
    valid_patterns = {pattern for pattern in ignore_patterns if isinstance(pattern, str)}
    invalid_patterns = ignore_patterns - valid_patterns

    if invalid_patterns:
        warnings.warn(f"Ignore patterns {invalid_patterns} are not strings. Skipping.", UserWarning)

    if not valid_patterns:
        return

    if query.ignore_patterns is None:
        query.ignore_patterns = valid_patterns
    else:
        query.ignore_patterns.update(valid_patterns)

    return


def _process_node(
    node: FileSystemNode,
    query: IngestionQuery,
    stats: FileSystemStats,
) -> None:
    """
    Process a file or directory item within a directory.

    This function handles each file or directory item, checking if it should be included or excluded based on the
    provided patterns. It handles symlinks, directories, and files accordingly.

    Parameters
    ----------
    node : FileSystemNode
        The current directory or file node being processed.
    query : IngestionQuery
        The parsed query object containing information about the repository and query parameters.
    stats : FileSystemStats
        Statistics tracking object for the total file count and size.
    """

    if limit_exceeded(stats, node.depth):
        return

    for sub_path in node.path.iterdir():

        if query.ignore_patterns and _should_exclude(sub_path, query.local_path, query.ignore_patterns):
            continue

        if query.include_patterns and not _should_include(sub_path, query.local_path, query.include_patterns):
            continue

        if sub_path.is_symlink():
            _process_symlink(path=sub_path, parent_node=node, stats=stats, local_path=query.local_path)
        elif sub_path.is_file():
            _process_file(path=sub_path, parent_node=node, stats=stats, local_path=query.local_path)
        elif sub_path.is_dir():

            child_directory_node = FileSystemNode(
                name=sub_path.name,
                type=FileSystemNodeType.DIRECTORY,
                path_str=str(sub_path.relative_to(query.local_path)),
                path=sub_path,
                depth=node.depth + 1,
            )

            _process_node(
                node=child_directory_node,
                query=query,
                stats=stats,
            )

            if not child_directory_node.children:
                continue

            node.children.append(child_directory_node)
            node.size += child_directory_node.size
            node.file_count += child_directory_node.file_count
            node.dir_count += 1 + child_directory_node.dir_count
        else:
            print(f"Warning: {sub_path} is an unknown file type, skipping")

    node.sort_children()


def _process_symlink(path: Path, parent_node: FileSystemNode, stats: FileSystemStats, local_path: Path) -> None:
    """
    Process a symlink in the file system.

    This function checks the symlink's target.

    Parameters
    ----------
    path : Path
        The full path of the symlink.
    parent_node : FileSystemNode
        The parent directory node.
    stats : FileSystemStats
        Statistics tracking object for the total file count and size.
    local_path : Path
        The base path of the repository or directory being processed.
    """
    child = FileSystemNode(
        name=path.name,
        type=FileSystemNodeType.SYMLINK,
        path_str=str(path.relative_to(local_path)),
        path=path,
        depth=parent_node.depth + 1,
    )
    stats.total_files += 1
    parent_node.children.append(child)
    parent_node.file_count += 1


def _process_file(path: Path, parent_node: FileSystemNode, stats: FileSystemStats, local_path: Path) -> None:
    """
    Process a file in the file system.

    This function checks the file's size, increments the statistics, and reads its content.
    If the file size exceeds the maximum allowed, it raises an error.

    Parameters
    ----------
    path : Path
        The full path of the file.
    parent_node : FileSystemNode
        The dictionary to accumulate the results.
    stats : FileSystemStats
        Statistics tracking object for the total file count and size.
    local_path : Path
        The base path of the repository or directory being processed.
    """
    file_size = path.stat().st_size
    if stats.total_size + file_size > MAX_TOTAL_SIZE_BYTES:
        print(f"Skipping file {path}: would exceed total size limit")
        return

    stats.total_files += 1
    stats.total_size += file_size

    if stats.total_files > MAX_FILES:
        print(f"Maximum file limit ({MAX_FILES}) reached")
        return

    child = FileSystemNode(
        name=path.name,
        type=FileSystemNodeType.FILE,
        size=file_size,
        file_count=1,
        path_str=str(path.relative_to(local_path)),
        path=path,
        depth=parent_node.depth + 1,
    )

    parent_node.children.append(child)
    parent_node.size += file_size
    parent_node.file_count += 1


def limit_exceeded(stats: FileSystemStats, depth: int) -> bool:
    """
    Check if any of the traversal limits have been exceeded.

    This function checks if the current traversal has exceeded any of the configured limits:
    maximum directory depth, maximum number of files, or maximum total size in bytes.

    Parameters
    ----------
    stats : FileSystemStats
        Statistics tracking object for the total file count and size.
    depth : int
        The current depth of directory traversal.

    Returns
    -------
    bool
        True if any limit has been exceeded, False otherwise.
    """
    if depth > MAX_DIRECTORY_DEPTH:
        print(f"Maximum depth limit ({MAX_DIRECTORY_DEPTH}) reached")
        return True

    if stats.total_files >= MAX_FILES:
        print(f"Maximum file limit ({MAX_FILES}) reached")
        return True  # TODO: end recursion

    if stats.total_size >= MAX_TOTAL_SIZE_BYTES:
        print(f"Maxumum total size limit ({MAX_TOTAL_SIZE_BYTES/1024/1024:.1f}MB) reached")
        return True  # TODO: end recursion

    return False



================================================
FILE: src/gitingest/output_formatters.py
================================================
"""Functions to ingest and analyze a codebase directory or single file."""

from typing import Optional, Tuple

import tiktoken

from gitingest.query_parsing import IngestionQuery
from gitingest.schemas import FileSystemNode, FileSystemNodeType


def format_node(node: FileSystemNode, query: IngestionQuery) -> Tuple[str, str, str]:
    """
    Generate a summary, directory structure, and file contents for a given file system node.

    If the node represents a directory, the function will recursively process its contents.

    Parameters
    ----------
    node : FileSystemNode
        The file system node to be summarized.
    query : IngestionQuery
        The parsed query object containing information about the repository and query parameters.

    Returns
    -------
    Tuple[str, str, str]
        A tuple containing the summary, directory structure, and file contents.
    """
    is_single_file = node.type == FileSystemNodeType.FILE
    summary = _create_summary_prefix(query, single_file=is_single_file)

    if node.type == FileSystemNodeType.DIRECTORY:
        summary += f"Files analyzed: {node.file_count}\n"
    elif node.type == FileSystemNodeType.FILE:
        summary += f"File: {node.name}\n"
        summary += f"Lines: {len(node.content.splitlines()):,}\n"

    tree = "Directory structure:\n" + _create_tree_structure(query, node)
    _create_tree_structure(query, node)

    content = _gather_file_contents(node)

    token_estimate = _format_token_count(tree + content)
    if token_estimate:
        summary += f"\nEstimated tokens: {token_estimate}"

    return summary, tree, content


def _create_summary_prefix(query: IngestionQuery, single_file: bool = False) -> str:
    """
    Create a prefix string for summarizing a repository or local directory.

    Includes repository name (if provided), commit/branch details, and subpath if relevant.

    Parameters
    ----------
    query : IngestionQuery
        The parsed query object containing information about the repository and query parameters.
    single_file : bool
        A flag indicating whether the summary is for a single file, by default False.

    Returns
    -------
    str
        A summary prefix string containing repository, commit, branch, and subpath details.
    """
    parts = []

    if query.user_name:
        parts.append(f"Repository: {query.user_name}/{query.repo_name}")
    else:
        # Local scenario
        parts.append(f"Directory: {query.slug}")

    if query.commit:
        parts.append(f"Commit: {query.commit}")
    elif query.branch and query.branch not in ("main", "master"):
        parts.append(f"Branch: {query.branch}")

    if query.subpath != "/" and not single_file:
        parts.append(f"Subpath: {query.subpath}")

    return "\n".join(parts) + "\n"


def _gather_file_contents(node: FileSystemNode) -> str:
    """
    Recursively gather contents of all files under the given node.

    This function recursively processes a directory node and gathers the contents of all files
    under that node. It returns the concatenated content of all files as a single string.

    Parameters
    ----------
    node : FileSystemNode
        The current directory or file node being processed.

    Returns
    -------
    str
        The concatenated content of all files under the given node.
    """
    if node.type != FileSystemNodeType.DIRECTORY:
        return node.content_string

    # Recursively gather contents of all files under the current directory
    return "\n".join(_gather_file_contents(child) for child in node.children)


def _create_tree_structure(query: IngestionQuery, node: FileSystemNode, prefix: str = "", is_last: bool = True) -> str:
    """
    Generate a tree-like string representation of the file structure.

    This function generates a string representation of the directory structure, formatted
    as a tree with appropriate indentation for nested directories and files.

    Parameters
    ----------
    query : IngestionQuery
        The parsed query object containing information about the repository and query parameters.
    node : FileSystemNode
        The current directory or file node being processed.
    prefix : str
        A string used for indentation and formatting of the tree structure, by default "".
    is_last : bool
        A flag indicating whether the current node is the last in its directory, by default True.

    Returns
    -------
    str
        A string representing the directory structure formatted as a tree.
    """
    if not node.name:
        # If no name is present, use the slug as the top-level directory name
        node.name = query.slug

    tree_str = ""
    current_prefix = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "

    # Indicate directories with a trailing slash
    display_name = node.name
    if node.type == FileSystemNodeType.DIRECTORY:
        display_name += "/"
    elif node.type == FileSystemNodeType.SYMLINK:
        display_name += " -> " + node.path.readlink().name

    tree_str += f"{prefix}{current_prefix}{display_name}\n"

    if node.type == FileSystemNodeType.DIRECTORY and node.children:
        prefix += "    " if is_last else "‚îÇ   "
        for i, child in enumerate(node.children):
            tree_str += _create_tree_structure(query, node=child, prefix=prefix, is_last=i == len(node.children) - 1)
    return tree_str


def _format_token_count(text: str) -> Optional[str]:
    """
    Return a human-readable string representing the token count of the given text.

    E.g., '120' -> '120', '1200' -> '1.2k', '1200000' -> '1.2M'.

    Parameters
    ----------
    text : str
        The text string for which the token count is to be estimated.

    Returns
    -------
    str, optional
        The formatted number of tokens as a string (e.g., '1.2k', '1.2M'), or `None` if an error occurs.
    """
    try:
        encoding = tiktoken.get_encoding("o200k_base")  # gpt-4o, gpt-4o-mini
        total_tokens = len(encoding.encode(text, disallowed_special=()))
    except (ValueError, UnicodeEncodeError) as exc:
        print(exc)
        return None

    if total_tokens >= 1_000_000:
        return f"{total_tokens / 1_000_000:.1f}M"

    if total_tokens >= 1_000:
        return f"{total_tokens / 1_000:.1f}k"

    return str(total_tokens)



================================================
FILE: src/gitingest/query_parsing.py
================================================
"""This module contains functions to parse and validate input sources and patterns."""

import re
import uuid
import warnings
from pathlib import Path
from typing import List, Optional, Set, Union
from urllib.parse import unquote, urlparse

from gitingest.config import TMP_BASE_PATH
from gitingest.schemas import IngestionQuery
from gitingest.utils.exceptions import InvalidPatternError
from gitingest.utils.git_utils import check_repo_exists, fetch_remote_branch_list
from gitingest.utils.ignore_patterns import DEFAULT_IGNORE_PATTERNS
from gitingest.utils.query_parser_utils import (
    KNOWN_GIT_HOSTS,
    _get_user_and_repo_from_path,
    _is_valid_git_commit_hash,
    _is_valid_pattern,
    _normalize_pattern,
    _validate_host,
    _validate_url_scheme,
)


async def parse_query(
    source: str,
    max_file_size: int,
    from_web: bool,
    include_patterns: Optional[Union[str, Set[str]]] = None,
    ignore_patterns: Optional[Union[str, Set[str]]] = None,
) -> IngestionQuery:
    """
    Parse the input source (URL or path) to extract relevant details for the query.

    This function parses the input source to extract details such as the username, repository name,
    commit hash, branch name, and other relevant information. It also processes the include and ignore
    patterns to filter the files and directories to include or exclude from the query.

    Parameters
    ----------
    source : str
        The source URL or file path to parse.
    max_file_size : int
        The maximum file size in bytes to include.
    from_web : bool
        Flag indicating whether the source is a web URL.
    include_patterns : Union[str, Set[str]], optional
        Patterns to include, by default None. Can be a set of strings or a single string.
    ignore_patterns : Union[str, Set[str]], optional
        Patterns to ignore, by default None. Can be a set of strings or a single string.

    Returns
    -------
    IngestionQuery
        A dataclass object containing the parsed details of the repository or file path.
    """

    # Determine the parsing method based on the source type
    if from_web or urlparse(source).scheme in ("https", "http") or any(h in source for h in KNOWN_GIT_HOSTS):
        # We either have a full URL or a domain-less slug
        query = await _parse_remote_repo(source)
    else:
        # Local path scenario
        query = _parse_local_dir_path(source)

    # Combine default ignore patterns + custom patterns
    ignore_patterns_set = DEFAULT_IGNORE_PATTERNS.copy()
    if ignore_patterns:
        ignore_patterns_set.update(_parse_patterns(ignore_patterns))

    # Process include patterns and override ignore patterns accordingly
    if include_patterns:
        parsed_include = _parse_patterns(include_patterns)
        # Override ignore patterns with include patterns
        ignore_patterns_set = set(ignore_patterns_set) - set(parsed_include)
    else:
        parsed_include = None

    return IngestionQuery(
        user_name=query.user_name,
        repo_name=query.repo_name,
        url=query.url,
        subpath=query.subpath,
        local_path=query.local_path,
        slug=query.slug,
        id=query.id,
        type=query.type,
        branch=query.branch,
        commit=query.commit,
        max_file_size=max_file_size,
        ignore_patterns=ignore_patterns_set,
        include_patterns=parsed_include,
    )


async def _parse_remote_repo(source: str, token: Optional[str] = None) -> IngestionQuery:
    """
    Parse a repository URL into a structured query dictionary.

    If source is:
      - A fully qualified URL (https://gitlab.com/...), parse & verify that domain
      - A URL missing 'https://' (gitlab.com/...), add 'https://' and parse
      - A 'slug' (like 'pandas-dev/pandas'), attempt known domains until we find one that exists.

    Parameters
    ----------
    source : str
        The URL or domain-less slug to parse.
    token : str, optional
        GitHub personal-access token (PAT). Needed when *source* refers to a
        **private** repository. Can also be set via the ``GITHUB_TOKEN`` env var.

    Returns
    -------
    IngestionQuery
        A dictionary containing the parsed details of the repository.
    """
    source = unquote(source)

    # Attempt to parse
    parsed_url = urlparse(source)

    if parsed_url.scheme:
        _validate_url_scheme(parsed_url.scheme)
        _validate_host(parsed_url.netloc.lower())

    else:  # Will be of the form 'host/user/repo' or 'user/repo'
        tmp_host = source.split("/")[0].lower()
        if "." in tmp_host:
            _validate_host(tmp_host)
        else:
            # No scheme, no domain => user typed "user/repo", so we'll guess the domain.
            host = await try_domains_for_user_and_repo(*_get_user_and_repo_from_path(source), token=token)
            source = f"{host}/{source}"

        source = "https://" + source
        parsed_url = urlparse(source)

    host = parsed_url.netloc.lower()
    user_name, repo_name = _get_user_and_repo_from_path(parsed_url.path)

    _id = str(uuid.uuid4())
    slug = f"{user_name}-{repo_name}"
    local_path = TMP_BASE_PATH / _id / slug
    url = f"https://{host}/{user_name}/{repo_name}"

    parsed = IngestionQuery(
        user_name=user_name,
        repo_name=repo_name,
        url=url,
        local_path=local_path,
        slug=slug,
        id=_id,
    )

    remaining_parts = parsed_url.path.strip("/").split("/")[2:]

    if not remaining_parts:
        return parsed

    possible_type = remaining_parts.pop(0)  # e.g. 'issues', 'pull', 'tree', 'blob'

    # If no extra path parts, just return
    if not remaining_parts:
        return parsed

    # If this is an issues page or pull requests, return early without processing subpath
    if remaining_parts and possible_type in ("issues", "pull"):
        return parsed

    parsed.type = possible_type

    # Commit or branch
    commit_or_branch = remaining_parts[0]
    if _is_valid_git_commit_hash(commit_or_branch):
        parsed.commit = commit_or_branch
        remaining_parts.pop(0)
    else:
        parsed.branch = await _configure_branch_and_subpath(remaining_parts, url)

    # Subpath if anything left
    if remaining_parts:
        parsed.subpath += "/".join(remaining_parts)

    return parsed


async def _configure_branch_and_subpath(remaining_parts: List[str], url: str) -> Optional[str]:
    """
    Configure the branch and subpath based on the remaining parts of the URL.
    Parameters
    ----------
    remaining_parts : List[str]
        The remaining parts of the URL path.
    url : str
        The URL of the repository.
    Returns
    -------
    str, optional
        The branch name if found, otherwise None.

    """
    try:
        # Fetch the list of branches from the remote repository
        branches: List[str] = await fetch_remote_branch_list(url)
    except RuntimeError as exc:
        warnings.warn(f"Warning: Failed to fetch branch list: {exc}", RuntimeWarning)
        return remaining_parts.pop(0)

    branch = []
    while remaining_parts:
        branch.append(remaining_parts.pop(0))
        branch_name = "/".join(branch)
        if branch_name in branches:
            return branch_name

    return None


def _parse_patterns(pattern: Union[str, Set[str]]) -> Set[str]:
    """
    Parse and validate file/directory patterns for inclusion or exclusion.

    Takes either a single pattern string or set of pattern strings and processes them into a normalized list.
    Patterns are split on commas and spaces, validated for allowed characters, and normalized.

    Parameters
    ----------
    pattern : Set[str] | str
        Pattern(s) to parse - either a single string or set of strings

    Returns
    -------
    Set[str]
        A set of normalized patterns.

    Raises
    ------
    InvalidPatternError
        If any pattern contains invalid characters. Only alphanumeric characters,
        dash (-), underscore (_), dot (.), forward slash (/), plus (+), and
        asterisk (*) are allowed.
    """
    patterns = pattern if isinstance(pattern, set) else {pattern}

    parsed_patterns: Set[str] = set()
    for p in patterns:
        parsed_patterns = parsed_patterns.union(set(re.split(",| ", p)))

    # Remove empty string if present
    parsed_patterns = parsed_patterns - {""}

    # Normalize Windows paths to Unix-style paths
    parsed_patterns = {p.replace("\\", "/") for p in parsed_patterns}

    # Validate and normalize each pattern
    for p in parsed_patterns:
        if not _is_valid_pattern(p):
            raise InvalidPatternError(p)

    return {_normalize_pattern(p) for p in parsed_patterns}


def _parse_local_dir_path(path_str: str) -> IngestionQuery:
    """
    Parse the given file path into a structured query dictionary.

    Parameters
    ----------
    path_str : str
        The file path to parse.

    Returns
    -------
    IngestionQuery
        A dictionary containing the parsed details of the file path.
    """
    path_obj = Path(path_str).resolve()
    slug = path_obj.name if path_str == "." else path_str.strip("/")
    return IngestionQuery(
        user_name=None,
        repo_name=None,
        url=None,
        local_path=path_obj,
        slug=slug,
        id=str(uuid.uuid4()),
    )


async def try_domains_for_user_and_repo(user_name: str, repo_name: str, token: Optional[str] = None) -> str:
    """
    Attempt to find a valid repository host for the given user_name and repo_name.

    Parameters
    ----------
    user_name : str
        The username or owner of the repository.
    repo_name : str
        The name of the repository.
    token : str, optional
        GitHub personal-access token (PAT). Needed when *source* refers to a
        **private** repository. Can also be set via the ``GITHUB_TOKEN`` env var.

    Returns
    -------
    str
        The domain of the valid repository host.

    Raises
    ------
    ValueError
        If no valid repository host is found for the given user_name and repo_name.
    """
    for domain in KNOWN_GIT_HOSTS:
        candidate = f"https://{domain}/{user_name}/{repo_name}"
        if await check_repo_exists(candidate, token=token if domain == "github.com" else None):
            return domain
    raise ValueError(f"Could not find a valid repository host for '{user_name}/{repo_name}'.")



================================================
FILE: src/gitingest/schemas/__init__.py
================================================
"""This module contains the schemas for the Gitingest package."""

from gitingest.schemas.filesystem_schema import FileSystemNode, FileSystemNodeType, FileSystemStats
from gitingest.schemas.ingestion_schema import CloneConfig, IngestionQuery

__all__ = ["FileSystemNode", "FileSystemNodeType", "FileSystemStats", "CloneConfig", "IngestionQuery"]



================================================
FILE: src/gitingest/schemas/filesystem_schema.py
================================================
"""Define the schema for the filesystem representation."""

from __future__ import annotations

import os
from dataclasses import dataclass, field
from enum import Enum, auto
from pathlib import Path

from gitingest.utils.file_utils import get_preferred_encodings, is_text_file
from gitingest.utils.notebook_utils import process_notebook

SEPARATOR = "=" * 48  # Tiktoken, the tokenizer openai uses, counts 2 tokens if we have more than 48


class FileSystemNodeType(Enum):
    """Enum representing the type of a file system node (directory or file)."""

    DIRECTORY = auto()
    FILE = auto()
    SYMLINK = auto()


@dataclass
class FileSystemStats:
    """Class for tracking statistics during file system traversal."""

    visited: set[Path] = field(default_factory=set)
    total_files: int = 0
    total_size: int = 0


@dataclass
class FileSystemNode:  # pylint: disable=too-many-instance-attributes
    """
    Class representing a node in the file system (either a file or directory).

    Tracks properties of files/directories for comprehensive analysis.
    """

    name: str
    type: FileSystemNodeType
    path_str: str
    path: Path
    size: int = 0
    file_count: int = 0
    dir_count: int = 0
    depth: int = 0
    children: list[FileSystemNode] = field(default_factory=list)

    def sort_children(self) -> None:
        """
        Sort the children nodes of a directory according to a specific order.

        Order of sorting:
          2. Regular files (not starting with dot)
          3. Hidden files (starting with dot)
          4. Regular directories (not starting with dot)
          5. Hidden directories (starting with dot)

        All groups are sorted alphanumerically within themselves.

        Raises
        ------
        ValueError
            If the node is not a directory.
        """
        if self.type != FileSystemNodeType.DIRECTORY:
            raise ValueError("Cannot sort children of a non-directory node")

        def _sort_key(child: FileSystemNode) -> tuple[int, str]:
            # returns the priority order for the sort function, 0 is first
            # Groups: 0=README, 1=regular file, 2=hidden file, 3=regular dir, 4=hidden dir
            name = child.name.lower()
            if child.type == FileSystemNodeType.FILE:
                if name == "readme.md":
                    return (0, name)
                return (1 if not name.startswith(".") else 2, name)
            return (3 if not name.startswith(".") else 4, name)

        self.children.sort(key=_sort_key)

    @property
    def content_string(self) -> str:
        """
        Return the content of the node as a string, including path and content.

        Returns
        -------
        str
            A string representation of the node's content.
        """
        parts = [
            SEPARATOR,
            f"{self.type.name}: {str(self.path_str).replace(os.sep, '/')}"
            + (f" -> {self.path.readlink().name}" if self.type == FileSystemNodeType.SYMLINK else ""),
            SEPARATOR,
            f"{self.content}",
        ]

        return "\n".join(parts) + "\n\n"

    @property
    def content(self) -> str:  # pylint: disable=too-many-return-statements
        """
        Read the content of a file if it's text (or a notebook). Return an error message otherwise.

        Returns
        -------
        str
            The content of the file, or an error message if the file could not be read.

        Raises
        ------
        ValueError
            If the node is a directory.
        """
        if self.type == FileSystemNodeType.DIRECTORY:
            raise ValueError("Cannot read content of a directory node")

        if self.type == FileSystemNodeType.SYMLINK:
            return ""

        if not is_text_file(self.path):
            return "[Non-text file]"

        if self.path.suffix == ".ipynb":
            try:
                return process_notebook(self.path)
            except Exception as exc:
                return f"Error processing notebook: {exc}"

        # Try multiple encodings
        for encoding in get_preferred_encodings():
            try:
                with self.path.open(encoding=encoding) as f:
                    return f.read()
            except UnicodeDecodeError:
                continue
            except UnicodeError:
                continue
            except OSError as exc:
                return f"Error reading file: {exc}"

        return "Error: Unable to decode file with available encodings"



================================================
FILE: src/gitingest/schemas/ingestion_schema.py
================================================
"""This module contains the dataclasses for the ingestion process."""

from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Set

from pydantic import BaseModel, ConfigDict, Field

from gitingest.config import MAX_FILE_SIZE


@dataclass
class CloneConfig:
    """
    Configuration for cloning a Git repository.

    This class holds the necessary parameters for cloning a repository to a local path, including
    the repository's URL, the target local path, and optional parameters for a specific commit or branch.

    Attributes
    ----------
    url : str
        The URL of the Git repository to clone.
    local_path : str
        The local directory where the repository will be cloned.
    commit : str, optional
        The specific commit hash to check out after cloning (default is None).
    branch : str, optional
        The branch to clone (default is None).
    subpath : str
        The subpath to clone from the repository (default is "/").
    blob: bool
        Whether the repository is a blob (default is False).
    """

    url: str
    local_path: str
    commit: Optional[str] = None
    branch: Optional[str] = None
    subpath: str = "/"
    blob: bool = False


class IngestionQuery(BaseModel):  # pylint: disable=too-many-instance-attributes
    """
    Pydantic model to store the parsed details of the repository or file path.
    """

    user_name: Optional[str] = None
    repo_name: Optional[str] = None
    local_path: Path
    url: Optional[str] = None
    slug: str
    id: str
    subpath: str = "/"
    type: Optional[str] = None
    branch: Optional[str] = None
    commit: Optional[str] = None
    max_file_size: int = Field(default=MAX_FILE_SIZE)
    ignore_patterns: Optional[Set[str]] = None
    include_patterns: Optional[Set[str]] = None

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def extract_clone_config(self) -> CloneConfig:
        """
        Extract the relevant fields for the CloneConfig object.

        Returns
        -------
        CloneConfig
            A CloneConfig object containing the relevant fields.

        Raises
        ------
        ValueError
            If the 'url' parameter is not provided.
        """
        if not self.url:
            raise ValueError("The 'url' parameter is required.")

        return CloneConfig(
            url=self.url,
            local_path=str(self.local_path),
            commit=self.commit,
            branch=self.branch,
            subpath=self.subpath,
            blob=self.type == "blob",
        )



================================================
FILE: src/gitingest/utils/__init__.py
================================================



================================================
FILE: src/gitingest/utils/exceptions.py
================================================
"""Custom exceptions for the Gitingest package."""


class InvalidPatternError(ValueError):
    """
    Exception raised when a pattern contains invalid characters.
    This exception is used to signal that a pattern provided for some operation
    contains characters that are not allowed. The valid characters for the pattern
    include alphanumeric characters, dash (-), underscore (_), dot (.), forward slash (/),
    plus (+), and asterisk (*).
    Parameters
    ----------
    pattern : str
        The invalid pattern that caused the error.
    """

    def __init__(self, pattern: str) -> None:
        super().__init__(
            f"Pattern '{pattern}' contains invalid characters. Only alphanumeric characters, dash (-), "
            "underscore (_), dot (.), forward slash (/), plus (+), and asterisk (*) are allowed."
        )


class AsyncTimeoutError(Exception):
    """
    Exception raised when an async operation exceeds its timeout limit.

    This exception is used by the `async_timeout` decorator to signal that the wrapped
    asynchronous function has exceeded the specified time limit for execution.
    """


class InvalidNotebookError(Exception):
    """Exception raised when a Jupyter notebook is invalid or cannot be processed."""

    def __init__(self, message: str) -> None:
        super().__init__(message)



================================================
FILE: src/gitingest/utils/file_utils.py
================================================
"""Utility functions for working with files and directories."""

import locale
import platform
from pathlib import Path
from typing import List

try:
    locale.setlocale(locale.LC_ALL, "")
except locale.Error:
    locale.setlocale(locale.LC_ALL, "C")


def get_preferred_encodings() -> List[str]:
    """
    Get list of encodings to try, prioritized for the current platform.

    Returns
    -------
    List[str]
        List of encoding names to try in priority order, starting with the
        platform's default encoding followed by common fallback encodings.
    """
    encodings = [locale.getpreferredencoding(), "utf-8", "utf-16", "utf-16le", "utf-8-sig", "latin"]
    if platform.system() == "Windows":
        encodings += ["cp1252", "iso-8859-1"]
    return encodings


def is_text_file(path: Path) -> bool:
    """
    Determine if the file is likely a text file by trying to decode a small chunk
    with multiple encodings, and checking for common binary markers.

    Parameters
    ----------
    path : Path
        The path to the file to check.

    Returns
    -------
    bool
        True if the file is likely textual; False if it appears to be binary.
    """

    # Attempt to read a portion of the file in binary mode
    try:
        with path.open("rb") as f:
            chunk = f.read(1024)
    except OSError:
        return False

    # If file is empty, treat as text
    if not chunk:
        return True

    # Check obvious binary bytes
    if b"\x00" in chunk or b"\xff" in chunk:
        return False

    # Attempt multiple encodings
    for enc in get_preferred_encodings():
        try:
            with path.open(encoding=enc) as f:
                f.read()
                return True
        except UnicodeDecodeError:
            continue
        except UnicodeError:
            continue
        except OSError:
            return False

    return False



================================================
FILE: src/gitingest/utils/git_utils.py
================================================
"""Utility functions for interacting with Git repositories."""

import asyncio
import base64
import re
from typing import List, Optional, Tuple

GITHUB_PAT_PATTERN = r"^(?:github_pat_|ghp_)[A-Za-z0-9_]{36,}$"


async def run_command(*args: str) -> Tuple[bytes, bytes]:
    """
    Execute a shell command asynchronously and return (stdout, stderr) bytes.

    Parameters
    ----------
    *args : str
        The command and its arguments to execute.

    Returns
    -------
    Tuple[bytes, bytes]
        A tuple containing the stdout and stderr of the command.

    Raises
    ------
    RuntimeError
        If command exits with a non-zero status.
    """
    # Execute the requested command
    proc = await asyncio.create_subprocess_exec(
        *args,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await proc.communicate()
    if proc.returncode != 0:
        error_message = stderr.decode().strip()
        raise RuntimeError(f"Command failed: {' '.join(args)}\nError: {error_message}")

    return stdout, stderr


async def ensure_git_installed() -> None:
    """
    Ensure Git is installed and accessible on the system.

    Raises
    ------
    RuntimeError
        If Git is not installed or not accessible.
    """
    try:
        await run_command("git", "--version")
    except RuntimeError as exc:
        raise RuntimeError("Git is not installed or not accessible. Please install Git first.") from exc


async def check_repo_exists(url: str, token: Optional[str] = None) -> bool:
    """
    Check if a Git repository exists at the provided URL.

    Parameters
    ----------
    url : str
        The URL of the Git repository to check.
    token : str, optional
        GitHub personal-access token (PAT). Needed when *source* refers to a
        **private** repository. Can also be set via the ``GITHUB_TOKEN`` env var.

    Returns
    -------
    bool
        True if the repository exists, False otherwise.

    Raises
    ------
    RuntimeError
        If the curl command returns an unexpected status code.
    """
    if token and "github.com" in url:
        return await _check_github_repo_exists(url, token)

    proc = await asyncio.create_subprocess_exec(
        "curl",
        "-I",
        url,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, _ = await proc.communicate()

    if proc.returncode != 0:
        return False  # likely unreachable or private

    response = stdout.decode()
    status_line = response.splitlines()[0].strip()
    parts = status_line.split(" ")
    if len(parts) >= 2:
        status_code_str = parts[1]
        if status_code_str in ("200", "301"):
            return True
        if status_code_str in ("302", "404"):
            return False
    raise RuntimeError(f"Unexpected status line: {status_line}")


async def _check_github_repo_exists(url: str, token: Optional[str] = None) -> bool:
    """
    Return True iff the authenticated user can see `url`.

    Parameters
    ----------
    url : str
        The URL of the GitHub repository to check.
    token : str, optional
        GitHub personal-access token (PAT). Needed when *source* refers to a
        **private** repository. Can also be set via the ``GITHUB_TOKEN`` env var.

    Returns
    -------
    bool
        True if the repository exists, False otherwise.

    Raises
    ------
    ValueError
        If the URL is not a valid GitHub repository URL.
    RuntimeError
        If the repository is not found, if the provided URL is invalid, or if the token format is invalid.
    """
    m = re.match(r"https?://github\.com/([^/]+)/([^/]+?)(?:\.git)?/?$", url)
    if not m:
        raise ValueError(f"Un-recognised GitHub URL: {url!r}")
    owner, repo = m.groups()

    api = f"https://api.github.com/repos/{owner}/{repo}"
    cmd = [
        "curl",
        "--silent",
        "--location",
        "--write-out",
        "%{http_code}",
        "-o",
        "/dev/null",
        "-H",
        "Accept: application/vnd.github+json",
    ]
    if token:
        cmd += ["-H", f"Authorization: Bearer {token}"]
    cmd.append(api)

    proc = await asyncio.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, _ = await proc.communicate()
    status = stdout.decode()[-3:]  # just the %{http_code}

    if status == "200":
        return True
    if status == "404":
        return False
    if status in ("401", "403"):
        raise RuntimeError("Token invalid or lacks permissions")
    raise RuntimeError(f"GitHub API returned unexpected HTTP {status}")


async def fetch_remote_branch_list(url: str, token: Optional[str] = None) -> List[str]:
    """
    Fetch the list of branches from a remote Git repository.

    Parameters
    ----------
    url : str
        The URL of the Git repository to fetch branches from.
    token : str, optional
        GitHub personal-access token (PAT). Needed when *source* refers to a
        **private** repository. Can also be set via the ``GITHUB_TOKEN`` env var.

    Returns
    -------
    List[str]
        A list of branch names available in the remote repository.
    """
    fetch_branches_command = ["git"]

    # Add authentication if needed
    if token and "github.com" in url:
        fetch_branches_command += ["-c", create_git_auth_header(token)]

    fetch_branches_command += ["ls-remote", "--heads", url]

    await ensure_git_installed()
    stdout, _ = await run_command(*fetch_branches_command)
    stdout_decoded = stdout.decode()

    return [
        line.split("refs/heads/", 1)[1]
        for line in stdout_decoded.splitlines()
        if line.strip() and "refs/heads/" in line
    ]


def create_git_command(base_cmd: List[str], local_path: str, url: str, token: Optional[str] = None) -> List[str]:
    """Create a git command with authentication if needed.

    Parameters
    ----------
    base_cmd : List[str]
        The base git command to start with
    local_path : str
        The local path where the git command should be executed
    url : str
        The repository URL to check if it's a GitHub repository
    token : Optional[str]
        GitHub personal access token for authentication

    Returns
    -------
    List[str]
        The git command with authentication if needed
    """
    cmd = base_cmd + ["-C", local_path]
    if token and url.startswith("https://github.com"):
        validate_github_token(token)
        cmd += ["-c", create_git_auth_header(token)]
    return cmd


def create_git_auth_header(token: str) -> str:
    """Create a Basic authentication header for GitHub git operations.

    Parameters
    ----------
    token : str
        GitHub personal access token

    Returns
    -------
    str
        The git config command for setting the authentication header
    """
    basic = base64.b64encode(f"x-oauth-basic:{token}".encode()).decode()
    return f"http.https://github.com/.extraheader=Authorization: Basic {basic}"


def validate_github_token(token: str) -> None:
    """Validate the format of a GitHub Personal Access Token.

    Parameters
    ----------
    token : str
        The GitHub token to validate

    Raises
    ------
    ValueError
        If the token format is invalid
    """
    if not re.match(GITHUB_PAT_PATTERN, token):
        raise ValueError(
            "Invalid GitHub token format. Token should start with 'github_pat_' or 'ghp_' "
            "followed by at least 36 characters of letters, numbers, and underscores."
        )



================================================
FILE: src/gitingest/utils/ignore_patterns.py
================================================
"""Default ignore patterns for Gitingest."""

from typing import Set

DEFAULT_IGNORE_PATTERNS: Set[str] = {
    # Python
    "*.pyc",
    "*.pyo",
    "*.pyd",
    "__pycache__",
    ".pytest_cache",
    ".coverage",
    ".tox",
    ".nox",
    ".mypy_cache",
    ".ruff_cache",
    ".hypothesis",
    "poetry.lock",
    "Pipfile.lock",
    # JavaScript/FileSystemNode
    "node_modules",
    "bower_components",
    "package-lock.json",
    "yarn.lock",
    ".npm",
    ".yarn",
    ".pnpm-store",
    "bun.lock",
    "bun.lockb",
    # Java
    "*.class",
    "*.jar",
    "*.war",
    "*.ear",
    "*.nar",
    ".gradle/",
    "build/",
    ".settings/",
    ".classpath",
    "gradle-app.setting",
    "*.gradle",
    # IDEs and editors / Java
    ".project",
    # C/C++
    "*.o",
    "*.obj",
    "*.dll",
    "*.dylib",
    "*.exe",
    "*.lib",
    "*.out",
    "*.a",
    "*.pdb",
    # Swift/Xcode
    ".build/",
    "*.xcodeproj/",
    "*.xcworkspace/",
    "*.pbxuser",
    "*.mode1v3",
    "*.mode2v3",
    "*.perspectivev3",
    "*.xcuserstate",
    "xcuserdata/",
    ".swiftpm/",
    # Ruby
    "*.gem",
    ".bundle/",
    "vendor/bundle",
    "Gemfile.lock",
    ".ruby-version",
    ".ruby-gemset",
    ".rvmrc",
    # Rust
    "Cargo.lock",
    "**/*.rs.bk",
    # Java / Rust
    "target/",
    # Go
    "pkg/",
    # .NET/C#
    "obj/",
    "*.suo",
    "*.user",
    "*.userosscache",
    "*.sln.docstates",
    "packages/",
    "*.nupkg",
    # Go / .NET / C#
    "bin/",
    # Version control
    ".git",
    ".svn",
    ".hg",
    ".gitignore",
    ".gitattributes",
    ".gitmodules",
    # Images and media
    "*.svg",
    "*.png",
    "*.jpg",
    "*.jpeg",
    "*.gif",
    "*.ico",
    "*.pdf",
    "*.mov",
    "*.mp4",
    "*.mp3",
    "*.wav",
    # Virtual environments
    "venv",
    ".venv",
    "env",
    ".env",
    "virtualenv",
    # IDEs and editors
    ".idea",
    ".vscode",
    ".vs",
    "*.swo",
    "*.swn",
    ".settings",
    "*.sublime-*",
    # Temporary and cache files
    "*.log",
    "*.bak",
    "*.swp",
    "*.tmp",
    "*.temp",
    ".cache",
    ".sass-cache",
    ".eslintcache",
    ".DS_Store",
    "Thumbs.db",
    "desktop.ini",
    # Build directories and artifacts
    "build",
    "dist",
    "target",
    "out",
    "*.egg-info",
    "*.egg",
    "*.whl",
    "*.so",
    # Documentation
    "site-packages",
    ".docusaurus",
    ".next",
    ".nuxt",
    # Other common patterns
    ## Minified files
    "*.min.js",
    "*.min.css",
    ## Source maps
    "*.map",
    ## Terraform
    ".terraform",
    "*.tfstate*",
    ## Dependencies in various languages
    "vendor/",
    # Gitingest
    "digest.txt",
}



================================================
FILE: src/gitingest/utils/ingestion_utils.py
================================================
"""Utility functions for the ingestion process."""

from fnmatch import fnmatch
from pathlib import Path
from typing import Set


def _should_include(path: Path, base_path: Path, include_patterns: Set[str]) -> bool:
    """
    Determine if the given file or directory path matches any of the include patterns.

    This function checks whether the relative path of a file or directory matches any of the specified patterns. If a
    match is found, it returns `True`, indicating that the file or directory should be included in further processing.

    Parameters
    ----------
    path : Path
        The absolute path of the file or directory to check.
    base_path : Path
        The base directory from which the relative path is calculated.
    include_patterns : Set[str]
        A set of patterns to check against the relative path.

    Returns
    -------
    bool
        `True` if the path matches any of the include patterns, `False` otherwise.
    """
    try:
        rel_path = path.relative_to(base_path)
    except ValueError:
        # If path is not under base_path at all
        return False

    rel_str = str(rel_path)

    # if path is a directory, include it by default
    if path.is_dir():
        return True

    for pattern in include_patterns:
        if fnmatch(rel_str, pattern):
            return True
    return False


def _should_exclude(path: Path, base_path: Path, ignore_patterns: Set[str]) -> bool:
    """
    Determine if the given file or directory path matches any of the ignore patterns.

    This function checks whether the relative path of a file or directory matches
    any of the specified ignore patterns. If a match is found, it returns `True`, indicating
    that the file or directory should be excluded from further processing.

    Parameters
    ----------
    path : Path
        The absolute path of the file or directory to check.
    base_path : Path
        The base directory from which the relative path is calculated.
    ignore_patterns : Set[str]
        A set of patterns to check against the relative path.

    Returns
    -------
    bool
        `True` if the path matches any of the ignore patterns, `False` otherwise.
    """
    try:
        rel_path = path.relative_to(base_path)
    except ValueError:
        # If path is not under base_path at all
        return True

    rel_str = str(rel_path)
    for pattern in ignore_patterns:
        if pattern and fnmatch(rel_str, pattern):
            return True
    return False



================================================
FILE: src/gitingest/utils/notebook_utils.py
================================================
"""Utilities for processing Jupyter notebooks."""

import json
import warnings
from itertools import chain
from pathlib import Path
from typing import Any, Dict, List, Optional

from gitingest.utils.exceptions import InvalidNotebookError


def process_notebook(file: Path, include_output: bool = True) -> str:
    """
    Process a Jupyter notebook file and return an executable Python script as a string.

    Parameters
    ----------
    file : Path
        The path to the Jupyter notebook file.
    include_output : bool
        Whether to include cell outputs in the generated script, by default True.

    Returns
    -------
    str
        The executable Python script as a string.

    Raises
    ------
    InvalidNotebookError
        If the notebook file is invalid or cannot be processed.
    """
    try:
        with file.open(encoding="utf-8") as f:
            notebook: Dict[str, Any] = json.load(f)
    except json.JSONDecodeError as exc:
        raise InvalidNotebookError(f"Invalid JSON in notebook: {file}") from exc

    # Check if the notebook contains worksheets
    worksheets = notebook.get("worksheets")
    if worksheets:
        warnings.warn(
            "Worksheets are deprecated as of IPEP-17. Consider updating the notebook. "
            "(See: https://github.com/jupyter/nbformat and "
            "https://github.com/ipython/ipython/wiki/IPEP-17:-Notebook-Format-4#remove-multiple-worksheets "
            "for more information.)",
            DeprecationWarning,
        )

        if len(worksheets) > 1:
            warnings.warn("Multiple worksheets detected. Combining all worksheets into a single script.", UserWarning)

        cells = list(chain.from_iterable(ws["cells"] for ws in worksheets))

    else:
        cells = notebook["cells"]

    result = ["# Jupyter notebook converted to Python script."]

    for cell in cells:
        cell_str = _process_cell(cell, include_output=include_output)
        if cell_str:
            result.append(cell_str)

    return "\n\n".join(result) + "\n"


def _process_cell(cell: Dict[str, Any], include_output: bool) -> Optional[str]:
    """
    Process a Jupyter notebook cell and return the cell content as a string.

    Parameters
    ----------
    cell : Dict[str, Any]
        The cell dictionary from a Jupyter notebook.
    include_output : bool
        Whether to include cell outputs in the generated script

    Returns
    -------
    str, optional
        The cell content as a string, or None if the cell is empty.

    Raises
    ------
    ValueError
        If an unexpected cell type is encountered.
    """
    cell_type = cell["cell_type"]

    # Validate cell type and handle unexpected types
    if cell_type not in ("markdown", "code", "raw"):
        raise ValueError(f"Unknown cell type: {cell_type}")

    cell_str = "".join(cell["source"])

    # Skip empty cells
    if not cell_str:
        return None

    # Convert Markdown and raw cells to multi-line comments
    if cell_type in ("markdown", "raw"):
        return f'"""\n{cell_str}\n"""'

    # Add cell output as comments
    outputs = cell.get("outputs")
    if include_output and outputs:

        # Include cell outputs as comments
        output_lines = []

        for output in outputs:
            output_lines += _extract_output(output)

        for output_line in output_lines:
            if not output_line.endswith("\n"):
                output_line += "\n"

        cell_str += "\n# Output:\n#   " + "\n#   ".join(output_lines)

    return cell_str


def _extract_output(output: Dict[str, Any]) -> List[str]:
    """
    Extract the output from a Jupyter notebook cell.

    Parameters
    ----------
    output : Dict[str, Any]
        The output dictionary from a Jupyter notebook cell.

    Returns
    -------
    List[str]
        The output as a list of strings.

    Raises
    ------
    ValueError
        If an unknown output type is encountered.
    """
    output_type = output["output_type"]

    if output_type == "stream":
        return output["text"]

    if output_type in ("execute_result", "display_data"):
        return output["data"]["text/plain"]

    if output_type == "error":
        return [f"Error: {output['ename']}: {output['evalue']}"]

    raise ValueError(f"Unknown output type: {output_type}")



================================================
FILE: src/gitingest/utils/os_utils.py
================================================
"""Utility functions for working with the operating system."""

import os
from pathlib import Path


async def ensure_directory(path: Path) -> None:
    """
    Ensure the directory exists, creating it if necessary.

    Parameters
    ----------
    path : Path
        The path to ensure exists

    Raises
    ------
    OSError
        If the directory cannot be created
    """
    try:
        os.makedirs(path, exist_ok=True)
    except OSError as exc:
        raise OSError(f"Failed to create directory {path}: {exc}") from exc



================================================
FILE: src/gitingest/utils/path_utils.py
================================================
"""Utility functions for working with file paths."""

import os
import platform
from pathlib import Path


def _is_safe_symlink(symlink_path: Path, base_path: Path) -> bool:
    """
    Check if a symlink points to a location within the base directory.

    This function resolves the target of a symlink and ensures it is within the specified
    base directory, returning `True` if it is safe, or `False` if the symlink points outside
    the base directory.

    Parameters
    ----------
    symlink_path : Path
        The path of the symlink to check.
    base_path : Path
        The base directory to ensure the symlink points within.

    Returns
    -------
    bool
        `True` if the symlink points within the base directory, `False` otherwise.
    """
    try:
        if platform.system() == "Windows":
            if not os.path.islink(str(symlink_path)):
                return False

        target_path = symlink_path.resolve()
        base_resolved = base_path.resolve()

        return base_resolved in target_path.parents or target_path == base_resolved
    except (OSError, ValueError):
        # If there's any error resolving the paths, consider it unsafe
        return False



================================================
FILE: src/gitingest/utils/query_parser_utils.py
================================================
"""Utility functions for parsing and validating query parameters."""

import os
import string
from typing import List, Set, Tuple

HEX_DIGITS: Set[str] = set(string.hexdigits)


KNOWN_GIT_HOSTS: List[str] = [
    "github.com",
    "gitlab.com",
    "bitbucket.org",
    "gitea.com",
    "codeberg.org",
    "gist.github.com",
]


def _is_valid_git_commit_hash(commit: str) -> bool:
    """
    Validate if the provided string is a valid Git commit hash.

    This function checks if the commit hash is a 40-character string consisting only
    of hexadecimal digits, which is the standard format for Git commit hashes.

    Parameters
    ----------
    commit : str
        The string to validate as a Git commit hash.

    Returns
    -------
    bool
        True if the string is a valid 40-character Git commit hash, otherwise False.
    """
    return len(commit) == 40 and all(c in HEX_DIGITS for c in commit)


def _is_valid_pattern(pattern: str) -> bool:
    """
    Validate if the given pattern contains only valid characters.

    This function checks if the pattern contains only alphanumeric characters or one
    of the following allowed characters: dash (`-`), underscore (`_`), dot (`.`),
    forward slash (`/`), plus (`+`), asterisk (`*`), or the at sign (`@`).

    Parameters
    ----------
    pattern : str
        The pattern to validate.

    Returns
    -------
    bool
        True if the pattern is valid, otherwise False.
    """
    return all(c.isalnum() or c in "-_./+*@" for c in pattern)


def _validate_host(host: str) -> None:
    """
    Validate the given host against the known Git hosts.

    Parameters
    ----------
    host : str
        The host to validate.

    Raises
    ------
    ValueError
        If the host is not a known Git host.
    """
    if host not in KNOWN_GIT_HOSTS:
        raise ValueError(f"Unknown domain '{host}' in URL")


def _validate_url_scheme(scheme: str) -> None:
    """
    Validate the given scheme against the known schemes.

    Parameters
    ----------
    scheme : str
        The scheme to validate.

    Raises
    ------
    ValueError
        If the scheme is not 'http' or 'https'.
    """
    if scheme not in ("https", "http"):
        raise ValueError(f"Invalid URL scheme '{scheme}' in URL")


def _get_user_and_repo_from_path(path: str) -> Tuple[str, str]:
    """
    Extract the user and repository names from a given path.

    Parameters
    ----------
    path : str
        The path to extract the user and repository names from.

    Returns
    -------
    Tuple[str, str]
        A tuple containing the user and repository names.

    Raises
    ------
    ValueError
        If the path does not contain at least two parts.
    """
    path_parts = path.lower().strip("/").split("/")
    if len(path_parts) < 2:
        raise ValueError(f"Invalid repository URL '{path}'")
    return path_parts[0], path_parts[1]


def _normalize_pattern(pattern: str) -> str:
    """
    Normalize the given pattern by removing leading separators and appending a wildcard.

    This function processes the pattern string by stripping leading directory separators
    and appending a wildcard (`*`) if the pattern ends with a separator.

    Parameters
    ----------
    pattern : str
        The pattern to normalize.

    Returns
    -------
    str
        The normalized pattern.
    """
    pattern = pattern.lstrip(os.sep)
    if pattern.endswith(os.sep):
        pattern += "*"
    return pattern



================================================
FILE: src/gitingest/utils/timeout_wrapper.py
================================================
"""Utility functions for the Gitingest package."""

import asyncio
import functools
from typing import Any, Awaitable, Callable, TypeVar

from gitingest.utils.exceptions import AsyncTimeoutError

T = TypeVar("T")


def async_timeout(seconds) -> Callable[[Callable[..., Awaitable[T]]], Callable[..., Awaitable[T]]]:
    """
    Async Timeout decorator.

    This decorator wraps an asynchronous function and ensures it does not run for
    longer than the specified number of seconds. If the function execution exceeds
    this limit, it raises an `AsyncTimeoutError`.

    Parameters
    ----------
    seconds : int
        The maximum allowed time (in seconds) for the asynchronous function to complete.

    Returns
    -------
    Callable[[Callable[..., Awaitable[T]]], Callable[..., Awaitable[T]]]
        A decorator that, when applied to an async function, ensures the function
        completes within the specified time limit. If the function takes too long,
        an `AsyncTimeoutError` is raised.
    """

    def decorator(func: Callable[..., Awaitable[T]]) -> Callable[..., Awaitable[T]]:
        @functools.wraps(func)
        async def wrapper(*args: Any, **kwargs: Any) -> T:
            try:
                return await asyncio.wait_for(func(*args, **kwargs), timeout=seconds)
            except asyncio.TimeoutError as exc:
                raise AsyncTimeoutError(f"Operation timed out after {seconds} seconds") from exc

        return wrapper

    return decorator



================================================
FILE: src/server/__init__.py
================================================



================================================
FILE: src/server/main.py
================================================
"""Main module for the FastAPI application."""

import os
from pathlib import Path
from typing import Dict

from dotenv import load_dotenv
from fastapi import FastAPI, Request
from fastapi.responses import FileResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from slowapi.errors import RateLimitExceeded
from starlette.middleware.trustedhost import TrustedHostMiddleware

from server.routers import download, dynamic, index
from server.server_config import templates
from server.server_utils import lifespan, limiter, rate_limit_exception_handler

# Load environment variables from .env file
load_dotenv()

# Initialize the FastAPI application with lifespan
app = FastAPI(lifespan=lifespan)
app.state.limiter = limiter

# Register the custom exception handler for rate limits
app.add_exception_handler(RateLimitExceeded, rate_limit_exception_handler)


# Mount static files dynamically to serve CSS, JS, and other static assets
static_dir = Path(__file__).parent.parent / "static"
app.mount("/static", StaticFiles(directory=static_dir), name="static")


# Fetch allowed hosts from the environment or use the default values
allowed_hosts = os.getenv("ALLOWED_HOSTS")
if allowed_hosts:
    allowed_hosts = allowed_hosts.split(",")
else:
    # Define the default allowed hosts for the application
    default_allowed_hosts = ["gitingest.com", "*.gitingest.com", "localhost", "127.0.0.1"]
    allowed_hosts = default_allowed_hosts

# Add middleware to enforce allowed hosts
app.add_middleware(TrustedHostMiddleware, allowed_hosts=allowed_hosts)


@app.get("/health")
async def health_check() -> Dict[str, str]:
    """
    Health check endpoint to verify that the server is running.

    Returns
    -------
    Dict[str, str]
        A JSON object with a "status" key indicating the server's health status.
    """
    return {"status": "healthy"}


@app.head("/")
async def head_root() -> HTMLResponse:
    """
    Respond to HTTP HEAD requests for the root URL.

    Mirrors the headers and status code of the index page.

    Returns
    -------
    HTMLResponse
        An empty HTML response with appropriate headers.
    """
    return HTMLResponse(content=None, headers={"content-type": "text/html; charset=utf-8"})


@app.get("/api/", response_class=HTMLResponse)
@app.get("/api", response_class=HTMLResponse)
async def api_docs(request: Request) -> HTMLResponse:
    """
    Render the API documentation page.

    Parameters
    ----------
    request : Request
        The incoming HTTP request.

    Returns
    -------
    HTMLResponse
        A rendered HTML page displaying API documentation.
    """
    return templates.TemplateResponse("api.jinja", {"request": request})


@app.get("/robots.txt")
async def robots() -> FileResponse:
    """
    Serve the `robots.txt` file to guide search engine crawlers.

    Returns
    -------
    FileResponse
        The `robots.txt` file located in the static directory.
    """
    return FileResponse("static/robots.txt")


# Include routers for modular endpoints
app.include_router(index)
app.include_router(download)
app.include_router(dynamic)



================================================
FILE: src/server/query_processor.py
================================================
"""Process a query by parsing input, cloning a repository, and generating a summary."""

from functools import partial

from fastapi import Request
from starlette.templating import _TemplateResponse

from gitingest.cloning import clone_repo
from gitingest.ingestion import ingest_query
from gitingest.query_parsing import IngestionQuery, parse_query
from server.server_config import EXAMPLE_REPOS, MAX_DISPLAY_SIZE, templates
from server.server_utils import Colors, log_slider_to_size


async def process_query(
    request: Request,
    input_text: str,
    slider_position: int,
    pattern_type: str = "exclude",
    pattern: str = "",
    is_index: bool = False,
) -> _TemplateResponse:
    """
    Process a query by parsing input, cloning a repository, and generating a summary.

    Handle user input, process Git repository data, and prepare
    a response for rendering a template with the processed results or an error message.

    Parameters
    ----------
    request : Request
        The HTTP request object.
    input_text : str
        Input text provided by the user, typically a Git repository URL or slug.
    slider_position : int
        Position of the slider, representing the maximum file size in the query.
    pattern_type : str
        Type of pattern to use, either "include" or "exclude" (default is "exclude").
    pattern : str
        Pattern to include or exclude in the query, depending on the pattern type.
    is_index : bool
        Flag indicating whether the request is for the index page (default is False).

    Returns
    -------
    _TemplateResponse
        Rendered template response containing the processed results or an error message.

    Raises
    ------
    ValueError
        If an invalid pattern type is provided.
    """
    if pattern_type == "include":
        include_patterns = pattern
        exclude_patterns = None
    elif pattern_type == "exclude":
        exclude_patterns = pattern
        include_patterns = None
    else:
        raise ValueError(f"Invalid pattern type: {pattern_type}")

    template = "index.jinja" if is_index else "git.jinja"
    template_response = partial(templates.TemplateResponse, name=template)
    max_file_size = log_slider_to_size(slider_position)

    context = {
        "request": request,
        "repo_url": input_text,
        "examples": EXAMPLE_REPOS if is_index else [],
        "default_file_size": slider_position,
        "pattern_type": pattern_type,
        "pattern": pattern,
    }

    try:
        query: IngestionQuery = await parse_query(
            source=input_text,
            max_file_size=max_file_size,
            from_web=True,
            include_patterns=include_patterns,
            ignore_patterns=exclude_patterns,
        )
        if not query.url:
            raise ValueError("The 'url' parameter is required.")

        clone_config = query.extract_clone_config()
        await clone_repo(clone_config)
        summary, tree, content = ingest_query(query)
        with open(f"{clone_config.local_path}.txt", "w", encoding="utf-8") as f:
            f.write(tree + "\n" + content)
    except Exception as exc:
        # hack to print error message when query is not defined
        if "query" in locals() and query is not None and isinstance(query, dict):
            _print_error(query["url"], exc, max_file_size, pattern_type, pattern)
        else:
            print(f"{Colors.BROWN}WARN{Colors.END}: {Colors.RED}<-  {Colors.END}", end="")
            print(f"{Colors.RED}{exc}{Colors.END}")

        context["error_message"] = f"Error: {exc}"
        if "405" in str(exc):
            context["error_message"] = (
                "Repository not found. Please make sure it is public (private repositories will be supported soon)"
            )
        return template_response(context=context)

    if len(content) > MAX_DISPLAY_SIZE:
        content = (
            f"(Files content cropped to {int(MAX_DISPLAY_SIZE / 1_000)}k characters, "
            "download full ingest to see more)\n" + content[:MAX_DISPLAY_SIZE]
        )

    _print_success(
        url=query.url,
        max_file_size=max_file_size,
        pattern_type=pattern_type,
        pattern=pattern,
        summary=summary,
    )

    context.update(
        {
            "result": True,
            "summary": summary,
            "tree": tree,
            "content": content,
            "ingest_id": query.id,
        }
    )

    return template_response(context=context)


def _print_query(url: str, max_file_size: int, pattern_type: str, pattern: str) -> None:
    """
    Print a formatted summary of the query details, including the URL, file size,
    and pattern information, for easier debugging or logging.

    Parameters
    ----------
    url : str
        The URL associated with the query.
    max_file_size : int
        The maximum file size allowed for the query, in bytes.
    pattern_type : str
        Specifies the type of pattern to use, either "include" or "exclude".
    pattern : str
        The actual pattern string to include or exclude in the query.
    """
    print(f"{Colors.WHITE}{url:<20}{Colors.END}", end="")
    if int(max_file_size / 1024) != 50:
        print(f" | {Colors.YELLOW}Size: {int(max_file_size/1024)}kb{Colors.END}", end="")
    if pattern_type == "include" and pattern != "":
        print(f" | {Colors.YELLOW}Include {pattern}{Colors.END}", end="")
    elif pattern_type == "exclude" and pattern != "":
        print(f" | {Colors.YELLOW}Exclude {pattern}{Colors.END}", end="")


def _print_error(url: str, e: Exception, max_file_size: int, pattern_type: str, pattern: str) -> None:
    """
    Print a formatted error message including the URL, file size, pattern details, and the exception encountered,
    for debugging or logging purposes.

    Parameters
    ----------
    url : str
        The URL associated with the query that caused the error.
    e : Exception
        The exception raised during the query or process.
    max_file_size : int
        The maximum file size allowed for the query, in bytes.
    pattern_type : str
        Specifies the type of pattern to use, either "include" or "exclude".
    pattern : str
        The actual pattern string to include or exclude in the query.
    """
    print(f"{Colors.BROWN}WARN{Colors.END}: {Colors.RED}<-  {Colors.END}", end="")
    _print_query(url, max_file_size, pattern_type, pattern)
    print(f" | {Colors.RED}{e}{Colors.END}")


def _print_success(url: str, max_file_size: int, pattern_type: str, pattern: str, summary: str) -> None:
    """
    Print a formatted success message, including the URL, file size, pattern details, and a summary with estimated
    tokens, for debugging or logging purposes.

    Parameters
    ----------
    url : str
        The URL associated with the successful query.
    max_file_size : int
        The maximum file size allowed for the query, in bytes.
    pattern_type : str
        Specifies the type of pattern to use, either "include" or "exclude".
    pattern : str
        The actual pattern string to include or exclude in the query.
    summary : str
        A summary of the query result, including details like estimated tokens.
    """
    estimated_tokens = summary[summary.index("Estimated tokens:") + len("Estimated ") :]
    print(f"{Colors.GREEN}INFO{Colors.END}: {Colors.GREEN}<-  {Colors.END}", end="")
    _print_query(url, max_file_size, pattern_type, pattern)
    print(f" | {Colors.PURPLE}{estimated_tokens}{Colors.END}")



================================================
FILE: src/server/server_config.py
================================================
"""Configuration for the server."""

from typing import Dict, List

from fastapi.templating import Jinja2Templates

MAX_DISPLAY_SIZE: int = 300_000
DELETE_REPO_AFTER: int = 60 * 60  # In seconds


EXAMPLE_REPOS: List[Dict[str, str]] = [
    {"name": "Gitingest", "url": "https://github.com/cyclotruc/gitingest"},
    {"name": "FastAPI", "url": "https://github.com/tiangolo/fastapi"},
    {"name": "Flask", "url": "https://github.com/pallets/flask"},
    {"name": "Excalidraw", "url": "https://github.com/excalidraw/excalidraw"},
    {"name": "ApiAnalytics", "url": "https://github.com/tom-draper/api-analytics"},
]

templates = Jinja2Templates(directory="server/templates")



================================================
FILE: src/server/server_utils.py
================================================
"""Utility functions for the server."""

import asyncio
import math
import shutil
import time
from contextlib import asynccontextmanager
from pathlib import Path

from fastapi import FastAPI, Request
from fastapi.responses import Response
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.errors import RateLimitExceeded
from slowapi.util import get_remote_address

from gitingest.config import TMP_BASE_PATH
from server.server_config import DELETE_REPO_AFTER

# Initialize a rate limiter
limiter = Limiter(key_func=get_remote_address)


async def rate_limit_exception_handler(request: Request, exc: Exception) -> Response:
    """
    Custom exception handler for rate-limiting errors.

    Parameters
    ----------
    request : Request
        The incoming HTTP request.
    exc : Exception
        The exception raised, expected to be RateLimitExceeded.

    Returns
    -------
    Response
        A response indicating that the rate limit has been exceeded.

    Raises
    ------
    exc
        If the exception is not a RateLimitExceeded error, it is re-raised.
    """
    if isinstance(exc, RateLimitExceeded):
        # Delegate to the default rate limit handler
        return _rate_limit_exceeded_handler(request, exc)
    # Re-raise other exceptions
    raise exc


@asynccontextmanager
async def lifespan(_: FastAPI):
    """
    Lifecycle manager for handling startup and shutdown events for the FastAPI application.

    Parameters
    ----------
    _ : FastAPI
        The FastAPI application instance (unused).

    Yields
    -------
    None
        Yields control back to the FastAPI application while the background task runs.
    """
    task = asyncio.create_task(_remove_old_repositories())

    yield
    # Cancel the background task on shutdown
    task.cancel()
    try:
        await task
    except asyncio.CancelledError:
        pass


async def _remove_old_repositories():
    """
    Periodically remove old repository folders.

    Background task that runs periodically to clean up old repository directories.

    This task:
    - Scans the TMP_BASE_PATH directory every 60 seconds
    - Removes directories older than DELETE_REPO_AFTER seconds
    - Before deletion, logs repository URLs to history.txt if a matching .txt file exists
    - Handles errors gracefully if deletion fails

    The repository URL is extracted from the first .txt file in each directory,
    assuming the filename format: "owner-repository.txt"
    """
    while True:
        try:
            if not TMP_BASE_PATH.exists():
                await asyncio.sleep(60)
                continue

            current_time = time.time()

            for folder in TMP_BASE_PATH.iterdir():
                # Skip if folder is not old enough
                if current_time - folder.stat().st_ctime <= DELETE_REPO_AFTER:
                    continue

                await _process_folder(folder)

        except Exception as exc:
            print(f"Error in _remove_old_repositories: {exc}")

        await asyncio.sleep(60)


async def _process_folder(folder: Path) -> None:
    """
    Process a single folder for deletion and logging.

    Parameters
    ----------
    folder : Path
        The path to the folder to be processed.
    """
    # Try to log repository URL before deletion
    try:
        txt_files = [f for f in folder.iterdir() if f.suffix == ".txt"]

        # Extract owner and repository name from the filename
        filename = txt_files[0].stem
        if txt_files and "-" in filename:
            owner, repo = filename.split("-", 1)
            repo_url = f"{owner}/{repo}"

            with open("history.txt", mode="a", encoding="utf-8") as history:
                history.write(f"{repo_url}\n")

    except Exception as exc:
        print(f"Error logging repository URL for {folder}: {exc}")

    # Delete the folder
    try:
        shutil.rmtree(folder)
    except Exception as exc:
        print(f"Error deleting {folder}: {exc}")


def log_slider_to_size(position: int) -> int:
    """
    Convert a slider position to a file size in bytes using a logarithmic scale.

    Parameters
    ----------
    position : int
        Slider position ranging from 0 to 500.

    Returns
    -------
    int
        File size in bytes corresponding to the slider position.
    """
    maxp = 500
    minv = math.log(1)
    maxv = math.log(102_400)
    return round(math.exp(minv + (maxv - minv) * pow(position / maxp, 1.5))) * 1024


## Color printing utility
class Colors:
    """ANSI color codes"""

    BLACK = "\033[0;30m"
    RED = "\033[0;31m"
    GREEN = "\033[0;32m"
    BROWN = "\033[0;33m"
    BLUE = "\033[0;34m"
    PURPLE = "\033[0;35m"
    CYAN = "\033[0;36m"
    LIGHT_GRAY = "\033[0;37m"
    DARK_GRAY = "\033[1;30m"
    LIGHT_RED = "\033[1;31m"
    LIGHT_GREEN = "\033[1;32m"
    YELLOW = "\033[1;33m"
    LIGHT_BLUE = "\033[1;34m"
    LIGHT_PURPLE = "\033[1;35m"
    LIGHT_CYAN = "\033[1;36m"
    WHITE = "\033[1;37m"
    BOLD = "\033[1m"
    FAINT = "\033[2m"
    ITALIC = "\033[3m"
    UNDERLINE = "\033[4m"
    BLINK = "\033[5m"
    NEGATIVE = "\033[7m"
    CROSSED = "\033[9m"
    END = "\033[0m"



================================================
FILE: src/server/routers/__init__.py
================================================
"""This module contains the routers for the FastAPI application."""

from server.routers.download import router as download
from server.routers.dynamic import router as dynamic
from server.routers.index import router as index

__all__ = ["download", "dynamic", "index"]



================================================
FILE: src/server/routers/download.py
================================================
"""This module contains the FastAPI router for downloading a digest file."""

from fastapi import APIRouter, HTTPException
from fastapi.responses import Response

from gitingest.config import TMP_BASE_PATH

router = APIRouter()


@router.get("/download/{digest_id}")
async def download_ingest(digest_id: str) -> Response:
    """
    Download a .txt file associated with a given digest ID.

    This function searches for a `.txt` file in a directory corresponding to the provided
    digest ID. If a file is found, it is read and returned as a downloadable attachment.
    If no `.txt` file is found, an error is raised.

    Parameters
    ----------
    digest_id : str
        The unique identifier for the digest. It is used to find the corresponding directory
        and locate the .txt file within that directory.

    Returns
    -------
    Response
        A FastAPI Response object containing the content of the found `.txt` file. The file is
        sent with the appropriate media type (`text/plain`) and the correct `Content-Disposition`
        header to prompt a file download.

    Raises
    ------
    HTTPException
        If the digest directory is not found or if no `.txt` file exists in the directory.
    """
    directory = TMP_BASE_PATH / digest_id

    try:
        if not directory.exists():
            raise FileNotFoundError("Directory not found")

        txt_files = [f for f in directory.iterdir() if f.suffix == ".txt"]
        if not txt_files:
            raise FileNotFoundError("No .txt file found")

    except FileNotFoundError as exc:
        raise HTTPException(status_code=404, detail="Digest not found") from exc

    # Find the first .txt file in the directory
    first_file = txt_files[0]

    with first_file.open(encoding="utf-8") as f:
        content = f.read()

    return Response(
        content=content,
        media_type="text/plain",
        headers={"Content-Disposition": f"attachment; filename={first_file.name}"},
    )



================================================
FILE: src/server/routers/dynamic.py
================================================
"""This module defines the dynamic router for handling dynamic path requests."""

from fastapi import APIRouter, Form, Request
from fastapi.responses import HTMLResponse

from server.query_processor import process_query
from server.server_config import templates
from server.server_utils import limiter

router = APIRouter()


@router.get("/{full_path:path}")
async def catch_all(request: Request, full_path: str) -> HTMLResponse:
    """
    Render a page with a Git URL based on the provided path.

    This endpoint catches all GET requests with a dynamic path, constructs a Git URL
    using the `full_path` parameter, and renders the `git.jinja` template with that URL.

    Parameters
    ----------
    request : Request
        The incoming request object, which provides context for rendering the response.
    full_path : str
        The full path extracted from the URL, which is used to build the Git URL.

    Returns
    -------
    HTMLResponse
        An HTML response containing the rendered template, with the Git URL
        and other default parameters such as loading state and file size.
    """
    return templates.TemplateResponse(
        "git.jinja",
        {
            "request": request,
            "repo_url": full_path,
            "loading": True,
            "default_file_size": 243,
        },
    )


@router.post("/{full_path:path}", response_class=HTMLResponse)
@limiter.limit("10/minute")
async def process_catch_all(
    request: Request,
    input_text: str = Form(...),
    max_file_size: int = Form(...),
    pattern_type: str = Form(...),
    pattern: str = Form(...),
) -> HTMLResponse:
    """
    Process the form submission with user input for query parameters.

    This endpoint handles POST requests, processes the input parameters (e.g., text, file size, pattern),
    and calls the `process_query` function to handle the query logic, returning the result as an HTML response.

    Parameters
    ----------
    request : Request
        The incoming request object, which provides context for rendering the response.
    input_text : str
        The input text provided by the user for processing, by default taken from the form.
    max_file_size : int
        The maximum allowed file size for the input, specified by the user.
    pattern_type : str
        The type of pattern used for the query, specified by the user.
    pattern : str
        The pattern string used in the query, specified by the user.

    Returns
    -------
    HTMLResponse
        An HTML response generated after processing the form input and query logic,
        which will be rendered and returned to the user.
    """
    return await process_query(
        request,
        input_text,
        max_file_size,
        pattern_type,
        pattern,
        is_index=False,
    )



================================================
FILE: src/server/routers/index.py
================================================
"""This module defines the FastAPI router for the home page of the application."""

from fastapi import APIRouter, Form, Request
from fastapi.responses import HTMLResponse

from server.query_processor import process_query
from server.server_config import EXAMPLE_REPOS, templates
from server.server_utils import limiter

router = APIRouter()


@router.get("/", response_class=HTMLResponse)
async def home(request: Request) -> HTMLResponse:
    """
    Render the home page with example repositories and default parameters.

    This endpoint serves the home page of the application, rendering the `index.jinja` template
    and providing it with a list of example repositories and default file size values.

    Parameters
    ----------
    request : Request
        The incoming request object, which provides context for rendering the response.

    Returns
    -------
    HTMLResponse
        An HTML response containing the rendered home page template, with example repositories
        and other default parameters such as file size.
    """
    return templates.TemplateResponse(
        "index.jinja",
        {
            "request": request,
            "examples": EXAMPLE_REPOS,
            "default_file_size": 243,
        },
    )


@router.post("/", response_class=HTMLResponse)
@limiter.limit("10/minute")
async def index_post(
    request: Request,
    input_text: str = Form(...),
    max_file_size: int = Form(...),
    pattern_type: str = Form(...),
    pattern: str = Form(...),
) -> HTMLResponse:
    """
    Process the form submission with user input for query parameters.

    This endpoint handles POST requests from the home page form. It processes the user-submitted
    input (e.g., text, file size, pattern type) and invokes the `process_query` function to handle
    the query logic, returning the result as an HTML response.

    Parameters
    ----------
    request : Request
        The incoming request object, which provides context for rendering the response.
    input_text : str
        The input text provided by the user for processing, by default taken from the form.
    max_file_size : int
        The maximum allowed file size for the input, specified by the user.
    pattern_type : str
        The type of pattern used for the query, specified by the user.
    pattern : str
        The pattern string used in the query, specified by the user.

    Returns
    -------
    HTMLResponse
        An HTML response containing the results of processing the form input and query logic,
        which will be rendered and returned to the user.
    """
    return await process_query(
        request,
        input_text,
        max_file_size,
        pattern_type,
        pattern,
        is_index=True,
    )



================================================
FILE: src/server/templates/api.jinja
================================================
{% extends "base.jinja" %}
{% block title %}Gitingest API{% endblock %}
{% block content %}
    <div class="relative">
        <div class="w-full h-full absolute inset-0 bg-black rounded-xl translate-y-2 translate-x-2"></div>
        <div class="bg-[#fff4da] rounded-xl border-[3px] border-gray-900 p-8 relative z-20">
            <h1 class="text-3xl font-bold text-gray-900 mb-4">API Documentation</h1>
            <div class="prose prose-blue max-w-none">
                <div class="bg-yellow-50 border-[3px] border-gray-900 p-4 mb-6 rounded-lg">
                    <div class="flex">
                        <div class="flex-shrink-0">
                            <svg class="h-5 w-5 text-yellow-400"
                                 viewBox="0 0 20 20"
                                 fill="currentColor">
                                <path fill-rule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clip-rule="evenodd" />
                            </svg>
                        </div>
                        <div class="ml-3">
                            <p class="text-sm text-gray-900">The API is currently under development..</p>
                        </div>
                    </div>
                </div>
                <p class="text-gray-900">
                    We're working on making our API available to the public.
                    In the meantime, you can
                    <a href="https://github.com/cyclotruc/gitingest/issues/new"
                       target="_blank"
                       rel="noopener noreferrer"
                       class="text-[#6e5000] hover:underline">Open an issue on GitHub</a>
                    to suggest features.
                </p>
            </div>
        </div>
    </div>
{% endblock %}



================================================
FILE: src/server/templates/base.jinja
================================================
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="icon" type="image/x-icon" href="/static/favicon.ico">
        <!-- Search Engine Meta Tags -->
        <meta name="description"
              content="Replace 'hub' with 'ingest' in any GitHub URL for a prompt-friendly text.">
        <meta name="keywords"
              content="Gitingest, AI tools, LLM integration, Ingest, Digest, Context, Prompt, Git workflow, codebase extraction, Git repository, Git automation, Summarize, prompt-friendly">
        <meta name="robots" content="index, follow">
        <!-- Favicons -->
        <link rel="icon" type="image/svg+xml" href="/static/favicon.svg">
        <link rel="icon"
              type="image/png"
              sizes="64x64"
              href="/static/favicon-64.png">
        <link rel="apple-touch-icon"
              sizes="180x180"
              href="/static/apple-touch-icon.png">
        <!-- Web App Meta -->
        <meta name="apple-mobile-web-app-title" content="Gitingest">
        <meta name="application-name" content="Gitingest">
        <meta name="theme-color" content="#FCA847">
        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="default">
        <!-- OpenGraph Meta Tags -->
        <meta property="og:title" content="Gitingest">
        <meta property="og:description"
              content="Replace 'hub' with 'ingest' in any GitHub URL for a prompt-friendly text.">
        <meta property="og:type" content="website">
        <meta property="og:url" content="{{ request.url }}">
        <meta property="og:image" content="/static/og-image.png">
        <title>
            {% block title %}Gitingest{% endblock %}
        </title>
        <script src="https://cdn.tailwindcss.com"></script>
        <script src="/static/js/utils.js"></script>
        <script>
        !function (t, e) { var o, n, p, r; e.__SV || (window.posthog = e, e._i = [], e.init = function (i, s, a) { function g(t, e) { var o = e.split("."); 2 == o.length && (t = t[o[0]], e = o[1]), t[e] = function () { t.push([e].concat(Array.prototype.slice.call(arguments, 0))) } } (p = t.createElement("script")).type = "text/javascript", p.crossOrigin = "anonymous", p.async = !0, p.src = s.api_host.replace(".i.posthog.com", "-assets.i.posthog.com") + "/static/array.js", (r = t.getElementsByTagName("script")[0]).parentNode.insertBefore(p, r); var u = e; for (void 0 !== a ? u = e[a] = [] : a = "posthog", u.people = u.people || [], u.toString = function (t) { var e = "posthog"; return "posthog" !== a && (e += "." + a), t || (e += " (stub)"), e }, u.people.toString = function () { return u.toString(1) + ".people (stub)" }, o = "init capture register register_once register_for_session unregister unregister_for_session getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSessionId getSurveys getActiveMatchingSurveys renderSurvey canRenderSurvey getNextSurveyStep identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty createPersonProfile opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing clear_opt_in_out_capturing debug getPageViewId".split(" "), n = 0; n < o.length; n++)g(u, o[n]); e._i.push([i, s, a]) }, e.__SV = 1) }(document, window.posthog || []);
        posthog.init('phc_9aNpiIVH2zfTWeY84vdTWxvrJRCQQhP5kcVDXUvcdou', {
            api_host: 'https://eu.i.posthog.com',
            person_profiles: 'always',
        })
        </script>
        {% block extra_head %}{% endblock %}
    </head>
    <body class="bg-[#FFFDF8] min-h-screen flex flex-col">
        {% include 'components/navbar.jinja' %}
        <!-- Main content wrapper -->
        <main class="flex-1 w-full">
            <div class="max-w-4xl mx-auto px-4 py-8">
                {% block content %}{% endblock %}
            </div>
        </main>
        {% include 'components/footer.jinja' %}
        {% block extra_scripts %}{% endblock %}
    </body>
</html>



================================================
FILE: src/server/templates/git.jinja
================================================
{% extends "base.jinja" %}
{% block content %}
    {% if error_message %}
        <div class="mb-6 p-4 bg-red-50 border border-red-200 rounded-lg text-red-700"
             id="error-message"
             data-message="{{ error_message }}">{{ error_message }}</div>
    {% endif %}
    {% with is_index=true, show_examples=false %}
        {% include 'components/git_form.jinja' %}
    {% endwith %}
    {% if loading %}
        <div class="relative mt-10">
            <div class="w-full h-full absolute inset-0 bg-black rounded-xl translate-y-2 translate-x-2"></div>
            <div class="bg-[#fafafa] rounded-xl border-[3px] border-gray-900 p-6 relative z-20 flex flex-col items-center space-y-4">
                <div class="loader border-8 border-[#fff4da] border-t-8 border-t-[#ffc480] rounded-full w-16 h-16 animate-spin"></div>
                <p class="text-lg font-bold text-gray-900">Loading...</p>
            </div>
        </div>
    {% endif %}
    {% include 'components/result.jinja' %}
{% endblock content %}
{% block extra_scripts %}
    <script>
    document.addEventListener('DOMContentLoaded', function() {
        const urlInput = document.getElementById('input_text');
        const form = document.getElementById('ingestForm');
        if (urlInput && urlInput.value.trim() && form) {
            // Wait for stars to be loaded before submitting
            waitForStars().then(() => {
                const submitEvent = new SubmitEvent('submit', {
                    cancelable: true,
                    bubbles: true
                });
                Object.defineProperty(submitEvent, 'target', {
                    value: form,
                    enumerable: true
                });
                handleSubmit(submitEvent, false);
            });
        }
    });

    function waitForStars() {
        return new Promise((resolve) => {
            const checkStars = () => {
                const stars = document.getElementById('github-stars');
                if (stars && stars.textContent !== '0') {
                    resolve();
                } else {
                    setTimeout(checkStars, 10);
                }
            };
            checkStars();
        });
    }
    </script>
{% endblock extra_scripts %}



================================================
FILE: src/server/templates/index.jinja
================================================
{% extends "base.jinja" %}
{% block extra_head %}
    <script>
    function submitExample(repoName) {
        const input = document.getElementById('input_text');
        input.value = repoName;
        input.focus();
    }
    </script>
{% endblock %}
{% block content %}
    <div class="mb-8">
        <div class="relative w-full mx-auto flex sm:flex-row flex-col justify-center items-start sm:items-center">
            <svg class="h-auto w-16 sm:w-20 md:w-24 flex-shrink-0 p-2 md:relative sm:absolute lg:absolute left-0 lg:-translate-x-full lg:ml-32 md:translate-x-10 sm:-translate-y-16 md:-translate-y-0 -translate-x-2 lg:-translate-y-10"
                 viewBox="0 0 91 98"
                 fill="none"
                 xmlns="http://www.w3.org/2000/svg">
                <path d="m35.878 14.162 1.333-5.369 1.933 5.183c4.47 11.982 14.036 21.085 25.828 24.467l5.42 1.555-5.209 2.16c-11.332 4.697-19.806 14.826-22.888 27.237l-1.333 5.369-1.933-5.183C34.56 57.599 24.993 48.496 13.201 45.114l-5.42-1.555 5.21-2.16c11.331-4.697 19.805-14.826 22.887-27.237Z" fill="#FE4A60" stroke="#000" stroke-width="3.445">
                </path>
                <path d="M79.653 5.729c-2.436 5.323-9.515 15.25-18.341 12.374m9.197 16.336c2.6-5.851 10.008-16.834 18.842-13.956m-9.738-15.07c-.374 3.787 1.076 12.078 9.869 14.943M70.61 34.6c.503-4.21-.69-13.346-9.49-16.214M14.922 65.967c1.338 5.677 6.372 16.756 15.808 15.659M18.21 95.832c-1.392-6.226-6.54-18.404-15.984-17.305m12.85-12.892c-.41 3.771-3.576 11.588-12.968 12.681M18.025 96c.367-4.21 3.453-12.905 12.854-14" stroke="#000" stroke-width="2.548" stroke-linecap="round">
                </path>
            </svg>
            <h1 class="text-4xl sm:text-5xl sm:pt-20 lg:pt-5 md:text-6xl lg:text-7xl font-bold tracking-tighter w-full inline-block text-left md:text-center relative">
                Prompt-friendly
                <br>
                codebase&nbsp;
            </h1>
            <svg class="w-16 lg:w-20 h-auto lg:absolute flex-shrink-0 right-0 bottom-0 md:block hidden translate-y-10 md:translate-y-20 lg:translate-y-4 lg:-translate-x-12 -translate-x-10"
                 viewBox="0 0 92 80"
                 fill="none"
                 xmlns="http://www.w3.org/2000/svg">
                <path d="m35.213 16.953.595-5.261 2.644 4.587a35.056 35.056 0 0 0 26.432 17.33l5.261.594-4.587 2.644A35.056 35.056 0 0 0 48.23 63.28l-.595 5.26-2.644-4.587a35.056 35.056 0 0 0-26.432-17.328l-5.261-.595 4.587-2.644a35.056 35.056 0 0 0 17.329-26.433Z" fill="#5CF1A4" stroke="#000" stroke-width="2.868" class="">
                </path>
                <path d="M75.062 40.108c1.07 5.255 1.072 16.52-7.472 19.54m7.422-19.682c1.836 2.965 7.643 8.14 16.187 5.121-8.544 3.02-8.207 15.23-6.971 20.957-1.97-3.343-8.044-9.274-16.588-6.254M12.054 28.012c1.34-5.22 6.126-15.4 14.554-14.369M12.035 28.162c-.274-3.487-2.93-10.719-11.358-11.75C9.104 17.443 14.013 6.262 15.414.542c.226 3.888 2.784 11.92 11.212 12.95" stroke="#000" stroke-width="2.319" stroke-linecap="round">
                </path>
            </svg>
        </div>
        <p class="text-gray-600 text-lg max-w-2xl mx-auto text-center mt-8">
            Turn any Git repository into a simple text digest of its codebase.
        </p>
        <p class="text-gray-600 text-lg max-w-2xl mx-auto text-center mt-0">
            This is useful for feeding a codebase into any LLM.
        </p>
    </div>
    {% if error_message %}
        <div class="mb-6 p-4 bg-red-50 border border-red-200 rounded-lg text-red-700"
             id="error-message"
             data-message="{{ error_message }}">{{ error_message }}</div>
    {% endif %}
    {% with is_index=true, show_examples=true %}
        {% include 'components/git_form.jinja' %}
    {% endwith %}
    <p class="text-gray-600 text-sm max-w-2xl mx-auto text-center mt-4">
        You can also replace 'hub' with 'ingest' in any GitHub URL.
    </p>
    {% include 'components/result.jinja' %}
{% endblock %}



================================================
FILE: src/server/templates/components/footer.jinja
================================================
<footer class="w-full border-t-[3px] border-gray-900 mt-auto">
    <div class="max-w-4xl mx-auto px-4 py-4">
        <div class="grid grid-cols-3 items-center text-gray-900 text-sm">
            <!-- Left column - GitHub links -->
            <div class="flex items-center space-x-4">
                <a href="https://github.com/cyclotruc/gitingest"
                   target="_blank"
                   rel="noopener noreferrer"
                   class="hover:underline flex items-center">
                    <svg class="w-4 h-4 mr-1"
                         xmlns="http://www.w3.org/2000/svg"
                         viewBox="0 0 496 512">
                        <path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z" />
                    </svg>
                    Suggest a feature
                </a>
            </div>
            <!-- Middle column - Made with love -->
            <div class="flex flex-col justify-center items-center">
                <div class="flex items-center">
                    made with ‚ù§Ô∏è by
                    <a href="https://x.com/romdot2"
                       target="_blank"
                       rel="noopener noreferrer"
                       class="ml-1 hover:underline">@rom2</a>
                </div>
                <div class="flex items-center mt-1">
                    Check out my
                    <a href="https://pad.ws"
                       target="_blank"
                       rel="noopener noreferrer"
                       referrerpolicy="origin"
                       class="ml-1 hover:underline">latest project</a>
                </div>
            </div>
            <!-- Right column - Discord -->
            <div class="flex justify-end">
                <a href="https://discord.gg/zerRaGK9EC"
                   target="_blank"
                   rel="noopener noreferrer"
                   class="hover:underline flex items-center">
                    <svg class="w-4 h-4 mr-1"
                         xmlns="http://www.w3.org/2000/svg"
                         viewBox="0 0 640 512">
                        <path fill="currentColor" d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z" />
                    </svg>
                    Discord
                </a>
            </div>
        </div>
    </div>
</footer>



================================================
FILE: src/server/templates/components/git_form.jinja
================================================
<script>
    function changePattern(element) {
        console.log("Pattern changed", element.value);
        let patternType = element.value;
        const files = document.getElementsByName("tree-line");

        Array.from(files).forEach((element) => {
            if (element.textContent.includes("Directory structure:")) {
                return;
            }

            element.classList.toggle('line-through');
            element.classList.toggle('text-gray-500');
            element.classList.toggle('hover:text-inherit');
            element.classList.toggle('hover:no-underline');
            element.classList.toggle('hover:line-through');
            element.classList.toggle('hover:text-gray-500');
        });
    }
</script>
<div class="relative">
    <div class="w-full h-full absolute inset-0 bg-gray-900 rounded-xl translate-y-2 translate-x-2"></div>
    <div class="rounded-xl relative z-20 pl-8 sm:pl-10 pr-8 sm:pr-16 py-8 border-[3px] border-gray-900 bg-[#fff4da]">
        <img src="https://cdn.devdojo.com/images/january2023/shape-1.png"
             class="absolute md:block hidden left-0 h-[4.5rem] w-[4.5rem] bottom-0 -translate-x-full ml-3">
        <form class="flex md:flex-row flex-col w-full h-full justify-center items-stretch space-y-5 md:space-y-0 md:space-x-5"
              id="ingestForm"
              onsubmit="handleSubmit(event{% if is_index %}, true{% endif %})">
            <div class="relative w-full h-full">
                <div class="w-full h-full rounded bg-gray-900 translate-y-1 translate-x-1 absolute inset-0 z-10"></div>
                <input type="text"
                       name="input_text"
                       id="input_text"
                       placeholder="https://github.com/..."
                       value="{{ repo_url if repo_url else '' }}"
                       required
                       class="border-[3px] w-full relative z-20 border-gray-900 placeholder-gray-600 text-lg font-medium focus:outline-none py-3.5 px-6 rounded">
            </div>
            <div class="relative w-auto flex-shrink-0 h-full group">
                <div class="w-full h-full rounded bg-gray-800 translate-y-1 translate-x-1 absolute inset-0 z-10"></div>
                <button type="submit"
                        class="py-3.5 rounded px-6 group-hover:-translate-y-px group-hover:-translate-x-px ease-out duration-300 z-20 relative w-full border-[3px] border-gray-900 font-medium bg-[#ffc480] tracking-wide text-lg flex-shrink-0 text-gray-900">
                    Ingest
                </button>
            </div>
            <input type="hidden" name="pattern_type" value="exclude">
            <input type="hidden" name="pattern" value="">
        </form>
        <div class="mt-4 relative z-20 flex flex-wrap gap-4 items-start">
            <!-- Pattern selector -->
            <div class="w-[200px] sm:w-[250px] mr-9 mt-4">
                <div class="relative">
                    <div class="w-full h-full rounded bg-gray-900 translate-y-1 translate-x-1 absolute inset-0 z-10"></div>
                    <div class="flex relative z-20 border-[3px] border-gray-900 rounded bg-white">
                        <div class="relative flex items-center">
                            <select id="pattern_type"
                                    onchange="changePattern(this)"
                                    name="pattern_type"
                                    class="w-21 py-2 pl-2 pr-6 appearance-none bg-[#e6e8eb] focus:outline-none border-r-[3px] border-gray-900">
                                <option value="exclude"
                                        {% if pattern_type == 'exclude' or not pattern_type %}selected{% endif %}>
                                    Exclude
                                </option>
                                <option value="include" {% if pattern_type == 'include' %}selected{% endif %}>Include</option>
                            </select>
                            <svg class="absolute right-2 w-4 h-4 pointer-events-none"
                                 xmlns="http://www.w3.org/2000/svg"
                                 viewBox="0 0 24 24"
                                 fill="none"
                                 stroke="currentColor"
                                 stroke-width="2"
                                 stroke-linecap="round"
                                 stroke-linejoin="round">
                                <polyline points="6 9 12 15 18 9" />
                            </svg>
                        </div>
                        <input type="text"
                               id="pattern"
                               name="pattern"
                               placeholder="*.md, src/ "
                               value="{{ pattern if pattern else '' }}"
                               class=" py-2 px-2 bg-[#E8F0FE] focus:outline-none w-full">
                    </div>
                </div>
            </div>
            <div class="w-[200px] sm:w-[200px] mt-3">
                <label for="file_size" class="block text-gray-700 mb-1">
                    Include files under: <span id="size_value" class="font-bold">50kb</span>
                </label>
                <input type="range"
                       id="file_size"
                       name="max_file_size"
                       min="0"
                       max="500"
                       required
                       value="{{ default_file_size }}"
                       class="w-full h-3 bg-[#FAFAFA] bg-no-repeat bg-[length:50%_100%] bg-[#ebdbb7] appearance-none border-[3px] border-gray-900 rounded-sm focus:outline-none bg-gradient-to-r from-[#FE4A60] to-[#FE4A60] [&::-webkit-slider-thumb]:w-5 [&::-webkit-slider-thumb]:h-7 [&::-webkit-slider-thumb]:appearance-none [&::-webkit-slider-thumb]:bg-white [&::-webkit-slider-thumb]:rounded-sm [&::-webkit-slider-thumb]:cursor-pointer [&::-webkit-slider-thumb]:border-solid [&::-webkit-slider-thumb]:border-[3px] [&::-webkit-slider-thumb]:border-gray-900 [&::-webkit-slider-thumb]:shadow-[3px_3px_0_#000]  ">
            </div>
        </div>
        {% if show_examples %}
            <!-- Example repositories section -->
            <div class="mt-4">
                <p class="opacity-70 mb-1">Try these example repositories:</p>
                <div class="flex flex-wrap gap-2">
                    {% for example in examples %}
                        <button onclick="submitExample('{{ example.url }}')"
                                class="px-4 py-1 bg-[#EBDBB7] hover:bg-[#FFC480] text-gray-900 rounded transition-colors duration-200 border-[3px] border-gray-900 relative hover:-translate-y-px hover:-translate-x-px">
                            {{ example.name }}
                        </button>
                    {% endfor %}
                </div>
            </div>
        {% endif %}
    </div>
</div>



================================================
FILE: src/server/templates/components/navbar.jinja
================================================
<script>
    function formatStarCount(count) {
        if (count >= 1000) {
            return (count / 1000).toFixed(1) + 'k';
        }
        return count.toString();
    }

    async function fetchGitHubStars() {
        try {
            const response = await fetch('https://api.github.com/repos/cyclotruc/gitingest');
            const data = await response.json();
            const starCount = data.stargazers_count;

            document.getElementById('github-stars').textContent = formatStarCount(starCount);
        } catch (error) {
            console.error('Error fetching GitHub stars:', error);
            document.getElementById('github-stars').parentElement.style.display = 'none';
        }
    }

    fetchGitHubStars();
</script>
<header class="sticky top-0 bg-[#FFFDF8] border-b-[3px] border-gray-900 z-50">
    <div class="max-w-4xl mx-auto px-4">
        <div class="flex justify-between items-center h-16">
            <!-- Logo -->
            <div class="flex items-center gap-4">
                <h1 class="text-2xl font-bold tracking-tight">
                    <a href="/" class="hover:opacity-80 transition-opacity">
                        <span class="text-gray-900">Git</span><span class="text-[#FE4A60]">ingest</span>
                    </a>
                </h1>
            </div>
            <!-- Navigation with updated styling -->
            <nav class="flex items-center space-x-6">
                <!-- Simplified Chrome extension button -->
                <a href="https://chromewebstore.google.com/detail/git-ingest-turn-any-git-r/adfjahbijlkjfoicpjkhjicpjpjfaood"
                   target="_blank"
                   rel="noopener noreferrer"
                   class="text-gray-900 hover:-translate-y-0.5 transition-transform flex items-center gap-1.5">
                    <div class="flex items-center">
                        <svg xmlns="http://www.w3.org/2000/svg"
                             width="24"
                             height="24"
                             viewBox="0 0 50 50"
                             fill="none"
                             stroke="currentColor"
                             stroke-width="3"
                             class="w-4 h-4 mx-1">
                            <path d="M 25 2 C 12.309295 2 2 12.309295 2 25 C 2 37.690705 12.309295 48 25 48 C 37.690705 48 48 37.690705 48 25 C 48 12.309295 37.690705 2 25 2 z M 25 4 C 32.987976 4 39.925645 8.44503 43.476562 15 L 25 15 A 1.0001 1.0001 0 0 0 24.886719 15.005859 C 19.738868 15.064094 15.511666 19.035373 15.046875 24.078125 L 8.0351562 12.650391 C 11.851593 7.4136918 18.014806 4 25 4 z M 6.8242188 14.501953 L 16.476562 30.230469 A 1.0001 1.0001 0 0 0 16.591797 30.388672 A 1.0001 1.0001 0 0 0 16.59375 30.392578 C 18.3752 33.158533 21.474925 35 25 35 C 26.413063 35 27.756327 34.701734 28.976562 34.169922 L 22.320312 45.824219 C 11.979967 44.509804 4 35.701108 4 25 C 4 21.169738 5.0375742 17.591533 6.8242188 14.501953 z M 25 17 C 29.430123 17 33 20.569877 33 25 C 33 26.42117 32.629678 27.751591 31.984375 28.90625 A 1.0001 1.0001 0 0 0 31.982422 28.908203 A 1.0001 1.0001 0 0 0 31.947266 28.966797 C 30.57172 31.37734 27.983486 33 25 33 C 20.569877 33 17 29.430123 17 25 C 17 20.569877 20.569877 17 25 17 z M 30.972656 17 L 44.421875 17 C 45.43679 19.465341 46 22.165771 46 25 C 46 36.609824 36.609824 46 25 46 C 24.842174 46 24.686285 45.991734 24.529297 45.988281 L 33.683594 29.958984 A 1.0001 1.0001 0 0 0 33.742188 29.841797 C 34.541266 28.405674 35 26.755664 35 25 C 35 21.728612 33.411062 18.825934 30.972656 17 z" />
                        </svg>
                        Extension
                    </div>
                </a>
                <div class="flex items-center gap-2">
                    <a href="https://github.com/cyclotruc/gitingest"
                       target="_blank"
                       rel="noopener noreferrer"
                       class="text-gray-900 hover:-translate-y-0.5 transition-transform flex items-center gap-1.5">
                        <svg class="w-4 h-4"
                             fill="currentColor"
                             viewBox="0 0 24 24"
                             aria-hidden="true">
                            <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd">
                            </path>
                        </svg>
                        GitHub
                    </a>
                    <div class="flex items-center text-sm text-gray-600">
                        <svg class="w-4 h-4 text-[#ffc480] mr-1"
                             fill="currentColor"
                             viewBox="0 0 20 20">
                            <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z" />
                        </svg>
                        <span id="github-stars">0</span>
                    </div>
                </div>
            </nav>
        </div>
    </div>
</header>



================================================
FILE: src/server/templates/components/result.jinja
================================================
<script>
    function getFileName(line) {
        // Skips "|", "‚îî", "‚îú" found in file tree
        const index = line.search(/[a-zA-Z0-9]/);
        return line.substring(index).trim();
    }

    function toggleFile(element) {
        const patternInput = document.getElementById("pattern");
        const patternFiles = patternInput.value ? patternInput.value.split(",").map(item => item.trim()) : [];

        if (element.textContent.includes("Directory structure:")) {
            return;
        }

        element.classList.toggle('line-through');
        element.classList.toggle('text-gray-500');

        const fileName = getFileName(element.textContent);
        const fileIndex = patternFiles.indexOf(fileName);

        if (fileIndex !== -1) {
            patternFiles.splice(fileIndex, 1);
        } else {
            patternFiles.push(fileName);
        }

        patternInput.value = patternFiles.join(", ");
    }
</script>
{% if result %}
    <div class="mt-10" data-results>
        <div class="relative">
            <div class="w-full h-full absolute inset-0 bg-gray-900 rounded-xl translate-y-2 translate-x-2"></div>
            <div class="bg-[#fafafa] rounded-xl border-[3px] border-gray-900 p-6 relative z-20 space-y-6">
                <!-- Summary and Directory Structure -->
                <div class="grid grid-cols-1 md:grid-cols-12 gap-6">
                    <!-- Summary Column -->
                    <div class="md:col-span-5">
                        <div class="flex justify-between items-center mb-4 py-2">
                            <h3 class="text-lg font-bold text-gray-900">Summary</h3>
                        </div>
                        <div class="relative">
                            <div class="w-full h-full rounded bg-gray-900 translate-y-1 translate-x-1 absolute inset-0"></div>
                            <textarea class="w-full h-[160px] p-4 bg-[#fff4da] border-[3px] border-gray-900 rounded font-mono text-sm resize-none focus:outline-none relative z-10"
                                      readonly>{{ summary }}</textarea>
                        </div>
                        {% if ingest_id %}
                            <div class="relative mt-4 inline-block group">
                                <div class="w-full h-full rounded bg-gray-900 translate-y-1 translate-x-1 absolute inset-0"></div>
                                <a href="/download/{{ ingest_id }}"
                                   class="inline-flex items-center px-4 py-2 bg-[#ffc480] border-[3px] border-gray-900 text-gray-900 rounded group-hover:-translate-y-px group-hover:-translate-x-px transition-transform relative z-10">
                                    <svg class="w-4 h-4 mr-2"
                                         fill="none"
                                         stroke="currentColor"
                                         viewBox="0 0 24 24">
                                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16v1a3 3 0 003 3h10a3 3 0 003-3v-1m-4-4l-4 4m0 0l-4-4m4 4V4" />
                                    </svg>
                                    Download
                                </a>
                            </div>
                            <div class="relative mt-4 inline-block group ml-4">
                                <div class="w-full h-full rounded bg-gray-900 translate-y-1 translate-x-1 absolute inset-0"></div>
                                <button onclick="copyFullDigest()"
                                        class="inline-flex items-center px-4 py-2 bg-[#ffc480] border-[3px] border-gray-900 text-gray-900 rounded group-hover:-translate-y-px group-hover:-translate-x-px transition-transform relative z-10">
                                    <svg class="w-4 h-4 mr-2"
                                         fill="none"
                                         stroke="currentColor"
                                         viewBox="0 0 24 24">
                                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 5H6a2 2 0 00-2 2v12a2 2 0 002 2h10a2 2 0 002-2v-1M8 5a2 2 0 002 2h2a2 2 0 002-2M8 5a2 2 0 012-2h2a2 2 0 012 2m0 0h2a2 2 0 012 2v3m2 4H10m0 0l3-3m-3 3l3 3" />
                                    </svg>
                                    Copy all
                                </button>
                            </div>
                        {% endif %}
                    </div>
                    <!-- Directory Structure Column -->
                    <div class="md:col-span-7">
                        <div class="flex justify-between items-center mb-4">
                            <h3 class="text-lg font-bold text-gray-900">Directory Structure</h3>
                            <div class="relative group">
                                <div class="w-full h-full rounded bg-gray-900 translate-y-1 translate-x-1 absolute inset-0"></div>
                                <button onclick="copyText('directory-structure')"
                                        class="px-4 py-2 bg-[#ffc480] border-[3px] border-gray-900 text-gray-900 rounded group-hover:-translate-y-px group-hover:-translate-x-px transition-transform relative z-10 flex items-center gap-2">
                                    <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 5H6a2 2 0 00-2 2v12a2 2 0 002 2h10a2 2 0 002-2v-1M8 5a2 2 0 002 2h2a2 2 0 002-2M8 5a2 2 0 012-2h2a2 2 0 012 2m0 0h2a2 2 0 012 2v3m2 4H10m0 0l3-3m-3 3l3 3" />
                                    </svg>
                                    Copy
                                </button>
                            </div>
                        </div>
                        <div class="relative">
                            <div class="w-full h-full rounded bg-gray-900 translate-y-1 translate-x-1 absolute inset-0"></div>
                            <div class="directory-structure w-full p-4 bg-[#fff4da] border-[3px] border-gray-900 rounded font-mono text-sm resize-y focus:outline-none relative z-10 h-[215px] overflow-auto"
                                 id="directory-structure-container"
                                 readonly>
                                <input type="hidden" id="directory-structure-content" value="{{ tree }}" />
                                {% for line in tree.splitlines() %}
                                    <pre name="tree-line"
                                         class="cursor-pointer hover:line-through hover:text-gray-500"
                                         onclick="toggleFile(this)">{{ line }}</pre>
                                {% endfor %}
                            </div>
                        </div>
                    </div>
                </div>
                <!-- Full Digest -->
                <div>
                    <div class="flex justify-between items-center mb-4">
                        <h3 class="text-lg font-bold text-gray-900">Files Content</h3>
                        <div class="relative group">
                            <div class="w-full h-full rounded bg-gray-900 translate-y-1 translate-x-1 absolute inset-0"></div>
                            <button onclick="copyText('result-text')"
                                    class="px-4 py-2 bg-[#ffc480] border-[3px] border-gray-900 text-gray-900 rounded group-hover:-translate-y-px group-hover:-translate-x-px transition-transform relative z-10 flex items-center gap-2">
                                <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 5H6a2 2 0 00-2 2v12a2 2 0 002 2h10a2 2 0 002-2v-1M8 5a2 2 0 002 2h2a2 2 0 002-2M8 5a2 2 0 012-2h2a2 2 0 012 2m0 0h2a2 2 0 012 2v3m2 4H10m0 0l3-3m-3 3l3 3" />
                                </svg>
                                Copy
                            </button>
                        </div>
                    </div>
                    <div class="relative">
                        <div class="w-full h-full rounded bg-gray-900 translate-y-1 translate-x-1 absolute inset-0"></div>
                        <textarea class="result-text w-full p-4 bg-[#fff4da] border-[3px] border-gray-900 rounded font-mono text-sm resize-y focus:outline-none relative z-10"
                                  style="min-height: {{ '600px' if content else 'calc(100vh-800px)' }}"
                                  readonly>{{ content }}</textarea>
                    </div>
                </div>
            </div>
        </div>
    </div>
{% endif %}



================================================
FILE: src/static/robots.txt
================================================
User-agent: *
Allow: /
Allow: /api/
Allow: /cyclotruc/gitingest/



================================================
FILE: src/static/js/utils.js
================================================
// Copy functionality
function copyText(className) {
    let textToCopy;

    if (className === 'directory-structure') {
        // For directory structure, get the hidden input value
        const hiddenInput = document.getElementById('directory-structure-content');
        if (!hiddenInput) return;
        textToCopy = hiddenInput.value;
    } else {
        // For other elements, get the textarea value
        const textarea = document.querySelector('.' + className);
        if (!textarea) return;
        textToCopy = textarea.value;
    }

    const button = document.querySelector(`button[onclick="copyText('${className}')"]`);
    if (!button) return;

    // Copy text
    navigator.clipboard.writeText(textToCopy)
        .then(() => {
            // Store original content
            const originalContent = button.innerHTML;

            // Change button content
            button.innerHTML = 'Copied!';

            // Reset after 1 second
            setTimeout(() => {
                button.innerHTML = originalContent;
            }, 1000);
        })
        .catch(err => {
            // Show error in button
            const originalContent = button.innerHTML;
            button.innerHTML = 'Failed to copy';
            setTimeout(() => {
                button.innerHTML = originalContent;
            }, 1000);
        });
}


function handleSubmit(event, showLoading = false) {
    event.preventDefault();
    const form = event.target || document.getElementById('ingestForm');
    if (!form) return;

    const submitButton = form.querySelector('button[type="submit"]');
    if (!submitButton) return;

    const formData = new FormData(form);

    // Update file size
    const slider = document.getElementById('file_size');
    if (slider) {
        formData.delete('max_file_size');
        formData.append('max_file_size', slider.value);
    }

    // Update pattern type and pattern
    const patternType = document.getElementById('pattern_type');
    const pattern = document.getElementById('pattern');
    if (patternType && pattern) {
        formData.delete('pattern_type');
        formData.delete('pattern');
        formData.append('pattern_type', patternType.value);
        formData.append('pattern', pattern.value);
    }

    const originalContent = submitButton.innerHTML;
    const currentStars = document.getElementById('github-stars')?.textContent;

    if (showLoading) {
        submitButton.disabled = true;
        submitButton.innerHTML = `
            <div class="flex items-center justify-center">
                <svg class="animate-spin h-5 w-5 text-gray-900" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24">
                    <circle class="opacity-25" cx="12" cy="12" r="10" stroke="currentColor" stroke-width="4"></circle>
                    <path class="opacity-75" fill="currentColor" d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4zm2 5.291A7.962 7.962 0 014 12H0c0 3.042 1.135 5.824 3 7.938l3-2.647z"></path>
                </svg>
                <span class="ml-2">Processing...</span>
            </div>
        `;
        submitButton.classList.add('bg-[#ffb14d]');
    }

    // Submit the form
    fetch(form.action, {
        method: 'POST',
        body: formData
    })
        .then(response => response.text())
        .then(html => {
            // Store the star count before updating the DOM
            const starCount = currentStars;

            // Replace the entire body content with the new HTML
            document.body.innerHTML = html;

            // Wait for next tick to ensure DOM is updated
            setTimeout(() => {
                // Reinitialize slider functionality
                initializeSlider();

                const starsElement = document.getElementById('github-stars');
                if (starsElement && starCount) {
                    starsElement.textContent = starCount;
                }

                // Scroll to results if they exist
                const resultsSection = document.querySelector('[data-results]');
                if (resultsSection) {
                    resultsSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
                }
            }, 0);
        })
        .catch(error => {
            submitButton.disabled = false;
            submitButton.innerHTML = originalContent;
        });
}

function copyFullDigest() {
    const directoryStructure = document.getElementById('directory-structure-content').value;
    const filesContent = document.querySelector('.result-text').value;
    const fullDigest = `${directoryStructure}\n\nFiles Content:\n\n${filesContent}`;
    const button = document.querySelector('[onclick="copyFullDigest()"]');
    const originalText = button.innerHTML;

    navigator.clipboard.writeText(fullDigest).then(() => {
        button.innerHTML = `
            <svg class="w-4 h-4 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path>
            </svg>
            Copied!
        `;

        setTimeout(() => {
            button.innerHTML = originalText;
        }, 2000);
    }).catch(err => {
        console.error('Failed to copy text: ', err);
    });
}

// Add the logSliderToSize helper function
function logSliderToSize(position) {
    const minp = 0;
    const maxp = 500;
    const minv = Math.log(1);
    const maxv = Math.log(102400);

    const value = Math.exp(minv + (maxv - minv) * Math.pow(position / maxp, 1.5));
    return Math.round(value);
}

// Move slider initialization to a separate function
function initializeSlider() {
    const slider = document.getElementById('file_size');
    const sizeValue = document.getElementById('size_value');

    if (!slider || !sizeValue) return;

    function updateSlider() {
        const value = logSliderToSize(slider.value);
        sizeValue.textContent = formatSize(value);
        slider.style.backgroundSize = `${(slider.value / slider.max) * 100}% 100%`;
    }

    // Update on slider change
    slider.addEventListener('input', updateSlider);

    // Initialize slider position
    updateSlider();
}

// Add helper function for formatting size
function formatSize(sizeInKB) {
    if (sizeInKB >= 1024) {
        return Math.round(sizeInKB / 1024) + 'mb';
    }
    return Math.round(sizeInKB) + 'kb';
}

// Initialize slider on page load
document.addEventListener('DOMContentLoaded', initializeSlider);

// Make sure these are available globally
window.copyText = copyText;

window.handleSubmit = handleSubmit;
window.initializeSlider = initializeSlider;
window.formatSize = formatSize;

// Add this new function
function setupGlobalEnterHandler() {
    document.addEventListener('keydown', function (event) {
        if (event.key === 'Enter' && !event.target.matches('textarea')) {
            const form = document.getElementById('ingestForm');
            if (form) {
                handleSubmit(new Event('submit'), true);
            }
        }
    });
}

// Add to the DOMContentLoaded event listener
document.addEventListener('DOMContentLoaded', () => {
    initializeSlider();
    setupGlobalEnterHandler();
});



================================================
FILE: tests/__init__.py
================================================



================================================
FILE: tests/conftest.py
================================================
"""
Fixtures for tests.

This file provides shared fixtures for creating sample queries, a temporary directory structure, and a helper function
to write `.ipynb` notebooks for testing notebook utilities.
"""

import json
from pathlib import Path
from typing import Any, Callable, Dict

import pytest

from gitingest.query_parsing import IngestionQuery

WriteNotebookFunc = Callable[[str, Dict[str, Any]], Path]


@pytest.fixture
def sample_query() -> IngestionQuery:
    """
    Provide a default `IngestionQuery` object for use in tests.

    This fixture returns a `IngestionQuery` pre-populated with typical fields and some default ignore patterns.

    Returns
    -------
    IngestionQuery
        The sample `IngestionQuery` object.
    """
    return IngestionQuery(
        user_name="test_user",
        repo_name="test_repo",
        url=None,
        subpath="/",
        local_path=Path("/tmp/test_repo").resolve(),
        slug="test_user/test_repo",
        id="id",
        branch="main",
        max_file_size=1_000_000,
        ignore_patterns={"*.pyc", "__pycache__", ".git"},
        include_patterns=None,
    )


@pytest.fixture
def temp_directory(tmp_path: Path) -> Path:
    """
    Create a temporary directory structure for testing repository scanning.

    The structure includes:
    test_repo/
    ‚îú‚îÄ‚îÄ file1.txt
    ‚îú‚îÄ‚îÄ file2.py
    ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îú‚îÄ‚îÄ subfile1.txt
    ‚îÇ   ‚îú‚îÄ‚îÄ subfile2.py
    ‚îÇ   ‚îî‚îÄ‚îÄ subdir/
    ‚îÇ       ‚îú‚îÄ‚îÄ file_subdir.txt
    ‚îÇ       ‚îî‚îÄ‚îÄ file_subdir.py
    ‚îú‚îÄ‚îÄ dir1/
    ‚îÇ   ‚îî‚îÄ‚îÄ file_dir1.txt
    ‚îî‚îÄ‚îÄ dir2/
        ‚îî‚îÄ‚îÄ file_dir2.txt

    Parameters
    ----------
    tmp_path : Path
        The temporary directory path provided by the `tmp_path` fixture.

    Returns
    -------
    Path
        The path to the created `test_repo` directory.
    """
    test_dir = tmp_path / "test_repo"
    test_dir.mkdir()

    # Root files
    (test_dir / "file1.txt").write_text("Hello World")
    (test_dir / "file2.py").write_text("print('Hello')")

    # src directory and its files
    src_dir = test_dir / "src"
    src_dir.mkdir()
    (src_dir / "subfile1.txt").write_text("Hello from src")
    (src_dir / "subfile2.py").write_text("print('Hello from src')")

    # src/subdir and its files
    subdir = src_dir / "subdir"
    subdir.mkdir()
    (subdir / "file_subdir.txt").write_text("Hello from subdir")
    (subdir / "file_subdir.py").write_text("print('Hello from subdir')")

    # dir1 and its file
    dir1 = test_dir / "dir1"
    dir1.mkdir()
    (dir1 / "file_dir1.txt").write_text("Hello from dir1")

    # dir2 and its file
    dir2 = test_dir / "dir2"
    dir2.mkdir()
    (dir2 / "file_dir2.txt").write_text("Hello from dir2")

    return test_dir


@pytest.fixture
def write_notebook(tmp_path: Path) -> WriteNotebookFunc:
    """
    Provide a helper function to write a `.ipynb` notebook file with the given content.

    Parameters
    ----------
    tmp_path : Path
        The temporary directory path provided by the `tmp_path` fixture.

    Returns
    -------
    WriteNotebookFunc
        A callable that accepts a filename and a dictionary (representing JSON notebook data), writes it to a `.ipynb`
        file, and returns the path to the file.
    """

    def _write_notebook(name: str, content: Dict[str, Any]) -> Path:
        notebook_path = tmp_path / name
        with notebook_path.open(mode="w", encoding="utf-8") as f:
            json.dump(content, f)
        return notebook_path

    return _write_notebook



================================================
FILE: tests/test_cli.py
================================================
"""Tests for the Gitingest CLI."""

import os
from inspect import signature
from pathlib import Path
from typing import List

import pytest
from _pytest.monkeypatch import MonkeyPatch
from click.testing import CliRunner, Result

from gitingest.cli import main
from gitingest.config import MAX_FILE_SIZE, OUTPUT_FILE_NAME


@pytest.mark.parametrize(
    "cli_args, expect_file",
    [
        pytest.param(["./"], True, id="default-options"),
        pytest.param(
            [
                "./",
                "--output",
                str(OUTPUT_FILE_NAME),
                "--max-size",
                str(MAX_FILE_SIZE),
                "--exclude-pattern",
                "tests/",
                "--include-pattern",
                "src/",
            ],
            True,
            id="custom-options",
        ),
    ],
)
def test_cli_writes_file(tmp_path: Path, monkeypatch: MonkeyPatch, cli_args: List[str], expect_file: bool) -> None:
    """Run the CLI and verify that the SARIF file is created (or not)."""
    # Work inside an isolated temp directory
    monkeypatch.chdir(tmp_path)

    result = _invoke_isolated_cli_runner(cli_args)

    assert result.exit_code == 0, result.stderr

    # Summary line should be on STDOUT
    stdout_lines = result.stdout.splitlines()
    assert f"Analysis complete! Output written to: {OUTPUT_FILE_NAME}" in stdout_lines

    # File side-effect
    sarif_file = tmp_path / OUTPUT_FILE_NAME
    assert sarif_file.exists() is expect_file, f"{OUTPUT_FILE_NAME} existence did not match expectation"


def test_cli_with_stdout_output() -> None:
    """Test CLI invocation with output directed to STDOUT."""
    result = _invoke_isolated_cli_runner(["./", "--output", "-", "--exclude-pattern", "tests/"])

    # ‚îÄ‚îÄ‚îÄ core expectations (stdout) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ-
    assert result.exit_code == 0, f"CLI exited with code {result.exit_code}, stderr: {result.stderr}"
    assert "---" in result.stdout, "Expected file separator '---' not found in STDOUT"
    assert "src/gitingest/cli.py" in result.stdout, "Expected content (e.g., src/gitingest/cli.py) not found in STDOUT"
    assert not os.path.exists(OUTPUT_FILE_NAME), f"Output file {OUTPUT_FILE_NAME} was unexpectedly created."

    # ‚îÄ‚îÄ‚îÄ the summary must *not* pollute STDOUT, must appear on STDERR ‚îÄ‚îÄ‚îÄ
    summary = "Analysis complete! Output sent to stdout."
    stdout_lines = result.stdout.splitlines()
    stderr_lines = result.stderr.splitlines()
    assert summary not in stdout_lines, "Unexpected summary message found in STDOUT"
    assert summary in stderr_lines, "Expected summary message not found in STDERR"
    assert f"Output written to: {OUTPUT_FILE_NAME}" not in stderr_lines


def _invoke_isolated_cli_runner(args: List[str]) -> Result:
    """Return a CliRunner that keeps stderr apart on Click 8.0-8.1."""
    kwargs = {}
    if "mix_stderr" in signature(CliRunner.__init__).parameters:
        kwargs["mix_stderr"] = False  # Click 8.0‚Äì8.1
    runner = CliRunner(**kwargs)
    return runner.invoke(main, args)



================================================
FILE: tests/test_flow_integration.py
================================================
"""Integration tests covering core functionalities, edge cases, and concurrency handling."""

import shutil
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from unittest.mock import patch

import pytest
from fastapi.testclient import TestClient

from src.server.main import app

BASE_DIR = Path(__file__).resolve().parent.parent
TEMPLATE_DIR = BASE_DIR / "src" / "templates"


@pytest.fixture(scope="module")
def test_client():
    """Create a test client fixture."""
    with TestClient(app) as client_instance:
        client_instance.headers.update({"Host": "localhost"})
        yield client_instance


@pytest.fixture(scope="module", autouse=True)
def mock_static_files():
    """Mock the static file mount to avoid directory errors."""
    with patch("src.server.main.StaticFiles") as mock_static:
        mock_static.return_value = None  # Mocks the StaticFiles response
        yield mock_static


@pytest.fixture(scope="module", autouse=True)
def mock_templates():
    """Mock Jinja2 template rendering to bypass actual file loading."""
    with patch("starlette.templating.Jinja2Templates.TemplateResponse") as mock_template:
        mock_template.return_value = "Mocked Template Response"
        yield mock_template


def cleanup_temp_directories():
    temp_dir = Path("/tmp/gitingest")
    if temp_dir.exists():
        try:
            shutil.rmtree(temp_dir)
        except PermissionError as exc:
            print(f"Error cleaning up {temp_dir}: {exc}")


@pytest.fixture(scope="module", autouse=True)
def cleanup():
    """Cleanup temporary directories after tests."""
    yield
    cleanup_temp_directories()


@pytest.mark.asyncio
async def test_remote_repository_analysis(request):
    """Test the complete flow of analyzing a remote repository."""
    client = request.getfixturevalue("test_client")
    form_data = {
        "input_text": "https://github.com/octocat/Hello-World",
        "max_file_size": "243",
        "pattern_type": "exclude",
        "pattern": "",
    }

    response = client.post("/", data=form_data)
    assert response.status_code == 200, f"Form submission failed: {response.text}"
    assert "Mocked Template Response" in response.text


@pytest.mark.asyncio
async def test_invalid_repository_url(request):
    """Test handling of an invalid repository URL."""
    client = request.getfixturevalue("test_client")
    form_data = {
        "input_text": "https://github.com/nonexistent/repo",
        "max_file_size": "243",
        "pattern_type": "exclude",
        "pattern": "",
    }

    response = client.post("/", data=form_data)
    assert response.status_code == 200, f"Request failed: {response.text}"
    assert "Mocked Template Response" in response.text


@pytest.mark.asyncio
async def test_large_repository(request):
    """Simulate analysis of a large repository with nested folders."""
    client = request.getfixturevalue("test_client")
    form_data = {
        "input_text": "https://github.com/large/repo-with-many-files",
        "max_file_size": "243",
        "pattern_type": "exclude",
        "pattern": "",
    }

    response = client.post("/", data=form_data)
    assert response.status_code == 200, f"Request failed: {response.text}"
    assert "Mocked Template Response" in response.text


@pytest.mark.asyncio
async def test_concurrent_requests(request):
    """Test handling of multiple concurrent requests."""
    client = request.getfixturevalue("test_client")

    def make_request():
        form_data = {
            "input_text": "https://github.com/octocat/Hello-World",
            "max_file_size": "243",
            "pattern_type": "exclude",
            "pattern": "",
        }
        response = client.post("/", data=form_data)
        assert response.status_code == 200, f"Request failed: {response.text}"
        assert "Mocked Template Response" in response.text

    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(make_request) for _ in range(5)]
        for future in futures:
            future.result()


@pytest.mark.asyncio
async def test_large_file_handling(request):
    """Test handling of repositories with large files."""
    client = request.getfixturevalue("test_client")
    form_data = {
        "input_text": "https://github.com/octocat/Hello-World",
        "max_file_size": "1",
        "pattern_type": "exclude",
        "pattern": "",
    }

    response = client.post("/", data=form_data)
    assert response.status_code == 200, f"Request failed: {response.text}"
    assert "Mocked Template Response" in response.text


@pytest.mark.asyncio
async def test_repository_with_patterns(request):
    """Test repository analysis with include/exclude patterns."""
    client = request.getfixturevalue("test_client")
    form_data = {
        "input_text": "https://github.com/octocat/Hello-World",
        "max_file_size": "243",
        "pattern_type": "include",
        "pattern": "*.md",
    }

    response = client.post("/", data=form_data)
    assert response.status_code == 200, f"Request failed: {response.text}"
    assert "Mocked Template Response" in response.text



================================================
FILE: tests/test_ingestion.py
================================================
"""
Tests for the `query_ingestion` module.

These tests validate directory scanning, file content extraction, notebook handling, and the overall ingestion logic,
including filtering patterns and subpaths.
"""

import re
from pathlib import Path
from typing import Set, TypedDict

import pytest

from gitingest.ingestion import ingest_query
from gitingest.query_parsing import IngestionQuery


def test_run_ingest_query(temp_directory: Path, sample_query: IngestionQuery) -> None:
    """
    Test `ingest_query` to ensure it processes the directory and returns expected results.

    Given a directory with .txt and .py files:
    When `ingest_query` is invoked,
    Then it should produce a summary string listing the files analyzed and a combined content string.
    """
    sample_query.local_path = temp_directory
    sample_query.subpath = "/"
    sample_query.type = None

    summary, _, content = ingest_query(sample_query)

    assert "Repository: test_user/test_repo" in summary
    assert "Files analyzed: 8" in summary

    # Check presence of key files in the content
    assert "src/subfile1.txt" in content
    assert "src/subfile2.py" in content
    assert "src/subdir/file_subdir.txt" in content
    assert "src/subdir/file_subdir.py" in content
    assert "file1.txt" in content
    assert "file2.py" in content
    assert "dir1/file_dir1.txt" in content
    assert "dir2/file_dir2.txt" in content


# TODO: Additional tests:
# - Multiple include patterns, e.g. ["*.txt", "*.py"] or ["/src/*", "*.txt"].
# - Edge cases with weird file names or deep subdirectory structures.
# TODO : def test_include_nonexistent_extension


class PatternScenario(TypedDict):
    include_patterns: Set[str]
    ignore_patterns: Set[str]
    expected_num_files: int
    expected_content: Set[str]
    expected_structure: Set[str]
    expected_not_structure: Set[str]


@pytest.mark.parametrize(
    "pattern_scenario",
    [
        pytest.param(
            PatternScenario(
                {
                    "include_patterns": {"file2.py", "dir2/file_dir2.txt"},
                    "ignore_patterns": {*()},
                    "expected_num_files": 2,
                    "expected_content": {"file2.py", "dir2/file_dir2.txt"},
                    "expected_structure": {"test_repo/", "dir2/"},
                    "expected_not_structure": {"src/", "subdir/", "dir1/"},
                }
            ),
            id="include-explicit-files",
        ),
        pytest.param(
            PatternScenario(
                {
                    "include_patterns": {
                        "file1.txt",
                        "file2.py",
                        "file_dir1.txt",
                        "*/file_dir2.txt",
                    },
                    "ignore_patterns": {*()},
                    "expected_num_files": 3,
                    "expected_content": {"file1.txt", "file2.py", "dir2/file_dir2.txt"},
                    "expected_structure": {"test_repo/", "dir2/"},
                    "expected_not_structure": {"src/", "subdir/", "dir1/"},
                }
            ),
            id="include-wildcard-directory",
        ),
        pytest.param(
            PatternScenario(
                {
                    "include_patterns": {"*.py"},
                    "ignore_patterns": {*()},
                    "expected_num_files": 3,
                    "expected_content": {
                        "file2.py",
                        "src/subfile2.py",
                        "src/subdir/file_subdir.py",
                    },
                    "expected_structure": {"test_repo/", "src/", "subdir/"},
                    "expected_not_structure": {"dir1/", "dir2/"},
                }
            ),
            id="include-wildcard-files",
        ),
        pytest.param(
            PatternScenario(
                {
                    "include_patterns": {"**/file_dir2.txt", "src/**/*.py"},
                    "ignore_patterns": {*()},
                    "expected_num_files": 2,
                    "expected_content": {
                        "dir2/file_dir2.txt",
                        "src/subdir/file_subdir.py",
                    },
                    "expected_structure": {"test_repo/", "dir2/", "src/", "subdir/"},
                    "expected_not_structure": {"dir1/"},
                }
            ),
            id="include-recursive-wildcard",
        ),
        pytest.param(
            PatternScenario(
                {
                    "include_patterns": {*()},
                    "ignore_patterns": {"file2.py", "dir2/file_dir2.txt"},
                    "expected_num_files": 6,
                    "expected_content": {
                        "file1.txt",
                        "src/subfile1.txt",
                        "src/subfile2.py",
                        "src/subdir/file_subdir.txt",
                        "src/subdir/file_subdir.py",
                        "dir1/file_dir1.txt",
                    },
                    "expected_structure": {"test_repo/", "src/", "subdir/", "dir1/"},
                    "expected_not_structure": {"dir2/"},
                }
            ),
            id="exclude-explicit-files",
        ),
        pytest.param(
            PatternScenario(
                {
                    "include_patterns": {*()},
                    "ignore_patterns": {"file1.txt", "file2.py", "*/file_dir1.txt"},
                    "expected_num_files": 5,
                    "expected_content": {
                        "src/subfile1.txt",
                        "src/subfile2.py",
                        "src/subdir/file_subdir.txt",
                        "src/subdir/file_subdir.py",
                        "dir2/file_dir2.txt",
                    },
                    "expected_structure": {"test_repo/", "src/", "subdir/", "dir2/"},
                    "expected_not_structure": {"dir1/"},
                }
            ),
            id="exclude-wildcard-directory",
        ),
        pytest.param(
            PatternScenario(
                {
                    "include_patterns": {*()},
                    "ignore_patterns": {"src/**/*.py"},
                    "expected_num_files": 7,
                    "expected_content": {
                        "file1.txt",
                        "file2.py",
                        "src/subfile1.txt",
                        "src/subfile2.py",
                        "src/subdir/file_subdir.txt",
                        "dir1/file_dir1.txt",
                        "dir2/file_dir2.txt",
                    },
                    "expected_structure": {
                        "test_repo/",
                        "dir1/",
                        "dir2/",
                        "src/",
                        "subdir/",
                    },
                    "expected_not_structure": {*()},
                }
            ),
            id="exclude-recursive-wildcard",
        ),
    ],
)
def test_include_ignore_patterns(
    temp_directory: Path,
    sample_query: IngestionQuery,
    pattern_scenario: PatternScenario,
) -> None:
    """
    Test `ingest_query` to ensure included and ignored paths are included and ignored respectively.

    Given a directory with .txt and .py files, and a set of include patterns or a set of ignore patterns:
    When `ingest_query` is invoked,
    Then it should produce a summary string listing the files analyzed and a combined content string.
    """

    sample_query.local_path = temp_directory
    sample_query.subpath = "/"
    sample_query.type = None
    sample_query.include_patterns = pattern_scenario["include_patterns"] or None
    sample_query.ignore_patterns = pattern_scenario["ignore_patterns"] or None

    summary, structure, content = ingest_query(sample_query)

    assert "Repository: test_user/test_repo" in summary
    num_files_regex = re.compile(r"^Files analyzed: (\d+)$", re.MULTILINE)
    assert (num_files_match := num_files_regex.search(summary)) is not None
    assert int(num_files_match.group(1)) == pattern_scenario["expected_num_files"]

    # Check presence of key files in the content
    for expected_content_item in pattern_scenario["expected_content"]:
        assert expected_content_item in content

    # check presence of included directories in structure
    for expected_structure_item in pattern_scenario["expected_structure"]:
        assert expected_structure_item in structure

    # check non-presence of non-included directories in structure
    for expected_not_structure_item in pattern_scenario["expected_not_structure"]:
        assert expected_not_structure_item not in structure



================================================
FILE: tests/test_notebook_utils.py
================================================
"""
Tests for the `notebook_utils` module.

These tests validate how notebooks are processed into Python-like output, ensuring that markdown/raw cells are
converted to triple-quoted blocks, code cells remain executable code, and various edge cases (multiple worksheets,
empty cells, outputs, etc.) are handled appropriately.
"""

import pytest

from gitingest.utils.notebook_utils import process_notebook
from tests.conftest import WriteNotebookFunc


def test_process_notebook_all_cells(write_notebook: WriteNotebookFunc) -> None:
    """
    Test processing a notebook containing markdown, code, and raw cells.

    Given a notebook with:
      - One markdown cell
      - One code cell
      - One raw cell
    When `process_notebook` is invoked,
    Then markdown and raw cells should appear in triple-quoted blocks, and code cells remain as normal code.
    """
    notebook_content = {
        "cells": [
            {"cell_type": "markdown", "source": ["# Markdown cell"]},
            {"cell_type": "code", "source": ['print("Hello Code")']},
            {"cell_type": "raw", "source": ["<raw content>"]},
        ]
    }
    nb_path = write_notebook("all_cells.ipynb", notebook_content)
    result = process_notebook(nb_path)

    assert result.count('"""') == 4, "Two non-code cells => 2 triple-quoted blocks => 4 total triple quotes."

    # Ensure markdown and raw cells are in triple quotes
    assert "# Markdown cell" in result
    assert "<raw content>" in result

    # Ensure code cell is not in triple quotes
    assert 'print("Hello Code")' in result
    assert '"""\nprint("Hello Code")\n"""' not in result


def test_process_notebook_with_worksheets(write_notebook: WriteNotebookFunc) -> None:
    """
    Test a notebook containing the (as of IPEP-17 deprecated) 'worksheets' key.

    Given a notebook that uses the 'worksheets' key with a single worksheet,
    When `process_notebook` is called,
    Then a `DeprecationWarning` should be raised, and the content should match an equivalent notebook
    that has top-level 'cells'.
    """
    with_worksheets = {
        "worksheets": [
            {
                "cells": [
                    {"cell_type": "markdown", "source": ["# Markdown cell"]},
                    {"cell_type": "code", "source": ['print("Hello Code")']},
                    {"cell_type": "raw", "source": ["<raw content>"]},
                ]
            }
        ]
    }
    without_worksheets = with_worksheets["worksheets"][0]  # same, but no 'worksheets' key

    nb_with = write_notebook("with_worksheets.ipynb", with_worksheets)
    nb_without = write_notebook("without_worksheets.ipynb", without_worksheets)

    with pytest.warns(DeprecationWarning, match="Worksheets are deprecated as of IPEP-17."):
        result_with = process_notebook(nb_with)

    # Should not raise a warning
    result_without = process_notebook(nb_without)

    assert result_with == result_without, "Content from the single worksheet should match the top-level equivalent."


def test_process_notebook_multiple_worksheets(write_notebook: WriteNotebookFunc) -> None:
    """
    Test a notebook containing multiple 'worksheets'.

    Given a notebook with two worksheets:
      - First with a markdown cell
      - Second with a code cell
    When `process_notebook` is called,
    Then a warning about multiple worksheets should be raised, and the second worksheet's content should appear
    in the final output.
    """
    multi_worksheets = {
        "worksheets": [
            {"cells": [{"cell_type": "markdown", "source": ["# First Worksheet"]}]},
            {"cells": [{"cell_type": "code", "source": ["# Second Worksheet"]}]},
        ]
    }

    single_worksheet = {
        "worksheets": [
            {"cells": [{"cell_type": "markdown", "source": ["# First Worksheet"]}]},
        ]
    }

    nb_multi = write_notebook("multiple_worksheets.ipynb", multi_worksheets)
    nb_single = write_notebook("single_worksheet.ipynb", single_worksheet)

    # Expect DeprecationWarning + UserWarning
    with pytest.warns(
        DeprecationWarning, match="Worksheets are deprecated as of IPEP-17. Consider updating the notebook."
    ):
        with pytest.warns(
            UserWarning, match="Multiple worksheets detected. Combining all worksheets into a single script."
        ):
            result_multi = process_notebook(nb_multi)

    # Expect DeprecationWarning only
    with pytest.warns(
        DeprecationWarning, match="Worksheets are deprecated as of IPEP-17. Consider updating the notebook."
    ):
        result_single = process_notebook(nb_single)

    assert result_multi != result_single, "Two worksheets should produce more content than one."
    assert len(result_multi) > len(result_single), "The multi-worksheet notebook should have extra code content."
    assert "# First Worksheet" in result_single
    assert "# Second Worksheet" not in result_single
    assert "# First Worksheet" in result_multi
    assert "# Second Worksheet" in result_multi


def test_process_notebook_code_only(write_notebook: WriteNotebookFunc) -> None:
    """
    Test a notebook containing only code cells.

    Given a notebook with code cells only:
    When `process_notebook` is called,
    Then no triple quotes should appear in the output.
    """
    notebook_content = {
        "cells": [
            {"cell_type": "code", "source": ["print('Code Cell 1')"]},
            {"cell_type": "code", "source": ["x = 42"]},
        ]
    }
    nb_path = write_notebook("code_only.ipynb", notebook_content)
    result = process_notebook(nb_path)

    assert '"""' not in result, "No triple quotes expected when there are only code cells."
    assert "print('Code Cell 1')" in result
    assert "x = 42" in result


def test_process_notebook_markdown_only(write_notebook: WriteNotebookFunc) -> None:
    """
    Test a notebook with only markdown cells.

    Given a notebook with two markdown cells:
    When `process_notebook` is called,
    Then each markdown cell should become a triple-quoted block (2 blocks => 4 triple quotes total).
    """
    notebook_content = {
        "cells": [
            {"cell_type": "markdown", "source": ["# Markdown Header"]},
            {"cell_type": "markdown", "source": ["Some more markdown."]},
        ]
    }
    nb_path = write_notebook("markdown_only.ipynb", notebook_content)
    result = process_notebook(nb_path)

    assert result.count('"""') == 4, "Two markdown cells => 2 blocks => 4 triple quotes total."
    assert "# Markdown Header" in result
    assert "Some more markdown." in result


def test_process_notebook_raw_only(write_notebook: WriteNotebookFunc) -> None:
    """
    Test a notebook with only raw cells.

    Given two raw cells:
    When `process_notebook` is called,
    Then each raw cell should become a triple-quoted block (2 blocks => 4 triple quotes total).
    """
    notebook_content = {
        "cells": [
            {"cell_type": "raw", "source": ["Raw content line 1"]},
            {"cell_type": "raw", "source": ["Raw content line 2"]},
        ]
    }
    nb_path = write_notebook("raw_only.ipynb", notebook_content)
    result = process_notebook(nb_path)

    assert result.count('"""') == 4, "Two raw cells => 2 blocks => 4 triple quotes."
    assert "Raw content line 1" in result
    assert "Raw content line 2" in result


def test_process_notebook_empty_cells(write_notebook: WriteNotebookFunc) -> None:
    """
    Test that cells with an empty 'source' are skipped.

    Given a notebook with 4 cells, 3 of which have empty `source`:
    When `process_notebook` is called,
    Then only the non-empty cell should appear in the output (1 block => 2 triple quotes).
    """
    notebook_content = {
        "cells": [
            {"cell_type": "markdown", "source": []},
            {"cell_type": "code", "source": []},
            {"cell_type": "raw", "source": []},
            {"cell_type": "markdown", "source": ["# Non-empty markdown"]},
        ]
    }
    nb_path = write_notebook("empty_cells.ipynb", notebook_content)
    result = process_notebook(nb_path)

    assert result.count('"""') == 2, "Only one non-empty cell => 1 block => 2 triple quotes"
    assert "# Non-empty markdown" in result


def test_process_notebook_invalid_cell_type(write_notebook: WriteNotebookFunc) -> None:
    """
    Test a notebook with an unknown cell type.

    Given a notebook cell whose `cell_type` is unrecognized:
    When `process_notebook` is called,
    Then a ValueError should be raised.
    """
    notebook_content = {
        "cells": [
            {"cell_type": "markdown", "source": ["# Valid markdown"]},
            {"cell_type": "unknown", "source": ["Unrecognized cell type"]},
        ]
    }
    nb_path = write_notebook("invalid_cell_type.ipynb", notebook_content)

    with pytest.raises(ValueError, match="Unknown cell type: unknown"):
        process_notebook(nb_path)


def test_process_notebook_with_output(write_notebook: WriteNotebookFunc) -> None:
    """
    Test a notebook that has code cells with outputs.

    Given a code cell and multiple output objects:
    When `process_notebook` is called with `include_output=True`,
    Then the outputs should be appended as commented lines under the code.
    """
    notebook_content = {
        "cells": [
            {
                "cell_type": "code",
                "source": [
                    "import matplotlib.pyplot as plt\n",
                    "print('my_data')\n",
                    "my_data = [1, 2, 3, 4, 5]\n",
                    "plt.plot(my_data)\n",
                    "my_data",
                ],
                "outputs": [
                    {"output_type": "stream", "text": ["my_data"]},
                    {"output_type": "execute_result", "data": {"text/plain": ["[1, 2, 3, 4, 5]"]}},
                    {"output_type": "display_data", "data": {"text/plain": ["<Figure size 640x480 with 1 Axes>"]}},
                ],
            }
        ]
    }

    nb_path = write_notebook("with_output.ipynb", notebook_content)
    with_output = process_notebook(nb_path, include_output=True)
    without_output = process_notebook(nb_path, include_output=False)

    expected_source = "\n".join(
        [
            "# Jupyter notebook converted to Python script.\n",
            "import matplotlib.pyplot as plt",
            "print('my_data')",
            "my_data = [1, 2, 3, 4, 5]",
            "plt.plot(my_data)",
            "my_data\n",
        ]
    )
    expected_output = "\n".join(
        [
            "# Output:",
            "#   my_data",
            "#   [1, 2, 3, 4, 5]",
            "#   <Figure size 640x480 with 1 Axes>\n",
        ]
    )

    expected_combined = expected_source + expected_output

    assert with_output == expected_combined, "Should include source code and comment-ified output."
    assert without_output == expected_source, "Should include only the source code without output."



================================================
FILE: tests/test_repository_clone.py
================================================
"""
Tests for the `cloning` module.

These tests cover various scenarios for cloning repositories, verifying that the appropriate Git commands are invoked
and handling edge cases such as nonexistent URLs, timeouts, redirects, and specific commits or branches.
"""

import asyncio
import os
from pathlib import Path
from unittest.mock import AsyncMock, patch

import pytest

from gitingest.cloning import clone_repo
from gitingest.schemas import CloneConfig
from gitingest.utils.exceptions import AsyncTimeoutError
from gitingest.utils.git_utils import check_repo_exists


@pytest.mark.asyncio
async def test_clone_with_commit() -> None:
    """
    Test cloning a repository with a specific commit hash.

    Given a valid URL and a commit hash:
    When `clone_repo` is called,
    Then the repository should be cloned and checked out at that commit.
    """
    clone_config = CloneConfig(
        url="https://github.com/user/repo",
        local_path="/tmp/repo",
        commit="a" * 40,  # Simulating a valid commit hash
        branch="main",
    )

    with patch("gitingest.cloning.check_repo_exists", return_value=True) as mock_check:
        with patch("gitingest.cloning.run_command", new_callable=AsyncMock) as mock_exec:
            mock_process = AsyncMock()
            mock_process.communicate.return_value = (b"output", b"error")
            mock_exec.return_value = mock_process

            await clone_repo(clone_config)

            mock_check.assert_called_once_with(clone_config.url, token=None)
            assert mock_exec.call_count == 2  # Clone and checkout calls


@pytest.mark.asyncio
async def test_clone_without_commit() -> None:
    """
    Test cloning a repository when no commit hash is provided.

    Given a valid URL and no commit hash:
    When `clone_repo` is called,
    Then only the clone_repo operation should be performed (no checkout).
    """
    query = CloneConfig(
        url="https://github.com/user/repo",
        local_path="/tmp/repo",
        commit=None,
        branch="main",
    )

    with patch("gitingest.cloning.check_repo_exists", return_value=True) as mock_check:
        with patch("gitingest.cloning.run_command", new_callable=AsyncMock) as mock_exec:
            mock_process = AsyncMock()
            mock_process.communicate.return_value = (b"output", b"error")
            mock_exec.return_value = mock_process

            await clone_repo(query)

            mock_check.assert_called_once_with(query.url, token=None)
            assert mock_exec.call_count == 1  # Only clone call


@pytest.mark.asyncio
async def test_clone_nonexistent_repository() -> None:
    """
    Test cloning a nonexistent repository URL.

    Given an invalid or nonexistent URL:
    When `clone_repo` is called,
    Then a ValueError should be raised with an appropriate error message.
    """
    clone_config = CloneConfig(
        url="https://github.com/user/nonexistent-repo",
        local_path="/tmp/repo",
        commit=None,
        branch="main",
    )
    with patch("gitingest.cloning.check_repo_exists", return_value=False) as mock_check:
        with pytest.raises(ValueError, match="Repository not found"):
            await clone_repo(clone_config)

            mock_check.assert_called_once_with(clone_config.url)


@pytest.mark.asyncio
@pytest.mark.parametrize(
    "mock_stdout, return_code, expected",
    [
        (b"HTTP/1.1 200 OK\n", 0, True),  # Existing repo
        (b"HTTP/1.1 404 Not Found\n", 0, False),  # Non-existing repo
        (b"HTTP/1.1 200 OK\n", 1, False),  # Failed request
    ],
)
async def test_check_repo_exists(mock_stdout: bytes, return_code: int, expected: bool) -> None:
    """
    Test the `_check_repo_exists` function with different Git HTTP responses.

    Given various stdout lines and return codes:
    When `_check_repo_exists` is called,
    Then it should correctly indicate whether the repository exists.
    """
    url = "https://github.com/user/repo"

    with patch("asyncio.create_subprocess_exec", new_callable=AsyncMock) as mock_exec:
        mock_process = AsyncMock()
        # Mock the subprocess output
        mock_process.communicate.return_value = (mock_stdout, b"")
        mock_process.returncode = return_code
        mock_exec.return_value = mock_process

        repo_exists = await check_repo_exists(url)

        assert repo_exists is expected


@pytest.mark.asyncio
async def test_clone_with_custom_branch() -> None:
    """
    Test cloning a repository with a specified custom branch.

    Given a valid URL and a branch:
    When `clone_repo` is called,
    Then the repository should be cloned shallowly to that branch.
    """
    clone_config = CloneConfig(url="https://github.com/user/repo", local_path="/tmp/repo", branch="feature-branch")
    with patch("gitingest.cloning.check_repo_exists", return_value=True):
        with patch("gitingest.cloning.run_command", new_callable=AsyncMock) as mock_exec:
            await clone_repo(clone_config)

            mock_exec.assert_called_once_with(
                "git",
                "clone",
                "--single-branch",
                "--depth=1",
                "--branch",
                "feature-branch",
                clone_config.url,
                clone_config.local_path,
            )


@pytest.mark.asyncio
async def test_git_command_failure() -> None:
    """
    Test cloning when the Git command fails during execution.

    Given a valid URL, but `run_command` raises a RuntimeError:
    When `clone_repo` is called,
    Then a RuntimeError should be raised with the correct message.
    """
    clone_config = CloneConfig(
        url="https://github.com/user/repo",
        local_path="/tmp/repo",
    )
    with patch("gitingest.cloning.check_repo_exists", return_value=True):
        with patch("gitingest.cloning.run_command", side_effect=RuntimeError("Git command failed")):
            with pytest.raises(RuntimeError, match="Git command failed"):
                await clone_repo(clone_config)


@pytest.mark.asyncio
async def test_clone_default_shallow_clone() -> None:
    """
    Test cloning a repository with the default shallow clone options.

    Given a valid URL and no branch or commit:
    When `clone_repo` is called,
    Then the repository should be cloned with `--depth=1` and `--single-branch`.
    """
    clone_config = CloneConfig(
        url="https://github.com/user/repo",
        local_path="/tmp/repo",
    )

    with patch("gitingest.cloning.check_repo_exists", return_value=True):
        with patch("gitingest.cloning.run_command", new_callable=AsyncMock) as mock_exec:
            await clone_repo(clone_config)

            mock_exec.assert_called_once_with(
                "git",
                "clone",
                "--single-branch",
                "--depth=1",
                clone_config.url,
                clone_config.local_path,
            )


@pytest.mark.asyncio
async def test_clone_commit_without_branch() -> None:
    """
    Test cloning when a commit hash is provided but no branch is specified.

    Given a valid URL and a commit hash (but no branch):
    When `clone_repo` is called,
    Then the repository should be cloned and checked out at that commit.
    """
    clone_config = CloneConfig(
        url="https://github.com/user/repo",
        local_path="/tmp/repo",
        commit="a" * 40,  # Simulating a valid commit hash
    )
    with patch("gitingest.cloning.check_repo_exists", return_value=True):
        with patch("gitingest.cloning.run_command", new_callable=AsyncMock) as mock_exec:
            await clone_repo(clone_config)

            assert mock_exec.call_count == 2  # Clone and checkout calls
            mock_exec.assert_any_call("git", "clone", "--single-branch", clone_config.url, clone_config.local_path)
            mock_exec.assert_any_call("git", "-C", clone_config.local_path, "checkout", clone_config.commit)


@pytest.mark.asyncio
async def test_check_repo_exists_with_redirect() -> None:
    """
    Test `check_repo_exists` when a redirect (302) is returned.

    Given a URL that responds with "302 Found":
    When `check_repo_exists` is called,
    Then it should return `False`, indicating the repo is inaccessible.
    """
    url = "https://github.com/user/repo"
    with patch("asyncio.create_subprocess_exec", new_callable=AsyncMock) as mock_exec:
        mock_process = AsyncMock()
        mock_process.communicate.return_value = (b"HTTP/1.1 302 Found\n", b"")
        mock_process.returncode = 0  # Simulate successful request
        mock_exec.return_value = mock_process

        repo_exists = await check_repo_exists(url)

        assert repo_exists is False


@pytest.mark.asyncio
async def test_check_repo_exists_with_permanent_redirect() -> None:
    """
    Test `check_repo_exists` when a permanent redirect (301) is returned.

    Given a URL that responds with "301 Found":
    When `check_repo_exists` is called,
    Then it should return `True`, indicating the repo may exist at the new location.
    """
    url = "https://github.com/user/repo"
    with patch("asyncio.create_subprocess_exec", new_callable=AsyncMock) as mock_exec:
        mock_process = AsyncMock()
        mock_process.communicate.return_value = (b"HTTP/1.1 301 Found\n", b"")
        mock_process.returncode = 0  # Simulate successful request
        mock_exec.return_value = mock_process

        repo_exists = await check_repo_exists(url)

        assert repo_exists


@pytest.mark.asyncio
async def test_clone_with_timeout() -> None:
    """
    Test cloning a repository when a timeout occurs.

    Given a valid URL, but `run_command` times out:
    When `clone_repo` is called,
    Then an `AsyncTimeoutError` should be raised to indicate the operation exceeded time limits.
    """
    clone_config = CloneConfig(url="https://github.com/user/repo", local_path="/tmp/repo")

    with patch("gitingest.cloning.check_repo_exists", return_value=True):
        with patch("gitingest.cloning.run_command", new_callable=AsyncMock) as mock_exec:
            mock_exec.side_effect = asyncio.TimeoutError
            with pytest.raises(AsyncTimeoutError, match="Operation timed out after"):
                await clone_repo(clone_config)


@pytest.mark.asyncio
async def test_clone_specific_branch(tmp_path):
    """
    Test cloning a specific branch of a repository.

    Given a valid repository URL and a branch name:
    When `clone_repo` is called,
    Then the repository should be cloned and checked out at that branch.
    """
    repo_url = "https://github.com/cyclotruc/gitingest.git"
    branch_name = "main"
    local_path = tmp_path / "gitingest"

    config = CloneConfig(url=repo_url, local_path=str(local_path), branch=branch_name)
    await clone_repo(config)

    # Assertions
    assert local_path.exists(), "The repository was not cloned successfully."
    assert local_path.is_dir(), "The cloned repository path is not a directory."

    # Check the current branch
    current_branch = os.popen(f"git -C {local_path} branch --show-current").read().strip()
    assert current_branch == branch_name, f"Expected branch '{branch_name}', got '{current_branch}'."


@pytest.mark.asyncio
async def test_clone_branch_with_slashes(tmp_path):
    """
    Test cloning a branch with slashes in the name.

    Given a valid repository URL and a branch name with slashes:
    When `clone_repo` is called,
    Then the repository should be cloned and checked out at that branch.
    """
    repo_url = "https://github.com/user/repo"
    branch_name = "fix/in-operator"
    local_path = tmp_path / "gitingest"

    clone_config = CloneConfig(url=repo_url, local_path=str(local_path), branch=branch_name)
    with patch("gitingest.cloning.check_repo_exists", return_value=True):
        with patch("gitingest.cloning.run_command", new_callable=AsyncMock) as mock_exec:
            await clone_repo(clone_config)

            mock_exec.assert_called_once_with(
                "git",
                "clone",
                "--single-branch",
                "--depth=1",
                "--branch",
                "fix/in-operator",
                clone_config.url,
                clone_config.local_path,
            )


@pytest.mark.asyncio
async def test_clone_creates_parent_directory(tmp_path: Path) -> None:
    """
    Test that clone_repo creates parent directories if they don't exist.

    Given a local path with non-existent parent directories:
    When `clone_repo` is called,
    Then it should create the parent directories before attempting to clone.
    """
    nested_path = tmp_path / "deep" / "nested" / "path" / "repo"
    clone_config = CloneConfig(
        url="https://github.com/user/repo",
        local_path=str(nested_path),
    )

    with patch("gitingest.cloning.check_repo_exists", return_value=True):
        with patch("gitingest.cloning.run_command", new_callable=AsyncMock) as mock_exec:
            await clone_repo(clone_config)

            # Verify parent directory was created
            assert nested_path.parent.exists()

            # Verify git clone was called with correct parameters
            mock_exec.assert_called_once_with(
                "git",
                "clone",
                "--single-branch",
                "--depth=1",
                clone_config.url,
                str(nested_path),
            )


@pytest.mark.asyncio
async def test_clone_with_specific_subpath() -> None:
    """
    Test cloning a repository with a specific subpath.

    Given a valid repository URL and a specific subpath:
    When `clone_repo` is called,
    Then the repository should be cloned with sparse checkout enabled and the specified subpath.
    """
    clone_config = CloneConfig(url="https://github.com/user/repo", local_path="/tmp/repo", subpath="src/docs")

    with patch("gitingest.cloning.check_repo_exists", return_value=True):
        with patch("gitingest.cloning.run_command", new_callable=AsyncMock) as mock_exec:
            await clone_repo(clone_config)

            # Verify the clone command includes sparse checkout flags
            mock_exec.assert_any_call(
                "git",
                "clone",
                "--single-branch",
                "--filter=blob:none",
                "--sparse",
                "--depth=1",
                clone_config.url,
                clone_config.local_path,
            )

            # Verify the sparse-checkout command sets the correct path
            mock_exec.assert_any_call("git", "-C", clone_config.local_path, "sparse-checkout", "set", "src/docs")

            assert mock_exec.call_count == 2


@pytest.mark.asyncio
async def test_clone_with_commit_and_subpath() -> None:
    """
    Test cloning a repository with both a specific commit and subpath.

    Given a valid repository URL, commit hash, and subpath:
    When `clone_repo` is called,
    Then the repository should be cloned with sparse checkout enabled,
    checked out at the specific commit, and only include the specified subpath.
    """
    clone_config = CloneConfig(
        url="https://github.com/user/repo",
        local_path="/tmp/repo",
        commit="a" * 40,  # Simulating a valid commit hash
        subpath="src/docs",
    )

    with patch("gitingest.cloning.check_repo_exists", return_value=True):
        with patch("gitingest.cloning.run_command", new_callable=AsyncMock) as mock_exec:
            await clone_repo(clone_config)

            # Verify the clone command includes sparse checkout flags
            mock_exec.assert_any_call(
                "git",
                "clone",
                "--single-branch",
                "--filter=blob:none",
                "--sparse",
                clone_config.url,
                clone_config.local_path,
            )

            # Verify sparse-checkout set
            mock_exec.assert_any_call(
                "git",
                "-C",
                clone_config.local_path,
                "sparse-checkout",
                "set",
                "src/docs",
            )

            # Verify checkout commit
            mock_exec.assert_any_call(
                "git",
                "-C",
                clone_config.local_path,
                "checkout",
                clone_config.commit,
            )

            assert mock_exec.call_count == 3



================================================
FILE: tests/.pylintrc
================================================
[MASTER]
init-hook=
    import sys
    sys.path.append('./src')

[MESSAGES CONTROL]
disable=missing-class-docstring,missing-function-docstring,protected-access,fixme

[FORMAT]
max-line-length=119



================================================
FILE: tests/query_parser/test_git_host_agnostic.py
================================================
"""
Tests to verify that the query parser is Git host agnostic.

These tests confirm that `parse_query` correctly identifies user/repo pairs and canonical URLs for GitHub, GitLab,
Bitbucket, Gitea, and Codeberg, even if the host is omitted.
"""

from typing import List

import pytest

from gitingest.query_parsing import parse_query


@pytest.mark.parametrize(
    "urls, expected_user, expected_repo, expected_url",
    [
        (
            [
                "https://github.com/tiangolo/fastapi",
                "github.com/tiangolo/fastapi",
                "tiangolo/fastapi",
            ],
            "tiangolo",
            "fastapi",
            "https://github.com/tiangolo/fastapi",
        ),
        (
            [
                "https://gitlab.com/gitlab-org/gitlab-runner",
                "gitlab.com/gitlab-org/gitlab-runner",
                "gitlab-org/gitlab-runner",
            ],
            "gitlab-org",
            "gitlab-runner",
            "https://gitlab.com/gitlab-org/gitlab-runner",
        ),
        (
            [
                "https://bitbucket.org/na-dna/llm-knowledge-share",
                "bitbucket.org/na-dna/llm-knowledge-share",
                "na-dna/llm-knowledge-share",
            ],
            "na-dna",
            "llm-knowledge-share",
            "https://bitbucket.org/na-dna/llm-knowledge-share",
        ),
        (
            [
                "https://gitea.com/xorm/xorm",
                "gitea.com/xorm/xorm",
                "xorm/xorm",
            ],
            "xorm",
            "xorm",
            "https://gitea.com/xorm/xorm",
        ),
        (
            [
                "https://codeberg.org/forgejo/forgejo",
                "codeberg.org/forgejo/forgejo",
                "forgejo/forgejo",
            ],
            "forgejo",
            "forgejo",
            "https://codeberg.org/forgejo/forgejo",
        ),
    ],
)
@pytest.mark.asyncio
async def test_parse_query_without_host(
    urls: List[str],
    expected_user: str,
    expected_repo: str,
    expected_url: str,
) -> None:
    """
    Test `parse_query` for Git host agnosticism.

    Given multiple URL variations for the same user/repo on different Git hosts (with or without host names):
    When `parse_query` is called with each variation,
    Then the parser should correctly identify the user, repo, canonical URL, and other default fields.
    """
    for url in urls:
        query = await parse_query(url, max_file_size=50, from_web=True)

        assert query.user_name == expected_user
        assert query.repo_name == expected_repo
        assert query.url == expected_url
        assert query.slug == f"{expected_user}-{expected_repo}"
        assert query.id is not None
        assert query.subpath == "/"
        assert query.branch is None
        assert query.commit is None
        assert query.type is None



================================================
FILE: tests/query_parser/test_query_parser.py
================================================
"""
Tests for the `query_parsing` module.

These tests cover URL parsing, pattern parsing, and handling of branches/subpaths for HTTP(S) repositories and local
paths.
"""

from pathlib import Path
from unittest.mock import AsyncMock, patch

import pytest

from gitingest.query_parsing import _parse_patterns, _parse_remote_repo, parse_query
from gitingest.utils.ignore_patterns import DEFAULT_IGNORE_PATTERNS


@pytest.mark.asyncio
async def test_parse_url_valid_https() -> None:
    """
    Test `_parse_remote_repo` with valid HTTPS URLs.

    Given various HTTPS URLs on supported platforms:
    When `_parse_remote_repo` is called,
    Then user name, repo name, and the URL should be extracted correctly.
    """
    test_cases = [
        "https://github.com/user/repo",
        "https://gitlab.com/user/repo",
        "https://bitbucket.org/user/repo",
        "https://gitea.com/user/repo",
        "https://codeberg.org/user/repo",
        "https://gist.github.com/user/repo",
    ]
    for url in test_cases:
        query = await _parse_remote_repo(url)

        assert query.user_name == "user"
        assert query.repo_name == "repo"
        assert query.url == url


@pytest.mark.asyncio
async def test_parse_url_valid_http() -> None:
    """
    Test `_parse_remote_repo` with valid HTTP URLs.

    Given various HTTP URLs on supported platforms:
    When `_parse_remote_repo` is called,
    Then user name, repo name, and the slug should be extracted correctly.
    """
    test_cases = [
        "http://github.com/user/repo",
        "http://gitlab.com/user/repo",
        "http://bitbucket.org/user/repo",
        "http://gitea.com/user/repo",
        "http://codeberg.org/user/repo",
        "http://gist.github.com/user/repo",
    ]
    for url in test_cases:
        query = await _parse_remote_repo(url)

        assert query.user_name == "user"
        assert query.repo_name == "repo"
        assert query.slug == "user-repo"


@pytest.mark.asyncio
async def test_parse_url_invalid() -> None:
    """
    Test `_parse_remote_repo` with an invalid URL.

    Given an HTTPS URL lacking a repository structure (e.g., "https://github.com"),
    When `_parse_remote_repo` is called,
    Then a ValueError should be raised indicating an invalid repository URL.
    """
    url = "https://github.com"
    with pytest.raises(ValueError, match="Invalid repository URL"):
        await _parse_remote_repo(url)


@pytest.mark.asyncio
@pytest.mark.parametrize("url", ["https://github.com/user/repo", "https://gitlab.com/user/repo"])
async def test_parse_query_basic(url):
    """
    Test `parse_query` with a basic valid repository URL.

    Given an HTTPS URL and ignore_patterns="*.txt":
    When `parse_query` is called,
    Then user/repo, URL, and ignore patterns should be parsed correctly.
    """
    query = await parse_query(source=url, max_file_size=50, from_web=True, ignore_patterns="*.txt")

    assert query.user_name == "user"
    assert query.repo_name == "repo"
    assert query.url == url
    assert query.ignore_patterns
    assert "*.txt" in query.ignore_patterns


@pytest.mark.asyncio
async def test_parse_query_mixed_case() -> None:
    """
    Test `parse_query` with mixed-case URLs.

    Given a URL with mixed-case parts (e.g. "Https://GitHub.COM/UsEr/rEpO"):
    When `parse_query` is called,
    Then the user and repo names should be normalized to lowercase.
    """
    url = "Https://GitHub.COM/UsEr/rEpO"
    query = await parse_query(url, max_file_size=50, from_web=True)

    assert query.user_name == "user"
    assert query.repo_name == "repo"


@pytest.mark.asyncio
async def test_parse_query_include_pattern() -> None:
    """
    Test `parse_query` with a specified include pattern.

    Given a URL and include_patterns="*.py":
    When `parse_query` is called,
    Then the include pattern should be set, and default ignore patterns remain applied.
    """
    url = "https://github.com/user/repo"
    query = await parse_query(url, max_file_size=50, from_web=True, include_patterns="*.py")

    assert query.include_patterns == {"*.py"}
    assert query.ignore_patterns == DEFAULT_IGNORE_PATTERNS


@pytest.mark.asyncio
async def test_parse_query_invalid_pattern() -> None:
    """
    Test `parse_query` with an invalid pattern.

    Given an include pattern containing special characters (e.g., "*.py;rm -rf"):
    When `parse_query` is called,
    Then a ValueError should be raised indicating invalid characters.
    """
    url = "https://github.com/user/repo"
    with pytest.raises(ValueError, match="Pattern.*contains invalid characters"):
        await parse_query(url, max_file_size=50, from_web=True, include_patterns="*.py;rm -rf")


@pytest.mark.asyncio
async def test_parse_url_with_subpaths() -> None:
    """
    Test `_parse_remote_repo` with a URL containing branch and subpath.

    Given a URL referencing a branch ("main") and a subdir ("subdir/file"):
    When `_parse_remote_repo` is called with remote branch fetching,
    Then user, repo, branch, and subpath should be identified correctly.
    """
    url = "https://github.com/user/repo/tree/main/subdir/file"
    with patch("gitingest.utils.git_utils.run_command", new_callable=AsyncMock) as mock_run_command:
        mock_run_command.return_value = (b"refs/heads/main\nrefs/heads/dev\nrefs/heads/feature-branch\n", b"")
        with patch(
            "gitingest.utils.git_utils.fetch_remote_branch_list", new_callable=AsyncMock
        ) as mock_fetch_branches:
            mock_fetch_branches.return_value = ["main", "dev", "feature-branch"]
            query = await _parse_remote_repo(url)

            assert query.user_name == "user"
            assert query.repo_name == "repo"
            assert query.branch == "main"
            assert query.subpath == "/subdir/file"


@pytest.mark.asyncio
async def test_parse_url_invalid_repo_structure() -> None:
    """
    Test `_parse_remote_repo` with a URL missing a repository name.

    Given a URL like "https://github.com/user":
    When `_parse_remote_repo` is called,
    Then a ValueError should be raised indicating an invalid repository URL.
    """
    url = "https://github.com/user"
    with pytest.raises(ValueError, match="Invalid repository URL"):
        await _parse_remote_repo(url)


def test_parse_patterns_valid() -> None:
    """
    Test `_parse_patterns` with valid comma-separated patterns.

    Given patterns like "*.py, *.md, docs/*":
    When `_parse_patterns` is called,
    Then it should return a set of parsed strings.
    """
    patterns = "*.py, *.md, docs/*"
    parsed_patterns = _parse_patterns(patterns)

    assert parsed_patterns == {"*.py", "*.md", "docs/*"}


def test_parse_patterns_invalid_characters() -> None:
    """
    Test `_parse_patterns` with invalid characters.

    Given a pattern string containing special characters (e.g. "*.py;rm -rf"):
    When `_parse_patterns` is called,
    Then a ValueError should be raised indicating invalid pattern syntax.
    """
    patterns = "*.py;rm -rf"
    with pytest.raises(ValueError, match="Pattern.*contains invalid characters"):
        _parse_patterns(patterns)


@pytest.mark.asyncio
async def test_parse_query_with_large_file_size() -> None:
    """
    Test `parse_query` with a very large file size limit.

    Given a URL and max_file_size=10**9:
    When `parse_query` is called,
    Then `max_file_size` should be set correctly and default ignore patterns remain unchanged.
    """
    url = "https://github.com/user/repo"
    query = await parse_query(url, max_file_size=10**9, from_web=True)

    assert query.max_file_size == 10**9
    assert query.ignore_patterns == DEFAULT_IGNORE_PATTERNS


@pytest.mark.asyncio
async def test_parse_query_empty_patterns() -> None:
    """
    Test `parse_query` with empty patterns.

    Given empty include_patterns and ignore_patterns:
    When `parse_query` is called,
    Then include_patterns becomes None and default ignore patterns apply.
    """
    url = "https://github.com/user/repo"
    query = await parse_query(url, max_file_size=50, from_web=True, include_patterns="", ignore_patterns="")

    assert query.include_patterns is None
    assert query.ignore_patterns == DEFAULT_IGNORE_PATTERNS


@pytest.mark.asyncio
async def test_parse_query_include_and_ignore_overlap() -> None:
    """
    Test `parse_query` with overlapping patterns.

    Given include="*.py" and ignore={"*.py", "*.txt"}:
    When `parse_query` is called,
    Then "*.py" should be removed from ignore patterns.
    """
    url = "https://github.com/user/repo"
    query = await parse_query(
        url,
        max_file_size=50,
        from_web=True,
        include_patterns="*.py",
        ignore_patterns={"*.py", "*.txt"},
    )

    assert query.include_patterns == {"*.py"}
    assert query.ignore_patterns is not None
    assert "*.py" not in query.ignore_patterns
    assert "*.txt" in query.ignore_patterns


@pytest.mark.asyncio
async def test_parse_query_local_path() -> None:
    """
    Test `parse_query` with a local file path.

    Given "/home/user/project" and from_web=False:
    When `parse_query` is called,
    Then the local path should be set, id generated, and slug formed accordingly.
    """
    path = "/home/user/project"
    query = await parse_query(path, max_file_size=100, from_web=False)
    tail = Path("home/user/project")

    assert query.local_path.parts[-len(tail.parts) :] == tail.parts
    assert query.id is not None
    assert query.slug == "home/user/project"


@pytest.mark.asyncio
async def test_parse_query_relative_path() -> None:
    """
    Test `parse_query` with a relative path.

    Given "./project" and from_web=False:
    When `parse_query` is called,
    Then local_path resolves relatively, and slug ends with "project".
    """
    path = "./project"
    query = await parse_query(path, max_file_size=100, from_web=False)
    tail = Path("project")

    assert query.local_path.parts[-len(tail.parts) :] == tail.parts
    assert query.slug.endswith("project")


@pytest.mark.asyncio
async def test_parse_query_empty_source() -> None:
    """
    Test `parse_query` with an empty string.

    Given an empty source string:
    When `parse_query` is called,
    Then a ValueError should be raised indicating an invalid repository URL.
    """
    with pytest.raises(ValueError, match="Invalid repository URL"):
        await parse_query("", max_file_size=100, from_web=True)


@pytest.mark.asyncio
@pytest.mark.parametrize(
    "url, expected_branch, expected_commit",
    [
        ("https://github.com/user/repo/tree/main", "main", None),
        (
            "https://github.com/user/repo/tree/abcd1234abcd1234abcd1234abcd1234abcd1234",
            None,
            "abcd1234abcd1234abcd1234abcd1234abcd1234",
        ),
    ],
)
async def test_parse_url_branch_and_commit_distinction(url: str, expected_branch: str, expected_commit: str) -> None:
    """
    Test `_parse_remote_repo` distinguishing branch vs. commit hash.

    Given either a branch URL (e.g., ".../tree/main") or a 40-character commit URL:
    When `_parse_remote_repo` is called with branch fetching,
    Then the function should correctly set `branch` or `commit` based on the URL content.
    """
    with patch("gitingest.utils.git_utils.run_command", new_callable=AsyncMock) as mock_run_command:
        # Mocking the return value to include 'main' and some additional branches
        mock_run_command.return_value = (b"refs/heads/main\nrefs/heads/dev\nrefs/heads/feature-branch\n", b"")
        with patch(
            "gitingest.utils.git_utils.fetch_remote_branch_list", new_callable=AsyncMock
        ) as mock_fetch_branches:
            mock_fetch_branches.return_value = ["main", "dev", "feature-branch"]

            query = await _parse_remote_repo(url)

            # Verify that `branch` and `commit` match our expectations
            assert query.branch == expected_branch
            assert query.commit == expected_commit


@pytest.mark.asyncio
async def test_parse_query_uuid_uniqueness() -> None:
    """
    Test `parse_query` for unique UUID generation.

    Given the same path twice:
    When `parse_query` is called repeatedly,
    Then each call should produce a different query id.
    """
    path = "/home/user/project"
    query_1 = await parse_query(path, max_file_size=100, from_web=False)
    query_2 = await parse_query(path, max_file_size=100, from_web=False)

    assert query_1.id != query_2.id


@pytest.mark.asyncio
async def test_parse_url_with_query_and_fragment() -> None:
    """
    Test `_parse_remote_repo` with query parameters and a fragment.

    Given a URL like "https://github.com/user/repo?arg=value#fragment":
    When `_parse_remote_repo` is called,
    Then those parts should be stripped, leaving a clean user/repo URL.
    """
    url = "https://github.com/user/repo?arg=value#fragment"
    query = await _parse_remote_repo(url)

    assert query.user_name == "user"
    assert query.repo_name == "repo"
    assert query.url == "https://github.com/user/repo"  # URL should be cleaned


@pytest.mark.asyncio
async def test_parse_url_unsupported_host() -> None:
    """
    Test `_parse_remote_repo` with an unsupported host.

    Given "https://only-domain.com":
    When `_parse_remote_repo` is called,
    Then a ValueError should be raised for the unknown domain.
    """
    url = "https://only-domain.com"
    with pytest.raises(ValueError, match="Unknown domain 'only-domain.com' in URL"):
        await _parse_remote_repo(url)


@pytest.mark.asyncio
async def test_parse_query_with_branch() -> None:
    """
    Test `parse_query` when a branch is specified in a blob path.

    Given "https://github.com/pandas-dev/pandas/blob/2.2.x/...":
    When `parse_query` is called,
    Then the branch should be identified, subpath set, and commit remain None.
    """
    url = "https://github.com/pandas-dev/pandas/blob/2.2.x/.github/ISSUE_TEMPLATE/documentation_improvement.yaml"
    query = await parse_query(url, max_file_size=10**9, from_web=True)

    assert query.user_name == "pandas-dev"
    assert query.repo_name == "pandas"
    assert query.url == "https://github.com/pandas-dev/pandas"
    assert query.slug == "pandas-dev-pandas"
    assert query.id is not None
    assert query.subpath == "/.github/ISSUE_TEMPLATE/documentation_improvement.yaml"
    assert query.branch == "2.2.x"
    assert query.commit is None
    assert query.type == "blob"


@pytest.mark.asyncio
@pytest.mark.parametrize(
    "url, expected_branch, expected_subpath",
    [
        ("https://github.com/user/repo/tree/main/src", "main", "/src"),
        ("https://github.com/user/repo/tree/fix1", "fix1", "/"),
        ("https://github.com/user/repo/tree/nonexistent-branch/src", "nonexistent-branch", "/src"),
    ],
)
async def test_parse_repo_source_with_failed_git_command(url, expected_branch, expected_subpath):
    """
    Test `_parse_remote_repo` when git fetch fails.

    Given a URL referencing a branch, but Git fetching fails:
    When `_parse_remote_repo` is called,
    Then it should fall back to path components for branch identification.
    """
    with patch("gitingest.utils.git_utils.fetch_remote_branch_list", new_callable=AsyncMock) as mock_fetch_branches:
        mock_fetch_branches.side_effect = Exception("Failed to fetch branch list")

        with pytest.warns(
            RuntimeWarning,
            match="Warning: Failed to fetch branch list: Command failed: "
            "git ls-remote --heads https://github.com/user/repo",
        ):

            query = await _parse_remote_repo(url)

            assert query.branch == expected_branch
            assert query.subpath == expected_subpath


@pytest.mark.asyncio
@pytest.mark.parametrize(
    "url, expected_branch, expected_subpath",
    [
        ("https://github.com/user/repo/tree/feature/fix1/src", "feature/fix1", "/src"),
        ("https://github.com/user/repo/tree/main/src", "main", "/src"),
        ("https://github.com/user/repo", None, "/"),  # No
        ("https://github.com/user/repo/tree/nonexistent-branch/src", None, "/"),  # Non-existent branch
        ("https://github.com/user/repo/tree/fix", "fix", "/"),
        ("https://github.com/user/repo/blob/fix/page.html", "fix", "/page.html"),
    ],
)
async def test_parse_repo_source_with_various_url_patterns(url, expected_branch, expected_subpath):
    """
    Test `_parse_remote_repo` with various URL patterns.

    Given multiple branch/blob patterns (including nonexistent branches):
    When `_parse_remote_repo` is called with remote branch fetching,
    Then the correct branch/subpath should be set or None if unmatched.
    """
    with patch("gitingest.utils.git_utils.run_command", new_callable=AsyncMock) as mock_run_command:
        with patch(
            "gitingest.utils.git_utils.fetch_remote_branch_list", new_callable=AsyncMock
        ) as mock_fetch_branches:
            mock_run_command.return_value = (
                b"refs/heads/feature/fix1\nrefs/heads/main\nrefs/heads/feature-branch\nrefs/heads/fix\n",
                b"",
            )
            mock_fetch_branches.return_value = ["feature/fix1", "main", "feature-branch"]

            query = await _parse_remote_repo(url)

            assert query.branch == expected_branch
            assert query.subpath == expected_subpath



================================================
FILE: .github/dependabot.yml
================================================
version: 2
updates:
  - package-ecosystem: "pip"
    directory: "/"
    schedule:
      interval: "daily"
      time: "06:00"
      timezone: "UTC"
    open-pull-requests-limit: 5
    labels:
      - "dependencies"
      - "pip"



================================================
FILE: .github/workflows/ci.yml
================================================
name: CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

permissions:
  contents: read

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: true
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ["3.8", "3.9", "3.10", "3.11", "3.12", "3.13"]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/*requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements-dev.txt

    - name: Run tests
      run: |
        pytest

    #  Run pre-commit only on Python 3.13 + ubuntu.
    - name: Run pre-commit hooks
      if: ${{ matrix.python-version == '3.13' && matrix.os == 'ubuntu-latest' }}
      run: |
        pre-commit run --all-files



================================================
FILE: .github/workflows/publish.yml
================================================
name: "Publish to PyPI"

on:
  release:
    types: [created]
  workflow_dispatch:

permissions:
  contents: read

jobs:
  release-build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      - name: Build package
        run: |
          pip install build
          python -m build
      - uses: actions/upload-artifact@v4
        with:
          name: dist
          path: dist/

  pypi-publish:
    needs: [release-build]
    runs-on: ubuntu-latest
    environment: pypi
    permissions:
      id-token: write
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: dist
          path: dist/
      - uses: pypa/gh-action-pypi-publish@release/v1



================================================
FILE: .github/workflows/scorecard.yml
================================================
name: OSSF Scorecard
on:
  # For Branch-Protection check. Only the default branch is supported. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#branch-protection
  branch_protection_rule:
  # To guarantee Maintained check is occasionally updated. See
  # https://github.com/ossf/scorecard/blob/main/docs/checks.md#maintained
  schedule:
    - cron: '33 11 * * 2'
  push:
    branches: [ "main" ]

# Declare default permissions as read only.
permissions: read-all

jobs:
  analysis:
    name: Scorecard analysis
    runs-on: ubuntu-latest
    permissions:
      # Needed to upload the results to code-scanning dashboard.
      security-events: write
      # Needed to publish results and get a badge (see publish_results below).
      id-token: write

    steps:
      - name: "Checkout code"
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          persist-credentials: false

      - name: "Run analysis"
        uses: ossf/scorecard-action@0864cf19026789058feabb7e87baa5f140aac736 # v2.3.1
        with:
          results_file: results.sarif
          results_format: sarif

          # Public repositories:
          #   - Publish results to OpenSSF REST API for easy access by consumers
          #   - Allows the repository to include the Scorecard badge.
          #   - See https://github.com/ossf/scorecard-action#publishing-results.
          publish_results: true

      # Upload the results as artifacts (optional). Commenting out will disable uploads of run results in SARIF
      # format to the repository Actions tab.

      # Upload the results to GitHub's code scanning dashboard (optional).
      # Commenting out will disable upload of results to your repo's Code Scanning dashboard
      - name: "Upload to code-scanning"
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: results.sarif





================================================
FILE: data/example.md
================================================
# Example Markdown
xxxxSearching for quaxxxxntum entanglement.



================================================
FILE: data/octocat-hello-world.txt
================================================
Directory structure:
‚îî‚îÄ‚îÄ octocat-hello-world/
    ‚îî‚îÄ‚îÄ README

================================================
FILE: README
================================================
Hello World!





================================================
FILE: data/quantum_computing.txt
================================================
Quantum computing is a rapidly evolving field that leverages the principles of quantum mechanics to process information in fundamentally new ways. Unlike classical computers, which use bits as the smallest unit of data, quantum computers use quantum bits, or qubits. Qubits can exist in multiple states simultaneously due to the phenomenon of superposition, allowing quantum computers to perform certain calculations much more efficiently than their classical counterparts. Another key property of qubits is entanglement, which enables qubits that are entangled to be correlated with each other regardless of the distance separating them. This property is harnessed in quantum algorithms to achieve computational speedups for specific problems, such as factoring large numbers or searching unsorted databases. Quantum computers have the potential to revolutionize fields like cryptography, materials science, and drug discovery by solving problems that are currently intractable for classical computers. However, building practical quantum computers remains a significant challenge due to issues like qubit decoherence and error correction. Researchers are actively exploring various physical implementations of qubits, including superconducting circuits, trapped ions, and topological qubits. As the technology matures, quantum computing may unlock new possibilities and transform the way we approach complex computational tasks.



================================================
FILE: data/quantum_mechanics.txt
================================================
Quantum mechanics is a fundamental theory in physics that describes the behavior of matter and energy at the smallest scales, such as atoms and subatomic particles. Unlike classical physics, which provides deterministic predictions, quantum mechanics is inherently probabilistic. It introduces concepts like wave-particle duality, where particles such as electrons exhibit both wave-like and particle-like properties. The Heisenberg uncertainty principle states that certain pairs of physical properties, like position and momentum, cannot both be precisely known at the same time. Quantum mechanics also explains phenomena such as quantum tunneling, where particles can pass through energy barriers that would be insurmountable in classical physics. The theory has been confirmed by countless experiments and underpins much of modern technology, including semiconductors, lasers, and magnetic resonance imaging. Quantum mechanics is governed by mathematical equations such as the Schr√∂dinger equation, which describes how the quantum state of a system evolves over time. Despite its abstract nature and counterintuitive predictions, quantum mechanics remains one of the most successful and well-tested theories in science, providing deep insights into the nature of reality.



================================================
FILE: data/sample.txt
================================================
xxxThis is a test file.
It talks about quanssssssstum physics.
End of file.


================================================
FILE: docs/API.md
================================================
# API Documentation

This document describes the APIs and endpoints available in the project.

## Multi-Agent System APIs

### Research Agent (Port 8001)

#### POST /research
Research and gather information on a given topic.

**Request:**
```json
{
    "query": "string",
    "max_sources": 5,
    "include_technical": true
}
```

**Response:**
```json
{
    "results": [
        {
            "source": "string",
            "content": "string",
            "relevance_score": 0.95,
            "timestamp": "2024-01-01T00:00:00Z"
        }
    ],
    "summary": "string",
    "confidence": 0.85
}
```

#### GET /health
Check agent health status.

**Response:**
```json
{
    "status": "healthy",
    "model": "mistral:7b",
    "uptime": "2h 30m"
}
```

### Writer Agent (Port 8002)

#### POST /write
Generate written content based on research data.

**Request:**
```json
{
    "topic": "string",
    "research_data": [
        {
            "source": "string",
            "content": "string"
        }
    ],
    "style": "technical|academic|casual",
    "length": "short|medium|long"
}
```

**Response:**
```json
{
    "content": "string",
    "word_count": 500,
    "style_score": 0.9,
    "citations": ["source1", "source2"]
}
```

#### POST /edit
Edit and improve existing content.

**Request:**
```json
{
    "content": "string",
    "instructions": "string",
    "preserve_style": true
}
```

**Response:**
```json
{
    "edited_content": "string",
    "changes_made": ["grammar", "clarity", "structure"],
    "improvement_score": 0.15
}
```

## RAG System APIs

### Simple RAG

#### Function: `query_documents(query: str, top_k: int = 5)`
Query the local document collection.

**Parameters:**
- `query`: Search query string
- `top_k`: Number of top results to return

**Returns:**
```python
{
    "answer": "string",
    "sources": [
        {
            "content": "string",
            "score": 0.95,
            "metadata": {}
        }
    ],
    "processing_time": 1.5
}
```

### OpenAI RAG

#### Function: `openai_rag_query(query: str, model: str = "gpt-3.5-turbo")`
Query using OpenAI models for comparison.

**Parameters:**
- `query`: Search query string
- `model`: OpenAI model to use

**Returns:**
```python
{
    "answer": "string",
    "model_used": "gpt-3.5-turbo",
    "tokens_used": 150,
    "cost_estimate": 0.0003
}
```

## GitIngest Integration APIs

### Basic GitIngest

#### Function: `get_repo_summary(repo_url: str)`
Get repository summary from GitIngest.

**Parameters:**
- `repo_url`: GitHub repository URL

**Returns:**
```python
{
    "success": True,
    "summary": "string",
    "file_count": 25,
    "total_lines": 1500,
    "languages": ["Python", "JavaScript"]
}
```

### Advanced GitIngest

#### Function: `process_and_ingest(repo_url: str, save_to_rag: bool = True)`
Process repository and optionally add to RAG system.

**Parameters:**
- `repo_url`: GitHub repository URL
- `save_to_rag`: Whether to add to RAG database

**Returns:**
```python
{
    "processed": True,
    "filename": "repo-summary.txt",
    "added_to_rag": True,
    "chunk_count": 12
}
```

## Model Configuration APIs

### Ollama Integration

#### Function: `test_ollama_connection(model: str = "mistral:7b")`
Test connection to Ollama service.

**Parameters:**
- `model`: Model name to test

**Returns:**
```python
{
    "connected": True,
    "model": "mistral:7b",
    "version": "0.1.25",
    "available_models": ["mistral:7b", "phi3:mini"]
}
```

## Error Handling

All APIs use consistent error response format:

```json
{
    "error": {
        "code": "ERROR_CODE",
        "message": "Human readable error message",
        "details": "Additional technical details",
        "timestamp": "2024-01-01T00:00:00Z"
    }
}
```

### Common Error Codes

- `MODEL_UNAVAILABLE`: AI model is not accessible
- `INVALID_INPUT`: Request validation failed
- `PROCESSING_ERROR`: Internal processing error
- `TIMEOUT`: Request timed out
- `RATE_LIMITED`: Too many requests

## Authentication

Currently, the system uses:
- **OpenAI API**: Requires `OPENAI_API_KEY` environment variable
- **Local APIs**: No authentication (intended for local development)
- **GitIngest**: No authentication required

## Rate Limits

- **Local Ollama**: Limited by hardware capability
- **OpenAI API**: Subject to OpenAI's rate limits
- **GitIngest**: Public API limits apply

## Response Times

Typical response times:
- Local RAG queries: 1-5 seconds
- OpenAI queries: 2-10 seconds
- Agent communication: 0.5-2 seconds
- GitIngest processing: 5-30 seconds (depending on repo size)

## WebSocket Support

Currently not implemented, but architecture supports:
- Real-time agent communication
- Streaming responses
- Live collaboration features

## Batch Operations

For processing multiple requests:
- Use individual API calls in sequence
- Implement client-side batching
- Consider rate limits and timeouts



================================================
FILE: docs/PROJECT_STRUCTURE.md
================================================
# Project Structure

This document provides an overview of the project structure and key components.

## Directory Structure

```
410_mistral7b_ollama_pydanticai/
‚îú‚îÄ‚îÄ docs/                          # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ PROJECT_STRUCTURE.md      # This file
‚îÇ   ‚îî‚îÄ‚îÄ API.md                     # API documentation
‚îú‚îÄ‚îÄ data/                          # Data files for RAG
‚îÇ   ‚îú‚îÄ‚îÄ cyclotruc-gitingest.txt   # GitIngest outputs
‚îÇ   ‚îú‚îÄ‚îÄ octocat-hello-world.txt   
‚îÇ   ‚îú‚îÄ‚îÄ quantum_computing.txt     
‚îÇ   ‚îú‚îÄ‚îÄ quantum_mechanics.txt     
‚îÇ   ‚îú‚îÄ‚îÄ sample.txt                
‚îÇ   ‚îî‚îÄ‚îÄ example.md                
‚îú‚îÄ‚îÄ __pycache__/                   # Python cache files
‚îú‚îÄ‚îÄ README.md                      # Main project documentation
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îú‚îÄ‚îÄ SETUP.md                       # Detailed setup instructions
‚îú‚îÄ‚îÄ EXAMPLES.md                    # Usage examples
‚îú‚îÄ‚îÄ CHANGELOG.md                   # Version history
‚îú‚îÄ‚îÄ .gitignore                     # Git ignore rules
‚îî‚îÄ‚îÄ LICENSE                        # Project license
```

## Core Components

### RAG System
- **`rag_file_loader.py`** - Document loading and vector database management
- **`simple_rag.py`** - Basic RAG query interface
- **`openai_rag_demo.py`** - OpenAI-powered RAG for comparison

### Multi-Agent System
- **`agent1_research.py`** - Research agent (port 8001)
- **`agent2_writer.py`** - Writer agent (port 8002)
- **`demo_client.py`** - Client for testing agents
- **`start_agents.sh`** - Script to start all agents

### GitIngest Integration
- **`gitingest_demo.py`** - Basic GitIngest API demonstration
- **`simple_gitingest_examples.py`** - Simple usage examples
- **`gitingest_agent_integration.py`** - Advanced agent integration

### Local Ollama Integration
- **`ollama_pydantic_agent.py`** - Pydantic AI with Ollama
- **`pydantic_agent_basic.py`** - Basic Pydantic AI examples
- **`test_ollama.py`** - Ollama connectivity tests
- **`mistral_ollama_fastapi.py`** - FastAPI with Mistral/Ollama

### Utility Scripts
- **`schemas.py`** - Pydantic schemas and data models
- **`clean_requirements.py`** - Dependency management
- **`find_files_with_phrase.py`** - Text search utilities
- **`selenium1.py`**, **`selenium2.py`** - Web scraping tools
- **`audio_transcribe_summarize.py`** - Audio processing

## Data Flow

1. **Document Ingestion**: Files from `data/` directory are loaded and vectorized
2. **Vector Storage**: ChromaDB stores document embeddings locally
3. **Query Processing**: User queries are vectorized and matched against documents
4. **Response Generation**: Local Ollama models or OpenAI generate responses
5. **Multi-Agent Coordination**: Research and Writer agents collaborate on complex tasks

## Key Features

### Local AI Models
- Mistral 7B via Ollama
- phi3:mini for lightweight operations
- ChromaDB for vector storage
- Local embeddings using scikit-learn

### External Integrations
- OpenAI API for quality comparison
- GitIngest.com for codebase summarization
- FastAPI for REST endpoints

### Agent Architecture
- Modular agent design
- RESTful communication between agents
- Configurable model backends
- Error handling and retry logic

## Configuration

### Environment Variables
- `OPENAI_API_KEY` - Required for OpenAI comparison features
- Model configurations in individual scripts

### Model Settings
- Default local model: `mistral:7b`
- Fallback model: `phi3:mini`
- Embedding model: TF-IDF (local) or OpenAI
- Vector database: ChromaDB (persistent)

## Extension Points

The architecture supports easy extension for:
- Additional AI models (Ollama, LM Studio, etc.)
- New agent types and capabilities
- Different vector databases
- Custom document processors
- Alternative embedding methods


